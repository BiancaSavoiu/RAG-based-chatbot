{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dawid_skene_model import list2array, DawidSkeneModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from huggingface_hub import login\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura la chiave API di OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Imposta la variabile d'ambiente\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# Esegui l'autenticazione\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "# Autenticazione google\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI AUSILIARIE\n",
    "# Converti il DataFrame in una lista nidificata (dataset_list)\n",
    "def dataframe_to_dataset_list(df, model_columns):\n",
    "    dataset_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = []\n",
    "        for model in model_columns:\n",
    "            response = row[model]  # Prendi la risposta del modello\n",
    "            task.append([0] if response == \"No\" else [1])  # Converti in formato numerico\n",
    "        dataset_list.append(task)\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "eval_dataset = Dataset.load_from_disk(\"eval_dataset\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_syntethic = eval_dataset.to_pandas()\n",
    "eval_df_syntethic = eval_df_syntethic[[\"question\", \"answer\", \"source_doc\", \"context\", \"chunk_num\"]]\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset_adjacent_chunks = Dataset.load_from_disk(\"eval_dataset_adjacent_chunks\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_adjacent_chunks_syntethic = eval_dataset_adjacent_chunks.to_pandas()\n",
    "eval_df_adjacent_chunks_syntethic = eval_df_adjacent_chunks_syntethic[[\"question\", \"answer\", \"source_doc\", \"context\", \"chunk_num\"]]\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset_random_chunks = Dataset.load_from_disk(\"eval_dataset_random_chunks\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_random_chunks_syntethic = eval_dataset_random_chunks.to_pandas()\n",
    "eval_df_random_chunks_syntethic = eval_df_random_chunks_syntethic[[\"question\", \"answer\", \"source_doc\", \"context\", \"chunk_num\"]]\n",
    "\n",
    "eval_df_syntethic = eval_df_syntethic[eval_df_syntethic['chunk_num'].notna()]\n",
    "eval_df_syntethic = eval_df_syntethic.reset_index(drop=True)\n",
    "eval_df_syntethic['chunk_num'] = eval_df_syntethic['chunk_num'].apply(lambda x: [int(x)])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df = pd.concat([\n",
    "    eval_df_random_chunks_syntethic, \n",
    "    eval_df_syntethic, \n",
    "    eval_df_adjacent_chunks_syntethic\n",
    "], ignore_index=True)\n",
    "\n",
    "df = df.drop(columns = [\"source_doc\", \"context\", \"chunk_num\"])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Configurazione modelli **\n",
    "MODEL_NAMES = {\n",
    "    \"BERTino\": \"dbmdz/bert-base-italian-xxl-cased\", #111M params\n",
    "    \"UmBERTo\": \"Musixmatch/umberto-commoncrawl-cased-v1\", #110M params\n",
    "    \"GePpeTto\": \"LorenzoDeMattei/GePpeTto\" #117M params\n",
    "}\n",
    "    \n",
    "class HuggingFaceModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        # **Aggiungi un token di padding se non presente**\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))  # Aggiorna la dimensione dei token nel modello\n",
    "\n",
    "    def generate(self, question, answer):\n",
    "        \"\"\"Esegue la valutazione del modello sulla coppia domanda-risposta.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            f\"Domanda: {question} Risposta: {answer}\", \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,  \n",
    "            max_length=512,  \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        return \"Sì\" if predicted_class == 1 else \"No\"\n",
    "\n",
    "# ** Inizializzazione dei modelli dell'Ensemble **\n",
    "ensemble_models = {name: HuggingFaceModel(model) for name, model in MODEL_NAMES.items()}\n",
    "\n",
    "# ** Inizializza il modello Gemini **\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Inizializza il modello di ChatGPT-4o\n",
    "chatgpt_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(model, row):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Sei un critico che valuta coppie di domande e risposte per una FAQ di un software gestionale. \\\n",
    "            Le coppie di domande e risposte devono soddisfare i seguenti criteri per essere considerate utili per valutare un chatbot destinato al supporto clienti di un software gestionale: \\\n",
    "            1. **Rilevanza**: Devono affrontare temi rilevanti per gli utenti di un software gestionale. \\\n",
    "            2. **Logicità e utilità**: Devono essere logiche e utili per fornire informazioni chiare e pratiche agli utenti. \\\n",
    "            Valuta la seguente coppia e decidi se è utile per testare un chatbot per il supporto clienti. \\\n",
    "            Rispondi esclusivamente con 'Sì' o 'No'.\"),\n",
    "        \n",
    "        HumanMessage(content=f\"**Domanda:** {row['question']}\\n**Risposta:** {row['answer']}\\n\\nRispondi esclusivamente con 'Sì' o 'No'.\")\n",
    "    ]\n",
    "\n",
    "    if isinstance(model, ChatOpenAI) or isinstance(model, ChatGoogleGenerativeAI):\n",
    "        response = model.invoke(messages)  # Usa invoke() per evitare errori di compatibilità\n",
    "        answer = response.content.strip()\n",
    "        return \"Sì\" if \"Sì\" in answer else \"No\"\n",
    "    else:\n",
    "        return model.generate(row['question'], row['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# ** Valutazione con Gemini **\n",
    "df[\"Gemini\"] = df.progress_apply(lambda row: evaluate_pair(model_gemini, row), axis=1)\n",
    "\n",
    "# ** Valutazione con l'ensemble di modelli piccoli **\n",
    "for model_name, model in ensemble_models.items():\n",
    "    df[model_name] = df.progress_apply(lambda row: evaluate_pair(model, row), axis=1)\n",
    "\n",
    "# **Funzione per Majority Voting**\n",
    "def majority_voting(row):\n",
    "    votes = [row[model] for model in MODEL_NAMES.keys()]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Opzione più votata\n",
    "\n",
    "# **Calcoliamo Majority Voting**\n",
    "df[\"Majority Voting\"] = df.apply(majority_voting, axis=1)\n",
    "\n",
    "# **Confrontiamo con Gemini**\n",
    "df[\"Agreement Majority-Gemini\"] = df[\"Majority Voting\"] == df[\"Gemini\"]\n",
    "\n",
    "# **Calcoliamo le percentuali di accordo**\n",
    "majority_vs_gemini = df[\"Agreement Majority-Gemini\"].mean() * 100\n",
    "\n",
    "# ** Encoding delle risposte per l'analisi di consenso **\n",
    "def encode_answers(df):\n",
    "    return df.replace({\"Sì\": 1, \"No\": 0})\n",
    "\n",
    "#encoded_df = encode_answers(df.iloc[:, 1:-1])  # Escludiamo la colonna delle domande\n",
    "encoded_df = encode_answers(df.iloc[:, 1:-1]).apply(pd.to_numeric, errors=\"coerce\")\n",
    "display(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Valutazione con GPT-4o **\n",
    "df[\"ChatGPT-4o\"] = df.progress_apply(lambda row: evaluate_pair(chatgpt_model, row), axis=1)\n",
    "\n",
    "# ** Confronto tra GPT e Gemini **\n",
    "df[\"Agreement Gemini-GPT4o\"] = df[\"Gemini\"] == df[\"ChatGPT-4o\"]\n",
    "gpt4o_vs_gemini = df[\"Agreement Gemini-GPT4o\"].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci le colonne dei modelli\n",
    "model_columns = [\"BERTino\", \"UmBERTo\", \"GePpeTto\"]\n",
    "\n",
    "# 3Converti il DataFrame in dataset_list\n",
    "dataset_list = dataframe_to_dataset_list(df, model_columns)\n",
    "\n",
    "# Converti in tensore NumPy\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor = list2array(class_num, dataset_list)\n",
    "\n",
    "# Inizializza e lancia il modello di Dawid & Skene\n",
    "model = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict, error_rates, worker_reliability, predict_label = model.run(dataset_tensor)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers = [\"Sì\" if p[1] > 0.5 else \"No\" for p in predict_label]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "df[\"Dawid & Skene Multi-Class\"] = final_answers\n",
    "\n",
    "# Mostra il confronto con Majority Voting e Gemini\n",
    "df[\"Agreement D&S Multi-Class-Gemini\"] = df[\"Dawid & Skene Multi-Class\"] == df[\"Gemini\"]\n",
    "\n",
    "# Calcola le percentuali di accordo\n",
    "ds_multi_vs_gemini = df[\"Agreement D&S Multi-Class-Gemini\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", \"ChatGPT-4o\"],\n",
    "    \"Concordanza con Gemini (%)\": [majority_vs_gemini, ds_multi_vs_gemini, gpt4o_vs_gemini]\n",
    "})\n",
    "\n",
    "display(summary_df)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = df[[\n",
    "    \"Majority Voting\", \n",
    "    \"Dawid & Skene Multi-Class\",\n",
    "    \"ChatGPT-4o\", \n",
    "    \"Gemini\"\n",
    "]].map(lambda x: 1 if x == \"Sì\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Confronto tra Modelli e Gemini\")\n",
    "plt.xlabel(\"Metodo di Ensemble o Modello\")\n",
    "plt.ylabel(\"Domande\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ensembles experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiungiamo LLaMA 3.2 al dizionario dei modelli\n",
    "MODEL_NAMES[\"LLaMA3.2\"] = \"meta-llama/Llama-3.2-1B\" #1B\n",
    "\n",
    "# Inizializzazione del modello LLaMA 3.2\n",
    "ensemble_models[\"LLaMA3.2\"] = HuggingFaceModel(MODEL_NAMES[\"LLaMA3.2\"])\n",
    "\n",
    "df[\"LLaMA3.2\"] = df.progress_apply(lambda row: evaluate_pair(ensemble_models[\"LLaMA3.2\"], row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES[\"GPT2-smallIta\"] = \"GroNLP/gpt2-small-italian\" #121M\n",
    "\n",
    "ensemble_models[\"GPT2-smallIta\"] = HuggingFaceModel(MODEL_NAMES[\"GPT2-smallIta\"])\n",
    "\n",
    "df[\"GPT2-smallIta\"] = df.progress_apply(lambda row: evaluate_pair(ensemble_models[\"GPT2-smallIta\"], row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo i modelli nell’ensemble\n",
    "model_columns_enhanced = ['BERTino', 'UmBERTo', 'GePpeTto', 'GPT2-smallIta', 'LLaMA3.2']\n",
    "def majority_voting_enhanced(row):\n",
    "    models_to_consider = model_columns_enhanced  # Updated models list\n",
    "    votes = [row[model] for model in models_to_consider]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Returns the most common choice\n",
    "\n",
    "# **Calcoliamo Majority Voting**\n",
    "df[\"Enhanced Majority Voting\"] = df.apply(majority_voting_enhanced, axis=1)\n",
    "\n",
    "# **Confrontiamo con Gemini**\n",
    "df[\"Agreement Enhanced Majority-Gemini\"] = df[\"Enhanced Majority Voting\"] == df[\"Gemini\"]\n",
    "\n",
    "# **Calcoliamo le percentuali di accordo**\n",
    "ensemble_enhanced_mv_vs_gemini = df[\"Agreement Enhanced Majority-Gemini\"].mean() * 100\n",
    "\n",
    "# Converti il DataFrame in lista nidificata\n",
    "dataset_list_enhanced = dataframe_to_dataset_list(df, model_columns_enhanced)\n",
    "\n",
    "# Converti in tensore NumPy per Dawid & Skene\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor_enhanced = list2array(class_num, dataset_list_enhanced)\n",
    "\n",
    "# Inizializza e lancia il modello Dawid & Skene\n",
    "ds_model_enhanced = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict_enhanced, error_rates_enhanced, worker_reliability_enhanced, predict_label_enhanced = ds_model_enhanced.run(dataset_tensor_enhanced)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers_enhanced = [\"Sì\" if p[1] > 0.5 else \"No\" for p in predict_label_enhanced]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "df[\"Ensemble Enhanced (Dawid & Skene)\"] = final_answers_enhanced\n",
    "\n",
    "df[\"Agreement Ensemble Enhanced (D&S)-Gemini\"] = df[\"Ensemble Enhanced (Dawid & Skene)\"] == df[\"Gemini\"]\n",
    "ensemble_enhanced_ds_vs_gemini = df[\"Agreement Ensemble Enhanced (D&S)-Gemini\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", \"Enhanced Majority Voting\", \"Ensemble Enhanced (Dawid & Skene)\", \"ChatGPT-4o\"],\n",
    "    \"Concordanza con Gemini (%)\": [majority_vs_gemini, ds_multi_vs_gemini, ensemble_enhanced_mv_vs_gemini, ensemble_enhanced_ds_vs_gemini, gpt4o_vs_gemini]\n",
    "})\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = df[[\n",
    "    \"Majority Voting\", \n",
    "    \"Dawid & Skene Multi-Class\",\n",
    "    \"Enhanced Majority Voting\", \n",
    "    \"Ensemble Enhanced (Dawid & Skene)\",\n",
    "    \"ChatGPT-4o\", \n",
    "    \"Gemini\"\n",
    "]].map(lambda x: 1 if x == \"Sì\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Confronto tra Modelli e Gemini\")\n",
    "plt.xlabel(\"Metodo di Ensemble o Modello\")\n",
    "plt.ylabel(\"Domande\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Distribuzione delle risposte per ciascun modello**\n",
    "print(\"\\nDistribuzione delle risposte per ciascun modello:\")\n",
    "for model in model_columns_enhanced:\n",
    "    print(f\"{model}:\")\n",
    "    print(df[model].value_counts(normalize=True) * 100, \"\\n\")\n",
    "\n",
    "# **Distribuzione delle risposte per Gemini**\n",
    "print(\"\\nDistribuzione delle risposte di Gemini:\")\n",
    "print(df[\"Gemini\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# **Distribuzione delle risposte per GPT**\n",
    "print(\"\\nDistribuzione delle risposte di ChatGPT-4o:\")\n",
    "print(df[\"ChatGPT-4o\"].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo i modelli nell’ensemble\n",
    "model_columns_enhanced = ['UmBERTo', 'GePpeTto']\n",
    "def majority_voting_enhanced(row):\n",
    "    models_to_consider = model_columns_enhanced  # Updated models list\n",
    "    votes = [row[model] for model in models_to_consider]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Returns the most common choice\n",
    "\n",
    "# **Calcoliamo Majority Voting**\n",
    "df[\"Enhanced3 Majority Voting\"] = df.apply(majority_voting_enhanced, axis=1)\n",
    "\n",
    "# **Confrontiamo con Gemini**\n",
    "df[\"Agreement Enhanced3 Majority-Gemini\"] = df[\"Enhanced3 Majority Voting\"] == df[\"Gemini\"]\n",
    "\n",
    "# **Calcoliamo le percentuali di accordo**\n",
    "ensemble_enhanced_mv_vs_gemini3 = df[\"Agreement Enhanced3 Majority-Gemini\"].mean() * 100\n",
    "\n",
    "# Converti il DataFrame in lista nidificata\n",
    "dataset_list_enhanced = dataframe_to_dataset_list(df, model_columns_enhanced)\n",
    "\n",
    "# Converti in tensore NumPy per Dawid & Skene\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor_enhanced = list2array(class_num, dataset_list_enhanced)\n",
    "\n",
    "# Inizializza e lancia il modello Dawid & Skene\n",
    "ds_model_enhanced = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict_enhanced, error_rates_enhanced, worker_reliability_enhanced, predict_label_enhanced = ds_model_enhanced.run(dataset_tensor_enhanced)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers_enhanced = [\"Sì\" if p[1] > 0.5 else \"No\" for p in predict_label_enhanced]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "df[\"Ensemble Enhanced3 (Dawid & Skene)\"] = final_answers_enhanced\n",
    "\n",
    "df[\"Agreement Ensemble Enhanced3 (D&S)-Gemini\"] = df[\"Ensemble Enhanced3 (Dawid & Skene)\"] == df[\"Gemini\"]\n",
    "ensemble_enhanced_ds_vs_gemini3 = df[\"Agreement Ensemble Enhanced3 (D&S)-Gemini\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", \"Enhanced3 Majority Voting\", \"Ensemble Enhanced3 (Dawid & Skene)\", \"ChatGPT-4o\"],\n",
    "    \"Concordanza con Gemini (%)\": [majority_vs_gemini, ds_multi_vs_gemini, ensemble_enhanced_mv_vs_gemini3, ensemble_enhanced_ds_vs_gemini3, gpt4o_vs_gemini]\n",
    "})\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = df[[\n",
    "    \"Majority Voting\", \n",
    "    \"Dawid & Skene Multi-Class\",\n",
    "    \"Enhanced3 Majority Voting\", \n",
    "    \"Ensemble Enhanced3 (Dawid & Skene)\",\n",
    "    \"ChatGPT-4o\", \n",
    "    \"Gemini\"\n",
    "]].map(lambda x: 1 if x == \"Sì\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Confronto tra Modelli e Gemini\")\n",
    "plt.xlabel(\"Metodo di Ensemble o Modello\")\n",
    "plt.ylabel(\"Domande\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
