{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dawid_skene_model import list2array, DawidSkeneModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pd.read_csv(\"updated_faq.csv\")\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Concordanza di ciascun modello con il groundtruth **\n",
    "model_columns = [\"GPT\", \"DistilBERT\", \"MiniSBERT\", \"Llama\", \"Gemma\"]\n",
    "print(\"\\nAgreement of each model with the groundtruth:\")\n",
    "for model in model_columns:\n",
    "    print(f\"{model}:\")\n",
    "    qa[f\"Agreement {model}-Groundtruth\"] = qa[model] == qa[\"winner\"]\n",
    "    print(qa[f\"Agreement {model}-Groundtruth\"].mean() * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel of 3 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI AUSILIARIE\n",
    "# Converti il DataFrame in una lista nidificata (dataset_list)\n",
    "def dataframe_to_dataset_list(df, model_columns):\n",
    "    dataset_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = []\n",
    "        for model in model_columns:\n",
    "            response = row[model]  # Prendi la risposta del modello\n",
    "            task.append([0] if response == \"model_b\" else [1])  # Converti in formato numerico\n",
    "        dataset_list.append(task)\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def majority_voting_analysis(qa, model_columns):\n",
    "    # Majority Voting Function\n",
    "    def majority_voting(row, models=model_columns):\n",
    "        votes = [row[model] for model in models]\n",
    "        return Counter(votes).most_common(1)[0][0]  # Most voted option\n",
    "    \n",
    "    # Compute Majority Voting\n",
    "    qa[\"Majority Voting\"] = qa.apply(majority_voting, axis=1)\n",
    "    \n",
    "    # Convert DataFrame to dataset_list\n",
    "    dataset_list = dataframe_to_dataset_list(qa, model_columns)\n",
    "    \n",
    "    # Convert to NumPy tensor\n",
    "    class_num = 2  # Two classes: Yes (1) and No (0)\n",
    "    dataset_tensor = list2array(class_num, dataset_list)\n",
    "    \n",
    "    # Initialize and run Dawid & Skene model\n",
    "    model = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "    marginal_predict, error_rates, worker_reliability, predict_label = model.run(dataset_tensor)\n",
    "    \n",
    "    # Convert final predictions to \"Sì\" or \"No\"\n",
    "    final_answers = [\"model_a\" if p[1] > 0.5 else \"model_b\" for p in predict_label]\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    qa[\"Dawid & Skene Multi-Class\"] = final_answers\n",
    "    \n",
    "    # Compare with ground truth\n",
    "    qa[\"Agreement D&S-Groundtruth\"] = qa[\"Dawid & Skene Multi-Class\"] == qa[\"winner\"]\n",
    "    qa[\"Agreement Majority-Groundtruth\"] = qa[\"Majority Voting\"] == qa[\"winner\"]\n",
    "    \n",
    "    # Calculate agreement percentages\n",
    "    ds_truth = qa[\"Agreement D&S-Groundtruth\"].mean() * 100\n",
    "    majority_truth = qa[\"Agreement Majority-Groundtruth\"].mean() * 100\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\"],\n",
    "        \"Concordanza con Groundtruth (%)\": [majority_truth, ds_truth]\n",
    "    })\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_models(qa):\n",
    "    # Convert categorical labels to numeric\n",
    "    label_mapping = {label: idx for idx, label in enumerate(qa[\"winner\"].unique())}\n",
    "    \n",
    "    ground_truth = qa[\"winner\"].map(label_mapping)\n",
    "    majority_voting_preds = qa[\"Majority Voting\"].map(label_mapping)\n",
    "    ds_preds = qa[\"Dawid & Skene Multi-Class\"].map(label_mapping)\n",
    "\n",
    "    # Compute Cohen's Kappa\n",
    "    kappa_mv = cohen_kappa_score(majority_voting_preds, ground_truth)\n",
    "    kappa_ds = cohen_kappa_score(ds_preds, ground_truth)\n",
    "\n",
    "    # Compute Pearson Correlation\n",
    "    pearson_mv, _ = pearsonr(majority_voting_preds, ground_truth)\n",
    "    pearson_ds, _ = pearsonr(ds_preds, ground_truth)\n",
    "\n",
    "    # Compute Kendall-Tau Correlation\n",
    "    kendall_mv, _ = kendalltau(majority_voting_preds, ground_truth)\n",
    "    kendall_ds, _ = kendalltau(ds_preds, ground_truth)\n",
    "\n",
    "    # Count exact matches\n",
    "    mv_matches = (majority_voting_preds == ground_truth).sum()\n",
    "    ds_matches = (ds_preds == ground_truth).sum()\n",
    "\n",
    "    total = len(qa)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    evaluation_results = {\n",
    "        \"Metric\": [\"Cohen’s Kappa\", \"Pearson Correlation\", \"Kendall-Tau\", \"Exact Matches\"],\n",
    "        \"Majority Voting\": [kappa_mv, pearson_mv, kendall_mv, f\"{mv_matches}/{total}\"],\n",
    "        \"Dawid & Skene\": [kappa_ds, pearson_ds, kendall_ds, f\"{ds_matches}/{total}\"]\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"GPT\", \"Llama\", \"Gemma\"] # different foundations models, as in the papers\n",
    "summary = majority_voting_analysis(qa, model_columns)\n",
    "display(summary)\n",
    "\n",
    "evaluate_models(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"MiniGptBased\", \"Llama\", \"GPT\"] # best performing models, without Mixtral\n",
    "summary = majority_voting_analysis(qa, model_columns)\n",
    "display(summary)\n",
    "\n",
    "evaluate_models(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"Mixtral\", \"MiniGptBased\", \"Llama\", \"GPT\"] # best performing models\n",
    "summary = majority_voting_analysis(qa, model_columns)\n",
    "display(summary)\n",
    "\n",
    "evaluate_models(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"MiniSBERT\", \"DistilBERT\", \"Gemma\", \"DeepHaiku\"] # worst performing models\n",
    "summary = majority_voting_analysis(qa, model_columns)\n",
    "display(summary)\n",
    "\n",
    "evaluate_models(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"Mixtral\", \"Llama\", \"Gemma\", \"MiniGptBased\", \"DistilBERT\", \"MiniSBERT\", \"GPT\", \"DeepHaiku\", \"PalmBonsai\"]\n",
    "summary = majority_voting_analysis(qa, model_columns)\n",
    "display(summary)\n",
    "\n",
    "evaluate_models(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"Llama\", \"Gemma\", \"MiniGptBased\", \"DistilBERT\", \"MiniSBERT\", \"GPT\", \"DeepHaiku\", \"PalmBonsai\"]\n",
    "summary = majority_voting_analysis(qa, model_columns)\n",
    "display(summary)\n",
    "\n",
    "evaluate_models(qa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
