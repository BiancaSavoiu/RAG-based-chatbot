{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dawid_skene_model import list2array, DawidSkeneModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from huggingface_hub import login\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui l'autenticazione\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carica il dataset Chatbot Arena Conversations\n",
    "chatbot_arena = load_dataset(\"lmsys/chatbot_arena_conversations\")\n",
    "display(chatbot_arena)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the dataset for italian conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte in DataFrame per un'analisi più semplice\n",
    "df = pd.DataFrame(chatbot_arena[\"train\"])\n",
    "\n",
    "# Filtra solo le conversazioni in italiano\n",
    "df_italian = df[df[\"language\"] == \"Italian\"]\n",
    "\n",
    "# Mostra alcune righe per conferma\n",
    "display(df_italian.head())\n",
    "\n",
    "# Numero di conversazioni in italiano\n",
    "print(f\"Numero totale di conversazioni in italiano: {len(df_italian)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta quante volte ogni modello è stato dichiarato vincitore\n",
    "winner_counts = df_italian[\"winner\"].value_counts()\n",
    "\n",
    "print(\"\\nDistribuzione delle vittorie per modello:\")\n",
    "print(winner_counts)\n",
    "\n",
    "# Visualizzazione con grafico a barre\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=winner_counts.index, y=winner_counts.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Numero di vittorie per ciascun modello (italiano)\")\n",
    "plt.xlabel(\"Modelli\")\n",
    "plt.ylabel(\"Numero di vittorie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta quante volte ogni modello è stato dichiarato vincitore\n",
    "turn_counts = df_italian[\"turn\"].value_counts()\n",
    "\n",
    "print(\"\\nDistribuzione del numero di turni per le conversazioni:\")\n",
    "print(turn_counts)\n",
    "\n",
    "# Visualizzazione con grafico a barre\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=turn_counts.index, y=turn_counts.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Numero di turn per ciascuna conversazione (italiano)\")\n",
    "plt.xlabel(\"Modelli\")\n",
    "plt.ylabel(\"Numero di turn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra solo le conversazioni con turn == 1\n",
    "df_italian_turn = df_italian[df_italian[\"turn\"] == 1]\n",
    "df_italian_turn = df_italian_turn.drop(columns=[\"turn\", \"anony\", \"language\", \"tstamp\", \"openai_moderation\", \"toxic_chat_tag\", \"judge\"])\n",
    "\n",
    "#  Reset dell'indice del DataFrame dopo il filtraggio\n",
    "df_italian_turn = df_italian_turn.reset_index(drop=True)\n",
    "\n",
    "# Mostra il numero di conversazioni rimaste\n",
    "print(f\"Numero di conversazioni con un solo turno: {len(df_italian_turn)}\")\n",
    "\n",
    "# Visualizza alcune righe per conferma\n",
    "display(df_italian_turn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df_italian_turn['conversation_a'][11])\n",
    "pprint(df_italian_turn['conversation_b'][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze dataset for english conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte in DataFrame per un'analisi più semplice\n",
    "df = pd.DataFrame(chatbot_arena[\"train\"])\n",
    "\n",
    "# Filtra solo le conversazioni in inglese\n",
    "df_eng = df[df[\"language\"] == \"English\"] \n",
    "\n",
    "# Filtra solo le conversazioni con turn == 1\n",
    "df_eng = df_eng[df_eng[\"turn\"] == 1]\n",
    "\n",
    "df_eng = df_eng[(df_eng[\"winner\"] == \"model_a\") | (df_eng[\"winner\"] == \"model_b\")]\n",
    "\n",
    "df_eng = df_eng.drop(columns=[\"turn\", \"anony\", \"language\", \"tstamp\", \"openai_moderation\", \"toxic_chat_tag\", \"judge\"])\n",
    "\n",
    "# Mostra alcune righe per conferma\n",
    "display(df_eng.head())\n",
    "\n",
    "# Numero di conversazioni in inglese\n",
    "print(f\"Numero totale di conversazioni in inglese con i filtri applicati: {len(df_eng)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione con grafico a barre\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Conta quante volte ogni modello è stato dichiarato vincitore\n",
    "turn_counts = df_eng[\"model_a\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=turn_counts.index, y=turn_counts.values, hue=turn_counts.index, dodge=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Numero di turn per ciascuna conversazione (italiano)\")\n",
    "plt.xlabel(\"Modelli\")\n",
    "plt.ylabel(\"Numero di turn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Creiamo una colonna con le coppie di modelli ordinate\n",
    "df_eng[\"model_pair\"] = df_eng.apply(lambda row: tuple(sorted([row[\"model_a\"], row[\"model_b\"]])), axis=1)\n",
    "\n",
    "# Conta il numero totale di sfide per ogni coppia di modelli\n",
    "model_pair_total = df_eng[\"model_pair\"].value_counts().reset_index()\n",
    "model_pair_total.columns = [\"model_pair\", \"Total Matches\"]\n",
    "\n",
    "# Conta le vittorie di ciascun modello all'interno di ogni coppia\n",
    "model_wins = df_eng.groupby([\"model_pair\", \"winner\"]).size().reset_index(name=\"Wins\")\n",
    "\n",
    "# Separiamo le vittorie di ciascun modello\n",
    "model_wins_a = model_wins[model_wins[\"winner\"] == \"model_a\"].drop(columns=[\"winner\"]).rename(columns={\"Wins\": \"Wins_A\"})\n",
    "model_wins_b = model_wins[model_wins[\"winner\"] == \"model_b\"].drop(columns=[\"winner\"]).rename(columns={\"Wins\": \"Wins_B\"})\n",
    "\n",
    "# Merge per ottenere il totale e le vittorie di ciascun modello\n",
    "model_stats = (\n",
    "    model_pair_total\n",
    "    .merge(model_wins_a, on=\"model_pair\", how=\"left\")\n",
    "    .merge(model_wins_b, on=\"model_pair\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Sostituiamo i NaN con 0 in modo sicuro usando `.fillna()` senza inplace=True\n",
    "model_stats = model_stats.assign(\n",
    "    Wins_A=model_stats[\"Wins_A\"].fillna(0),\n",
    "    Wins_B=model_stats[\"Wins_B\"].fillna(0)\n",
    ")\n",
    "\n",
    "# Calcoliamo le percentuali di vittoria\n",
    "model_stats[\"Win_A (%)\"] = (model_stats[\"Wins_A\"] / model_stats[\"Total Matches\"]) * 100\n",
    "model_stats[\"Win_B (%)\"] = (model_stats[\"Wins_B\"] / model_stats[\"Total Matches\"]) * 100\n",
    "\n",
    "# Ordiniamo il DataFrame per il numero totale di partite giocate\n",
    "model_stats = model_stats.sort_values(by=\"Total Matches\", ascending=False)\n",
    "\n",
    "# Mostriamo le prime 10 coppie più frequenti con percentuale di vittoria\n",
    "print(\"\\nTop 10 coppie di modelli con percentuali di vittoria:\")\n",
    "display(model_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng[((df_eng[\"model_a\"] == \"vicuna-13b\") & (df_eng[\"model_b\"] == \"koala-13b\")) |\n",
    "                ((df_eng[\"model_a\"] == \"koala-13b\") & (df_eng[\"model_b\"] == \"vicuna-13b\"))]\n",
    "\n",
    "# Reset index\n",
    "df_eng = df_eng.reset_index(drop=True)\n",
    "\n",
    "# Mostra alcune righe per conferma\n",
    "display(df_eng.head())\n",
    "\n",
    "# Numero di conversazioni in inglese\n",
    "print(f\"Numero totale di conversazioni in inglese con i filtri applicati: {len(df_eng)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df_eng['conversation_a'][11])\n",
    "pprint(df_eng['conversation_b'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng['conversation_a'][0][0]['content'] #question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng['conversation_a'][0][1]['content'] #answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'content' with role 'user' as question and the one with role 'assistant' as answer for conversation 'a' and 'b'\n",
    "# Save question and answer in a dataframe\n",
    "qa = pd.DataFrame(columns=['question', 'answer_a'])\n",
    "idx = 0\n",
    "\n",
    "for elem in df_eng['conversation_a']:\n",
    "    if elem[0]['role'] == 'user':\n",
    "        qa.loc[idx] = [elem[0]['content'], elem[1]['content']]\n",
    "        idx = idx + 1\n",
    "\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add answer_b for the conversation_b of the dataset\n",
    "qa['answer_b'] = df_eng['conversation_b'].apply(lambda x: x[1]['content'])\n",
    "qa['winner'] = df_eng['winner']\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create big model reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "token_pro = os.getenv('HUGGINGFACE_TOKEN')\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=token_pro\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        model=repo_id,  # Explicitly specify the model\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1028},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Register tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "def evaluate_pair(model, row):\n",
    "    prompt = f\"\"\"\n",
    "    You are a critic evaluating two different answers to the same question on any subject.\n",
    "    Compare the two answers based on:\n",
    "    1. **Completeness**: Does the answer fully clarify the question?\n",
    "    2. **Relevance**: Is the answer relevant to the question?\n",
    "    3. **Correctness**: Is the information in the answer accurate?\n",
    "    4. **Clarity**: Is the answer clearly understandable by a human reader?\n",
    "    \n",
    "    Evaluate the following question and its two answers, and determine which is better, more correct, and understandable by a human reader.\n",
    "    Respond with 'model_a' if the first answer is better, otherwise respond with 'model_b'.\n",
    "\n",
    "    Give the response exactly in the following format, and nothing else:\n",
    "\n",
    "    Output:::\n",
    "    Best model: [model_a/model_b]\n",
    "    Output:::\n",
    "\n",
    "    Now, consider the following question and its two answers:\n",
    "    **Question:** {row['question']}\n",
    "    **Answer 1:** {row['answer_a']}\n",
    "    **Answer 2:** {row['answer_b']}\n",
    "\n",
    "    Output:::\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(model, InferenceClient):\n",
    "        response = call_llm(model, prompt)\n",
    "\n",
    "        # Extract the best model from the response\n",
    "        match = re.search(r\"Best model:\\s*(model_a|model_b)\", response)\n",
    "        return match.group(1) if match else \"Unknown\"  # Default to \"Unknown\" if parsing fails\n",
    "    else:\n",
    "        response = model.generate(row['question'], row['answer_a'], row['answer_b'])\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Valutazione con Mixtral **\n",
    "qa[\"Mixtral\"] = qa.progress_apply(lambda row: evaluate_pair(llm_client, row), axis=1)\n",
    "\n",
    "# ** Confronto tra GPT e Gemini **\n",
    "qa[\"Agreement Mixtral-Groundtruth\"] = qa[\"Mixtral\"] == qa[\"winner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa['Mixtral'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the rows of the dataframe 'qa' that have the value 'Unknown' in the column 'Mixtral'\n",
    "qa = qa[qa['Mixtral'] != 'Unknown']\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset qa index\n",
    "qa = qa.reset_index(drop=True)\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Mixtral correct judgment\n",
    "mixtral_vs_groundtruth = qa[\"Agreement Mixtral-Groundtruth\"].mean() * 100\n",
    "print(\"Correct judgment Mixtral:\", mixtral_vs_groundtruth)\n",
    "\n",
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = qa[[\n",
    "    \"winner\", \n",
    "    \"Mixtral\"\n",
    "]].map(lambda x: 1 if x == \"model_a\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Compare Mixtral judgment with the groundtruth\")\n",
    "plt.xlabel(\"Mixtral model\")\n",
    "plt.ylabel(\"Groundtruth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "qa.to_csv(\"faq_evaluation_results_with_Mixtral.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models - Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ** Ensemble Model Configuration **\n",
    "MODEL_NAMES = {\n",
    "    \"DistilBERT\": \"distilbert/distilbert-base-uncased\", \n",
    "    \"MiniSBERT\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"PalmBonsai\": \"Writer/palmyra-small\", \n",
    "    \"MiniGptBased\": \"ComCom/gpt2-small\"\n",
    "}\n",
    "\n",
    "class HuggingFaceModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = token_pro)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, use_auth_token = token_pro)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def generate(self, question, answer1, answer2):\n",
    "        inputs1 = self.tokenizer(f\"Question: {question} Answer: {answer1}\", return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
    "        inputs2 = self.tokenizer(f\"Question: {question} Answer: {answer2}\", return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits1 = self.model(**inputs1).logits\n",
    "            logits2 = self.model(**inputs2).logits\n",
    "        \n",
    "        score1 = torch.softmax(logits1, dim=1)[0][1].item()\n",
    "        score2 = torch.softmax(logits2, dim=1)[0][1].item()\n",
    "        \n",
    "        return \"model_a\" if score1 > score2 else \"model_b\"\n",
    "\n",
    "# ** Initialize Ensemble Models **\n",
    "ensemble_models = {name: HuggingFaceModel(model) for name, model in MODEL_NAMES.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Enable progress tracking **\n",
    "tqdm.pandas()\n",
    "\n",
    "# ** Evaluation with ensemble models **\n",
    "for model_name, model in ensemble_models.items():\n",
    "    qa[model_name] = qa.progress_apply(lambda row: evaluate_pair(model, row), axis=1)\n",
    "\n",
    "# ** Majority Voting Function **\n",
    "def majority_voting(row):\n",
    "    votes = [row[model] for model in MODEL_NAMES.keys()]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Most voted option\n",
    "\n",
    "# ** Compute Majority Voting **\n",
    "qa[\"Majority Voting\"] = qa.apply(majority_voting, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Compare with groundtruth **\n",
    "qa[\"Agreement Majority-Mixtral\"] = qa[\"Majority Voting\"] == qa[\"Mixtral\"]\n",
    "\n",
    "# ** Compute Agreement Percentage **\n",
    "majority_vs_mixtral = qa[\"Agreement Majority-Mixtral\"].mean() * 100\n",
    "\n",
    "# ** Encode Answers for Consensus Analysis **\n",
    "def encode_answers(qa):\n",
    "    return qa.replace({\"model_a\": 1, \"model_b\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encode_answers(qa.iloc[:, 3:-1]).apply(pd.to_numeric, errors=\"coerce\")\n",
    "display(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci le colonne dei modelli\n",
    "model_columns = [\"DistilBERT\", \"MiniSBERT\", \"PalmBonsai\", \"MiniGptBased\"]\n",
    "\n",
    "# FUNZIONI AUSILIARIE\n",
    "# Converti il DataFrame in una lista nidificata (dataset_list)\n",
    "def dataframe_to_dataset_list(df, model_columns):\n",
    "    dataset_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = []\n",
    "        for model in model_columns:\n",
    "            response = row[model]  # Prendi la risposta del modello\n",
    "            task.append([0] if response == \"model_b\" else [1])  # Converti in formato numerico\n",
    "        dataset_list.append(task)\n",
    "    return dataset_list\n",
    "\n",
    "# Converti il DataFrame in dataset_list\n",
    "dataset_list = dataframe_to_dataset_list(qa, model_columns)\n",
    "\n",
    "# Converti in tensore NumPy\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor = list2array(class_num, dataset_list)\n",
    "\n",
    "# Inizializza e lancia il modello di Dawid & Skene\n",
    "model = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict, error_rates, worker_reliability, predict_label = model.run(dataset_tensor)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers = [\"model_a\" if p[1] > 0.5 else \"model_b\" for p in predict_label]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "qa[\"Dawid & Skene Multi-Class\"] = final_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra il confronto con Majority Voting e Gemini\n",
    "qa[\"Agreement D&S-Mixtral\"] = qa[\"Dawid & Skene Multi-Class\"] == qa[\"Mixtral\"]\n",
    "\n",
    "# Calcola le percentuali di accordo\n",
    "ds_multi_vs_gemini = qa[\"Agreement D&S-Mixtral\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", ],\n",
    "    \"Concordanza con Groundtruth (%)\": [majority_vs_mixtral, ds_multi_vs_gemini]\n",
    "})\n",
    "\n",
    "display(summary_df)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = qa[[\n",
    "    \"Majority Voting\", \n",
    "    \"Dawid & Skene Multi-Class\"\n",
    "]].map(lambda x: 1 if x == \"model_a\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Confronto tra Majority Voting e Dawid & Skene\")\n",
    "plt.xlabel(\"Metodo di Ensemble o Modello\")\n",
    "plt.ylabel(\"Domande\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the results with the original groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra il confronto con Majority Voting e Gemini\n",
    "qa[\"Agreement D&S-Groundtruth\"] = qa[\"Dawid & Skene Multi-Class\"] == qa[\"winner\"]\n",
    "qa[\"Agreement Majority-Groundtruth\"] = qa[\"Majority Voting\"] == qa[\"winner\"]\n",
    "\n",
    "# Calcola le percentuali di accordo\n",
    "ds_truth = qa[\"Agreement D&S-Groundtruth\"].mean() * 100\n",
    "majority_truth = qa[\"Agreement Majority-Groundtruth\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", \"Mixtral\"],\n",
    "    \"Concordanza con Groundtruth (%)\": [majority_truth, ds_truth, mixtral_vs_groundtruth]\n",
    "})\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Distribuzione delle risposte per ciascun modello**\n",
    "print(\"\\nDistribuzione delle risposte per ciascun modello:\")\n",
    "for model in model_columns:\n",
    "    print(f\"{model}:\")\n",
    "    print(qa[model].value_counts(normalize=True) * 100, \"\\n\")\n",
    "\n",
    "# **Distribuzione delle risposte per Mixtral**\n",
    "print(\"\\nDistribuzione delle risposte di Mixtral:\")\n",
    "print(qa[\"Mixtral\"].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoLL with bigger models to improve the results of Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ** Ensemble Model Configuration **\n",
    "MODEL_NAMES_NEW = {\n",
    "    \"GPT\": \"openai-community/gpt2\",\n",
    "    \"DistilBERT\": \"distilbert/distilbert-base-uncased\",\n",
    "    \"MiniSBERT\": \"sentence-transformers/all-MiniLM-L6-v2\", \n",
    "    \"PalmBonsai\": \"Writer/palmyra-small\", \n",
    "    \"MiniGptBased\": \"ComCom/gpt2-small\", \n",
    "    \"DeepHaiku\": \"fabianmmueller/deep-haiku-gpt-2\", \n",
    "    #\"Llama\": \"meta-llama/Llama-3.2-1B\"\n",
    "}\n",
    "\n",
    "class HuggingFaceModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = token_pro)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, use_auth_token = token_pro)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def generate(self, question, answer1, answer2):\n",
    "        inputs1 = self.tokenizer(f\"Question: {question} Answer: {answer1}\", return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
    "        inputs2 = self.tokenizer(f\"Question: {question} Answer: {answer2}\", return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits1 = self.model(**inputs1).logits\n",
    "            logits2 = self.model(**inputs2).logits\n",
    "        \n",
    "        score1 = torch.softmax(logits1, dim=1)[0][1].item()\n",
    "        score2 = torch.softmax(logits2, dim=1)[0][1].item()\n",
    "        \n",
    "        return \"model_a\" if score1 > score2 else \"model_b\"\n",
    "\n",
    "# ** Initialize Ensemble Models **\n",
    "ensemble_models = {name: HuggingFaceModel(model) for name, model in MODEL_NAMES_NEW.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file \"faq_evaluation_results_with_Mixtral.csv\"\n",
    "qa = pd.read_csv(\"faq_evaluation_results_with_Mixtral.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Enable progress tracking **\n",
    "tqdm.pandas()\n",
    "\n",
    "# ** Evaluation with ensemble models **\n",
    "for model_name, model in ensemble_models.items():\n",
    "    qa[model_name] = qa.progress_apply(lambda row: evaluate_pair(model, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Majority Voting Function **\n",
    "models = ['GPT', 'DistilBERT', 'Mixtral', 'PalmBonsai', 'MiniGptBased']\n",
    "def majority_voting(row, models = models):\n",
    "    votes = [row[model] for model in models]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Most voted option\n",
    "\n",
    "# ** Compute Majority Voting **\n",
    "qa[\"Majority Voting\"] = qa.apply(majority_voting, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci le colonne dei modelli\n",
    "model_columns = models\n",
    "\n",
    "# FUNZIONI AUSILIARIE\n",
    "# Converti il DataFrame in una lista nidificata (dataset_list)\n",
    "def dataframe_to_dataset_list(df, model_columns):\n",
    "    dataset_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = []\n",
    "        for model in model_columns:\n",
    "            response = row[model]  # Prendi la risposta del modello\n",
    "            task.append([0] if response == \"model_b\" else [1])  # Converti in formato numerico\n",
    "        dataset_list.append(task)\n",
    "    return dataset_list\n",
    "\n",
    "# Converti il DataFrame in dataset_list\n",
    "dataset_list = dataframe_to_dataset_list(qa, model_columns)\n",
    "\n",
    "# Converti in tensore NumPy\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor = list2array(class_num, dataset_list)\n",
    "\n",
    "# Inizializza e lancia il modello di Dawid & Skene\n",
    "model = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict, error_rates, worker_reliability, predict_label = model.run(dataset_tensor)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers = [\"model_a\" if p[1] > 0.5 else \"model_b\" for p in predict_label]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "qa[\"Dawid & Skene Multi-Class\"] = final_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv file with all results\n",
    "qa.to_csv(\"faq_evaluation_results_with_Mixtral_and_Ensemble.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "qa = pd.read_csv(\"faq_evaluation_results_with_Mixtral_and_Ensemble.csv\")\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Mixtral correct judgment\n",
    "mixtral_vs_groundtruth = qa[\"Agreement Mixtral-Groundtruth\"].mean() * 100\n",
    "print(\"Correct judgment Mixtral:\", mixtral_vs_groundtruth)\n",
    "\n",
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = qa[[\n",
    "    \"winner\", \n",
    "    \"Mixtral\"\n",
    "]].map(lambda x: 1 if x == \"model_a\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Compare Mixtral judgment with the groundtruth\")\n",
    "plt.xlabel(\"Mixtral model\")\n",
    "plt.ylabel(\"Groundtruth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra il confronto con Majority Voting e Gemini\n",
    "qa[\"Agreement D&S-Groundtruth\"] = qa[\"Dawid & Skene Multi-Class\"] == qa[\"winner\"]\n",
    "qa[\"Agreement Majority-Groundtruth\"] = qa[\"Majority Voting\"] == qa[\"winner\"]\n",
    "\n",
    "# Calcola le percentuali di accordo\n",
    "ds_truth = qa[\"Agreement D&S-Groundtruth\"].mean() * 100\n",
    "majority_truth = qa[\"Agreement Majority-Groundtruth\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", \"Mixtral\"],\n",
    "    \"Concordanza con Groundtruth (%)\": [majority_truth, ds_truth, mixtral_vs_groundtruth]\n",
    "})\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Distribuzione delle risposte per ciascun modello**\n",
    "print(\"\\nDistribuzione delle risposte per ciascun modello:\")\n",
    "for model in model_columns:\n",
    "    print(f\"{model}:\")\n",
    "    print(qa[model].value_counts(normalize=True) * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Concordanza di ciascun modello con il groundtruth **\n",
    "print(\"\\nConcordanza di ciascun modello con il groundtruth:\")\n",
    "for model in model_columns:\n",
    "    print(f\"{model}:\")\n",
    "    qa[f\"Agreement {model}-Groundtruth\"] = qa[model] == qa[\"winner\"]\n",
    "    print(qa[f\"Agreement {model}-Groundtruth\"].mean() * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze models correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file \"faq_evaluation_results_with_Mixtral.csv\"\n",
    "qa = pd.read_csv(\"updated_faq.csv\")\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Concordanza di ciascun modello con il groundtruth **\n",
    "model_columns = [\"Mixtral\", \"GPT\", \"DistilBERT\", \"MiniSBERT\", \"PalmBonsai\", \"MiniGptBased\", \"DeepHaiku\", \"Llama\", \"Gemma\"]\n",
    "print(\"\\nConcordanza di ciascun modello con il groundtruth:\")\n",
    "for model in model_columns:\n",
    "    print(f\"{model}:\")\n",
    "    qa[f\"Agreement {model}-Groundtruth\"] = qa[model] == qa[\"winner\"]\n",
    "    print(qa[f\"Agreement {model}-Groundtruth\"].mean() * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Majority Voting Function **\n",
    "models = [\"Mixtral\", \"GPT\", \"DistilBERT\", \"MiniSBERT\", \"PalmBonsai\", \"MiniGptBased\", \"DeepHaiku\", \"Llama\", \"Gemma\"]\n",
    "def majority_voting(row, models = models):\n",
    "    votes = [row[model] for model in models]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Most voted option\n",
    "\n",
    "# ** Compute Majority Voting **\n",
    "qa[\"Majority Voting\"] = qa.apply(majority_voting, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci le colonne dei modelli\n",
    "model_columns = models\n",
    "\n",
    "# FUNZIONI AUSILIARIE\n",
    "# Converti il DataFrame in una lista nidificata (dataset_list)\n",
    "def dataframe_to_dataset_list(df, model_columns):\n",
    "    dataset_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = []\n",
    "        for model in model_columns:\n",
    "            response = row[model]  # Prendi la risposta del modello\n",
    "            task.append([0] if response == \"model_b\" else [1])  # Converti in formato numerico\n",
    "        dataset_list.append(task)\n",
    "    return dataset_list\n",
    "\n",
    "# Converti il DataFrame in dataset_list\n",
    "dataset_list = dataframe_to_dataset_list(qa, model_columns)\n",
    "\n",
    "# Converti in tensore NumPy\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor = list2array(class_num, dataset_list)\n",
    "\n",
    "# Inizializza e lancia il modello di Dawid & Skene\n",
    "model = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict, error_rates, worker_reliability, predict_label = model.run(dataset_tensor)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers = [\"model_a\" if p[1] > 0.5 else \"model_b\" for p in predict_label]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "qa[\"Dawid & Skene Multi-Class\"] = final_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Mixtral correct judgment\n",
    "mixtral_vs_groundtruth = qa[\"Agreement Mixtral-Groundtruth\"].mean() * 100\n",
    "print(\"Correct judgment Mixtral:\", mixtral_vs_groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra il confronto con Majority Voting e Gemini\n",
    "qa[\"Agreement D&S-Groundtruth\"] = qa[\"Dawid & Skene Multi-Class\"] == qa[\"winner\"]\n",
    "qa[\"Agreement Majority-Groundtruth\"] = qa[\"Majority Voting\"] == qa[\"winner\"]\n",
    "\n",
    "# Calcola le percentuali di accordo\n",
    "ds_truth = qa[\"Agreement D&S-Groundtruth\"].mean() * 100\n",
    "majority_truth = qa[\"Agreement Majority-Groundtruth\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", \"Mixtral\"],\n",
    "    \"Concordanza con Groundtruth (%)\": [majority_truth, ds_truth, mixtral_vs_groundtruth]\n",
    "})\n",
    "\n",
    "display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
