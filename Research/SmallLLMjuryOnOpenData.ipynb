{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dawid_skene_model import list2array, DawidSkeneModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from huggingface_hub import login\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=os.getenv('HUGGINGFACE_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomedical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds[\"test\"])\n",
    "# Keep only columns of 'question' and 'answers'\n",
    "qa = df[['question', 'answer']]\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create big model reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "token_pro = os.getenv('HUGGINGFACE_TOKEN')\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=token_pro\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        model=repo_id,  # Explicitly specify the model\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1028},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Register tqdm with pandas\n",
    "tqdm.pandas()\n",
    "    \n",
    "def evaluate_pair(model, row):\n",
    "    prompt = f\"\"\"You are a reviewer evaluating question-answer pairs for a biomedical FAQ.\n",
    "        The question-answer pairs must meet the following criteria to be considered useful for evaluating a chatbot designed for biomedical customer support:\n",
    "\n",
    "        1. **Relevance**: They should address topics relevant to biomedical research, healthcare, or medical information.\n",
    "        2. **Logicality and Usefulness**: They should be logical and provide clear, practical information for users seeking biomedical knowledge.\n",
    "        3. **Correctness and Clarity**: They should give correct and accurate information, fully clarify the question, and be understandable by a human reader.\n",
    "\n",
    "        Evaluate the following question-answer pair and decide if it is useful for testing a biomedical support chatbot.  \n",
    "        Respond exclusively with 'Yes' or 'No'.\n",
    "\n",
    "        **Question:** {row['question']}  \n",
    "        **Answer:** {row['answer']}  \n",
    "\n",
    "        Respond exclusively with 'Yes' or 'No'.\"\"\"\n",
    "\n",
    "    if isinstance(model, InferenceClient):\n",
    "        response = call_llm(model, prompt)  # Now passing a string, not a list\n",
    "        answer = response.strip()\n",
    "        return \"Yes\" if \"Yes\" in answer else \"No\"\n",
    "    else:\n",
    "        return model.generate(row['question'], row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Valutazione con Mixtral **\n",
    "qa[\"Mixtral\"] = qa.progress_apply(lambda row: evaluate_pair(llm_client, row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa['Mixtral'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Confronto tra GPT e Gemini **\n",
    "for elem in qa['Mixtral']:\n",
    "    if elem == \"Yes\":\n",
    "        qa[\"Agreement Mixtral-Groundtruth\"] = 1\n",
    "    else:\n",
    "        qa[\"Agreement Mixtral-Groundtruth\"] = 0\n",
    "\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Mixtral correct judgment\n",
    "mixtral_vs_groundtruth = qa[\"Agreement Mixtral-Groundtruth\"].mean() * 100\n",
    "print(\"Correct judgment Mixtral:\", mixtral_vs_groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "qa.to_csv(\"faq_evaluation_results_with_Mixtral_on_open_data_biomedical.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models - Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "qa = pd.read_csv(\"faq_evaluation_results_with_Mixtral_on_open_data_biomedical.csv\")\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ** Ensemble Model Configuration **\n",
    "MODEL_NAMES = {\n",
    "    \"DistilBERT\": \"distilbert/distilbert-base-uncased\", \n",
    "    \"MiniSBERT\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"MiniGptBased\": \"ComCom/gpt2-small\", \n",
    "    \"T5\": \"google-t5/t5-small\"\n",
    "}\n",
    "\n",
    "class HuggingFaceModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        # **Aggiungi un token di padding se non presente**\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))  # Aggiorna la dimensione dei token nel modello\n",
    "\n",
    "    def generate(self, question, answer):\n",
    "        \"\"\"Esegue la valutazione del modello sulla coppia domanda-risposta.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            f\"Question: {question} Answer: {answer}\", \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,  \n",
    "            max_length=512,  \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        return \"Yes\" if predicted_class == 1 else \"No\"\n",
    "\n",
    "# ** Inizializzazione dei modelli dell'Ensemble **\n",
    "ensemble_models = {name: HuggingFaceModel(model) for name, model in MODEL_NAMES.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save on csv also the ensemble models\n",
    "qa.to_csv(\"faq_evaluation_results_with_ensemble_models_on_open_data_biomedical.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Enable progress tracking **\n",
    "tqdm.pandas()\n",
    "\n",
    "# ** Evaluation with ensemble models **\n",
    "for model_name, model in ensemble_models.items():\n",
    "    qa[model_name] = qa.progress_apply(lambda row: evaluate_pair(model, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Majority Voting Function **\n",
    "def majority_voting(row):\n",
    "    votes = [row[model] for model in MODEL_NAMES.keys()]\n",
    "    return Counter(votes).most_common(1)[0][0]  # Most voted option\n",
    "\n",
    "# ** Compute Majority Voting **\n",
    "qa[\"Majority Voting\"] = qa.apply(majority_voting, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Compare with groundtruth **\n",
    "qa[\"Agreement Majority-Mixtral\"] = qa[\"Majority Voting\"] == qa[\"Mixtral\"]\n",
    "\n",
    "# ** Compute Agreement Percentage **\n",
    "majority_vs_mixtral = qa[\"Agreement Majority-Mixtral\"].mean() * 100\n",
    "print(\"Correct judgment Majority Voting vs Mixtral:\", majority_vs_mixtral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Encoding delle risposte per l'analisi di consenso **\n",
    "def encode_answers(df):\n",
    "    return df.replace({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "#encoded_df = encode_answers(df.iloc[:, 1:-1])  # Escludiamo la colonna delle domande\n",
    "encoded_df = encode_answers(qa.iloc[:, 2:-1]).apply(pd.to_numeric, errors=\"coerce\")\n",
    "display(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Distribuzione delle risposte per ciascun modello**\n",
    "print(\"\\nDistribuzione delle risposte per ciascun modello:\")\n",
    "for model in MODEL_NAMES.keys():\n",
    "    print(f\"{model}:\")\n",
    "    print(qa[model].value_counts(normalize=True) * 100, \"\\n\")\n",
    "\n",
    "# **Distribuzione delle risposte per Mixtral**\n",
    "print(\"\\nDistribuzione delle risposte di Mixtral:\")\n",
    "print(qa[\"Mixtral\"].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci le colonne dei modelli\n",
    "model_columns = [\"DistilBERT\", \"MiniSBERT\", \"MiniGptBased\", \"T5\"]\n",
    "\n",
    "# FUNZIONI AUSILIARIE\n",
    "# Converti il DataFrame in una lista nidificata (dataset_list)\n",
    "def dataframe_to_dataset_list(df, model_columns):\n",
    "    dataset_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        task = []\n",
    "        for model in model_columns:\n",
    "            response = row[model]  # Prendi la risposta del modello\n",
    "            task.append([0] if response == \"No\" else [1])  # Converti in formato numerico\n",
    "        dataset_list.append(task)\n",
    "    return dataset_list\n",
    "\n",
    "# Converti il DataFrame in dataset_list\n",
    "dataset_list = dataframe_to_dataset_list(qa, model_columns)\n",
    "\n",
    "# Converti in tensore NumPy\n",
    "class_num = 2  # Solo due classi: Sì (1) e No (0)\n",
    "dataset_tensor = list2array(class_num, dataset_list)\n",
    "\n",
    "# Inizializza e lancia il modello di Dawid & Skene\n",
    "model = DawidSkeneModel(class_num=2, max_iter=40, tolerance=1e-5)\n",
    "marginal_predict, error_rates, worker_reliability, predict_label = model.run(dataset_tensor)\n",
    "\n",
    "# Converti le predizioni finali in \"Sì\" o \"No\"\n",
    "final_answers = [\"Yes\" if p[1] > 0.5 else \"No\" for p in predict_label]\n",
    "\n",
    "# Aggiungi i risultati al DataFrame\n",
    "qa[\"Dawid & Skene Multi-Class\"] = final_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra il confronto con Majority Voting e Gemini\n",
    "qa[\"Agreement D&S-Mixtral\"] = qa[\"Dawid & Skene Multi-Class\"] == qa[\"Mixtral\"]\n",
    "\n",
    "# Calcola le percentuali di accordo\n",
    "ds_multi_vs_gemini = qa[\"Agreement D&S-Mixtral\"].mean() * 100\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metodo\": [\"Majority Voting\", \"Dawid & Skene Multi-Class\", ],\n",
    "    \"Concordanza con Groundtruth (%)\": [majority_vs_mixtral, ds_multi_vs_gemini]\n",
    "})\n",
    "\n",
    "display(summary_df)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertiamo le risposte in valori numerici per la heatmap\n",
    "heatmap_df = qa[[\n",
    "    \"Majority Voting\", \n",
    "    \"Dawid & Skene Multi-Class\"\n",
    "]].map(lambda x: 1 if x == \"Yes\" else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Confronto tra Majority Voting e Dawid & Skene\")\n",
    "plt.xlabel(\"Metodo di Ensemble o Modello\")\n",
    "plt.ylabel(\"Domande\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
