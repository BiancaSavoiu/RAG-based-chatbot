{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import os\n",
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter generated testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset = Dataset.load_from_disk(\"eval_dataset\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_syntethic = eval_dataset.to_pandas()\n",
    "eval_df_syntethic = eval_df_syntethic[[\"question\", \"answer\", \"source_doc\", \"context\", \"chunk_num\"]]\n",
    "print(len(eval_df_syntethic))\n",
    "display(eval_df_syntethic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset_adjacent_chunks = Dataset.load_from_disk(\"eval_dataset_adjacent_chunks\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_adjacent_chunks_syntethic = eval_dataset_adjacent_chunks.to_pandas()\n",
    "eval_df_adjacent_chunks_syntethic = eval_df_adjacent_chunks_syntethic[[\"question\", \"answer\", \"source_doc\", \"context\", \"chunk_num\"]]\n",
    "print(len(eval_df_adjacent_chunks_syntethic))\n",
    "display(eval_df_adjacent_chunks_syntethic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset_random_chunks = Dataset.load_from_disk(\"eval_dataset_random_chunks\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_random_chunks_syntethic = eval_dataset_random_chunks.to_pandas()\n",
    "eval_df_random_chunks_syntethic = eval_df_random_chunks_syntethic[[\"question\", \"answer\", \"source_doc\", \"context\", \"chunk_num\"]]\n",
    "print(len(eval_df_random_chunks_syntethic))\n",
    "display(eval_df_random_chunks_syntethic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE ROWS WITH NAN CHUNK NUM \n",
    "# Remove rows where 'column_name' contains NaN values\n",
    "eval_df_syntethic = eval_df_syntethic[eval_df_syntethic['chunk_num'].notna()]\n",
    "\n",
    "# Optionally, reset the index of the new dataframe (to avoid gaps in index after removal)\n",
    "eval_df_syntethic = eval_df_syntethic.reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "eval_df_syntethic['chunk_num'] = eval_df_syntethic['chunk_num'].apply(lambda x: [int(x)])\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(len(eval_df_syntethic))\n",
    "display(eval_df_syntethic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames\n",
    "df = pd.concat([\n",
    "    eval_df_random_chunks_syntethic, \n",
    "    eval_df_syntethic, \n",
    "    eval_df_adjacent_chunks_syntethic\n",
    "], ignore_index=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big LLM - Gemini 1.5 pro latest to filter the good QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Initialize the Gemini model\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def evaluate_pair(row):\n",
    "    # Costruisci un input strutturato per il modello\n",
    "    messages = [\n",
    "    SystemMessage(content=\"Sei un critico che valuta coppie di domande e risposte per una FAQ di un software gestionale. \\\n",
    "        Le coppie di domande e risposte devono soddisfare i seguenti criteri per essere considerate utili per valutare un chatbot destinato al supporto clienti di un software gestionale: \\\n",
    "        1. **Rilevanza**: Devono affrontare temi rilevanti per gli utenti di un software gestionale. \\\n",
    "        2. **Logicità e utilità**: Devono essere logiche e utili per fornire informazioni chiare e pratiche agli utenti. \\\n",
    "        Valuta la seguente coppia e decidi se è utile per testare un chatbot per il supporto clienti. Fornisci un feedback strutturato e dettagliato seguendo il formato specificato.\"),\n",
    "    HumanMessage(content=f\"**Domanda:** {row['question']}\\n**Risposta:** {row['answer']}\\n\\nValuta questa coppia di domanda-risposta e fornisci un feedback nel seguente formato:\\n\\\n",
    "        [Sì/No] \\\n",
    "                \\\n",
    "        Spiegazione delle risposta:\\\n",
    "        - **Motivazione del perchè la coppia domanda-risposta è considerata utile o meno** \\\n",
    "        - **Motivazione sulla rilevanza**: [Breve spiegazione, se rilevante o non rilevante] \\\n",
    "        - **Motivazione sulla logicità**: [Breve spiegazione, se logica e utile o meno] \\founda\n",
    "        Indica anche eventuali miglioramenti necessari nella domanda o nella risposta.\")\n",
    "    ]\n",
    "\n",
    "    # Invia l'input strutturato al modello\n",
    "    response = model_gemini(messages)\n",
    "    return response.content[:8]  # Estrai l'output del modello\n",
    "\n",
    "# Apply the evaluation function to each row\n",
    "df[\"feedback\"] = df.progress_apply(evaluate_pair, axis=1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "df = pd.read_csv(\"filtered_testset_withGemini.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Sì' is present in the 'feedback' column\n",
    "filtered_testset = df[df['feedback'].str.contains('Sì', na=False)]\n",
    "\n",
    "# Reset the index if desired\n",
    "filtered_testset = filtered_testset.reset_index(drop=True)\n",
    "filtered_testset = filtered_testset.drop(columns=\"feedback\")\n",
    "filtered_testset = filtered_testset.drop(columns=\"source_doc\")\n",
    "\n",
    "# Display the filtered dataframe\n",
    "display(filtered_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "extract = random.randint(0, len(filtered_testset))\n",
    "pprint.pprint(filtered_testset['question'][extract])\n",
    "pprint.pprint(filtered_testset['answer'][extract])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import also semantic testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_semantic_dataset = Dataset.load_from_disk(\"eval_semantic_dataset_random_chunks\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_semantic_df_syntethic = eval_semantic_dataset.to_pandas()\n",
    "eval_semantic_df_syntethic = eval_semantic_df_syntethic[[\"question\", \"answer\", \"context\", \"chunk_num\"]]\n",
    "print(len(eval_semantic_df_syntethic))\n",
    "display(eval_semantic_df_syntethic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of smaller LLMs, small models to filter good QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model4 = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed evaluation set to reduce the bias in choosing the chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation testset created with chunks from recursive splitting technique\n",
    "df_recursive_splitting =  pd.read_csv('filtered_matching_questions.csv')\n",
    "df_recursive_splitting = df_recursive_splitting.drop(columns = \"source_doc\")\n",
    "df_recursive_splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "extract = random.randint(0, len(df_recursive_splitting))\n",
    "pprint.pprint(df_recursive_splitting['question'][extract])\n",
    "pprint.pprint(df_recursive_splitting['answer'][extract])\n",
    "print(\"\\n\\n-------------------------------------------------------------------------------\\nContext:\")\n",
    "pprint.pprint(df_recursive_splitting['context'][extract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_semantic_dataset = Dataset.load_from_disk(\"eval_semantic_dataset_random_chunks\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_semantic_df_syntethic = eval_semantic_dataset.to_pandas()\n",
    "eval_semantic_df_syntethic = eval_semantic_df_syntethic[[\"question\", \"answer\", \"context\", \"chunk_num\"]]\n",
    "print(len(eval_semantic_df_syntethic))\n",
    "display(eval_semantic_df_syntethic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "extract = random.randint(0, len(eval_semantic_df_syntethic))\n",
    "pprint.pprint(eval_semantic_df_syntethic['question'][extract])\n",
    "pprint.pprint(eval_semantic_df_syntethic['answer'][extract])\n",
    "print(\"\\n\\n-------------------------------------------------------------------------------\\nContext:\")\n",
    "pprint.pprint(eval_semantic_df_syntethic['context'][extract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames\n",
    "df_semantic = pd.concat([\n",
    "    eval_semantic_df_syntethic\n",
    "], ignore_index=True)\n",
    "\n",
    "display(df_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the semantic testset using a big LLM as Gemini-1.5\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Initialize the Gemini model\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def evaluate_pair(row):\n",
    "    # Costruisci un input strutturato per il modello\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Sei un critico che valuta coppie di domande e risposte per una FAQ di un software gestionale. \\\n",
    "        Le coppie di domande e risposte devono soddisfare i seguenti criteri per essere considerate utili per valutare un chatbot destinato al supporto clienti di un software gestionale: \\\n",
    "        1. **Rilevanza**: Devono affrontare temi rilevanti per gli utenti di un software gestionale. \\\n",
    "        2. **Logicità e utilità**: Devono essere logiche e utili per fornire informazioni chiare e pratiche agli utenti. \\\n",
    "        Valuta la seguente coppia e decidi se è utile per testare un chatbot per il supporto clienti. Fornisci un feedback strutturato e dettagliato seguendo il formato specificato.\"),\n",
    "        HumanMessage(content=(\n",
    "            f\"**Domanda:** {row['question']}\\n**Risposta:** {row['answer']}\\n\\n\"\n",
    "            \"Valuta questa coppia di domanda-risposta e fornisci un feedback nel seguente formato:\\n\"\n",
    "            \"[Sì/No]\\n\"\n",
    "            \"Spiegazione delle risposta:\\n\"\n",
    "            \"- **Motivazione del perchè la coppia domanda-risposta è considerata utile o meno**\\n\"\n",
    "            \"- **Motivazione sulla rilevanza**: [Breve spiegazione, se rilevante o non rilevante]\\n\"\n",
    "            \"- **Motivazione sulla logicità**: [Breve spiegazione, se logica e utile o meno]\\n\"\n",
    "            \"Indica anche eventuali miglioramenti necessari nella domanda o nella risposta.\"\n",
    "        ))\n",
    "    ]\n",
    "\n",
    "    # Invia l'input strutturato al modello\n",
    "    response = model_gemini(messages)\n",
    "    return response.content[:8]  # Estrai l'output del modello\n",
    "\n",
    "\n",
    "# Apply the evaluation function to each row\n",
    "df_semantic[\"feedback\"] = df_semantic.progress_apply(evaluate_pair, axis=1)\n",
    "display(df_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic.to_csv(\"filtered_semantic_testset_withGemini\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Sì' is present in the 'feedback' column\n",
    "filtered_semantic_testset = df_semantic[df_semantic['feedback'].str.contains('Sì', na=False)]\n",
    "\n",
    "# Reset the index if desired\n",
    "filtered_semantic_testset = filtered_semantic_testset.reset_index(drop=True)\n",
    "filtered_semantic_testset = filtered_semantic_testset.drop(columns=\"feedback\")\n",
    "\n",
    "# Display the filtered dataframe\n",
    "display(filtered_semantic_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "extract = random.randint(0, len(filtered_semantic_testset))\n",
    "pprint.pprint(filtered_semantic_testset['question'][extract])\n",
    "pprint.pprint(filtered_semantic_testset['answer'][extract])\n",
    "print(\"\\n\\n-------------------------------------------------------------------------------\\nContext:\")\n",
    "pprint.pprint(filtered_semantic_testset['context'][extract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data extraction for final mix evaluation set\n",
    "# Assume df is your DataFrame\n",
    "random_50_rows_recursive_splitting = df_recursive_splitting.sample(n=50, random_state=42, ignore_index=True)\n",
    "\n",
    "# Display the extracted rows\n",
    "display(random_50_rows_recursive_splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data extraction for final mix evaluation set\n",
    "# Assume df is your DataFrame\n",
    "random_50_rows_semantic_splitting = filtered_semantic_testset.sample(n=50, random_state=42, ignore_index=True)\n",
    "\n",
    "# Display the extracted rows\n",
    "display(random_50_rows_semantic_splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat([\n",
    "    random_50_rows_recursive_splitting, \n",
    "    random_50_rows_semantic_splitting\n",
    "], ignore_index=True)\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"final_mixed_recursive_semantic_evaluation_set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
