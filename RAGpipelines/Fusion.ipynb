{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved Naive RAG:\n",
    "- image augmented dataset\n",
    "- simple preprocessing\n",
    "- optimized recursive chunking (size 1500, overlap 150)\n",
    "- hybrid retriever - dense vectors and TF-IDF with k = 4 (sparse, weight 0.2) + 2 (dense, weight 0.8)\n",
    "- bge-m3\n",
    "- question re-writing\n",
    "- reranking/filtering\n",
    "- RRF\n",
    "- prompting\n",
    "- Gemini LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset - vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "    \n",
    "embd = M3EmbeddingFP16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the documents without any data preprocessing steps\n",
    "vectorstore = FAISS.load_local(\"cleaned_recursive_augmented_faiss_index\", embd, allow_dangerous_deserialization=True)\n",
    "vectorstore, vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "csv_input_path = os.path.dirname(path) + \"/augmented_dataset_final_outputs.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_input_path, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the contents\n",
    "display(df)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "docs_data = loader.load()\n",
    "\n",
    "import importlib\n",
    "import Data_preprocessing\n",
    "importlib.reload(Data_preprocessing)\n",
    "\n",
    "# Initialize the Preprocessing object\n",
    "preprocessing = Data_preprocessing.Preprocessing()\n",
    "\n",
    "for doc in docs_data:\n",
    "    doc.page_content = preprocessing.clean_text_template(doc.page_content)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "\n",
    "splits = text_splitter.split_documents(docs_data)\n",
    "pprint.pprint(splits[0:6])\n",
    "pprint.pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import TFIDFRetriever, EnsembleRetriever\n",
    "\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "sparse_retriever = TFIDFRetriever.from_documents(splits)\n",
    "sparse_retriever.k =  4\n",
    "\n",
    "retriever = EnsembleRetriever(retrievers=[sparse_retriever, dense_retriever], weights=[0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "template = \"\"\"\n",
    "Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "Se nessuna delle domande è rilevante per il contesto del software fornisci come output solamente il tag: \"OUT-OF SCOPE\".\n",
    "\n",
    "**Esempi:**\n",
    "1. Domanda originale: \"Come posso specificare il tipo costo da assegnare al nuovo ambiente di commessa?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per definire il tipo di costo da applicare a un nuovo ambiente di commessa?\n",
    "   - Come posso configurare il tipo di costo per un nuovo ambiente di commessa nel sistema?\n",
    "   - Quali sono i passaggi per assegnare un tipo di costo a un ambiente di commessa appena creato?\n",
    "   - Dove posso trovare l'opzione per impostare il tipo di costo di un nuovo ambiente di commessa?\n",
    "   - Come posso scegliere il tipo di costo da associare a un ambiente di commessa nel mio sistema?\n",
    "\n",
    "2. Domanda originale: \"Come si fa la carbonara?\"\n",
    "   Risposta: OUT-OF SCOPE\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico. \n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model_gpt = ChatOpenAI(temperature=0, model=\"gpt-4o\") \n",
    "\n",
    "generate_queries = (\n",
    "    prompt \n",
    "    | model_gpt\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking/Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def compute_cosine_similarity(embedding, embeddings):\n",
    "    \"\"\"Computes cosine similarity between one vector and a set of vectors.\"\"\"\n",
    "    # Compute the dot product between the single vector and each vector in embeddings\n",
    "    dot_products = np.dot(embeddings, embedding)\n",
    "    \n",
    "    # Compute the norms\n",
    "    norm_embedding = np.linalg.norm(embedding)\n",
    "    norms_embeddings = np.linalg.norm(embeddings, axis=1)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = dot_products / (norm_embedding * norms_embeddings)\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def compute_bm25_score(original, rewrites):\n",
    "    \"\"\"Computes BM25 scores for original question against rewritten questions.\"\"\"\n",
    "    # Prepare documents for BM25\n",
    "    documents = [original] + rewrites\n",
    "    tokenized_documents = [doc.split(\" \") for doc in documents]\n",
    "    \n",
    "    # Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    \n",
    "    # Get BM25 scores for the original question against all rewrites\n",
    "    scores = bm25.get_scores(tokenized_documents[0])\n",
    "    \n",
    "    return scores[1:]  # Exclude the score for the original question\n",
    "\n",
    "def rerank_questions(original_question, alpha=0.5, threshold=1):\n",
    "    \"\"\"Rerank the rewritten questions based on cosine similarity and BM25 scores.\"\"\"\n",
    "    \n",
    "    # Load a pre-trained model\n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    original_embedding = model.encode(original_question)\n",
    "    rewritten_questions = generate_queries.invoke({\"question\": original_question})\n",
    "    rewritten_embeddings = model.encode(rewritten_questions)\n",
    "\n",
    "    # Convert list of embeddings to a numpy array\n",
    "    rewritten_embeddings = np.vstack(rewritten_embeddings)  # Stack into a single 2D array\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = compute_cosine_similarity(original_embedding, rewritten_embeddings)\n",
    "    \n",
    "    # Compute BM25 scores\n",
    "    bm25_scores = compute_bm25_score(original_question, rewritten_questions)\n",
    "\n",
    "    # Combine scores\n",
    "    weighted_scores = alpha * cosine_similarities + (1 - alpha) * bm25_scores\n",
    "    \n",
    "    # Create a ranking of rewritten questions\n",
    "    ranked_indices = np.argsort(weighted_scores)[::-1]  # Sort in descending order\n",
    "    \n",
    "    # Filter questions based on the threshold\n",
    "    filtered_questions = [(rewritten_questions[i], weighted_scores[i]) for i in ranked_indices if weighted_scores[i] >= threshold]\n",
    "    # EXPERIMENT IF IT WORKS BETTER WITH OR WITHOUT THE ORIGINAL QUESTION\n",
    "    questions = []\n",
    "    # questions = [original_question]\n",
    "    for question, score in filtered_questions:\n",
    "        print(question, score)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            # k is a constant smoothing factor that prevents documents from being overly penalized for being far down the list\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template_RAG_generation = \"\"\"   \n",
    "Comportati come un assistente che risponde alle domande del cliente.   \n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}.\n",
    "\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.   \n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata. \n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento,\n",
    "specificando, invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question} \n",
    "\"\"\"\n",
    "\n",
    "generation_prompt = ChatPromptTemplate.from_template(template_RAG_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "basic_rag_chain = (\n",
    "    {\"context\": rerank_questions | retriever.map() | reciprocal_rank_fusion | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generation_prompt\n",
    "    | model_gemini\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Advanced RAG pipeline on a small testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved CSV file\n",
    "eval_df = pd.read_csv('filtered_matching_questions.csv')\n",
    "\n",
    "# Display the first few rows of the loaded DataFrame\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import E2E_Evaluation_metrics\n",
    "importlib.reload(E2E_Evaluation_metrics)\n",
    "from E2E_Evaluation_metrics import RAGEvaluator\n",
    "from E2E_Evaluation_metrics import SemScoreQueryRewriting\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "semscore = SemScoreQueryRewriting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_answers(generation_chain, df, model_name, chunking_type, preprocessing, retriever, techniques):\n",
    "    # Create a copy of the original dataframe to avoid modifying it\n",
    "    new_df = df.copy()\n",
    "    new_df['generated_answer'] = None\n",
    "    new_df['model'] = None\n",
    "    new_df['chunking'] = None\n",
    "    new_df['preprocessing'] = None\n",
    "    new_df['retriever'] = None\n",
    "    new_df['advanced_techniques'] = None\n",
    "\n",
    "    # Iterate through the dataframe and generate answers\n",
    "    for idx, elem in new_df.iterrows():\n",
    "        question = elem[\"question\"]\n",
    "        new_df.at[idx, 'generated_answer'] = generation_chain.invoke(question) \n",
    "        new_df.at[idx, 'model'] = model_name\n",
    "        new_df.at[idx, 'chunking'] = chunking_type\n",
    "        new_df.at[idx, 'preprocessing'] = preprocessing\n",
    "        new_df.at[idx, 'retriever'] = retriever\n",
    "        new_df.at[idx, 'advanced_techniques'] = techniques\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_answers(basic_rag_chain, eval_df, 'Gemini', 'Recursive', 'Simple Preprocessing', 'Dense-6', 'No advanced techniques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['BLEU', 'ROUGE-2', 'ROUGE-L', 'BERT P', 'BERT R', 'Perplexity', 'Diversity']\n",
    "\n",
    "def evaluate_responses(eval_df, evaluator):\n",
    "    results = []\n",
    "    for _, row in eval_df.iterrows():\n",
    "        response = row['generated_answer']\n",
    "        reference = row['answer']\n",
    "        \n",
    "        # Check if either response or reference is empty, and skip this row\n",
    "        if not response or not reference:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate and store the results\n",
    "        evaluation = evaluator.evaluate_all(response, reference)\n",
    "        results.append(evaluation)\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    eval_df = pd.DataFrame(results)\n",
    "    return eval_df\n",
    "\n",
    "\n",
    "def process_evaluation_and_metrics(data_frame, model_name, evaluator = evaluator, semscore = semscore, columns_to_drop = columns_to_drop):\n",
    "    \"\"\"\n",
    "    Evaluate responses, compute semantic scores, and merge results into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data_frame (pd.DataFrame): The input DataFrame with original and rewritten questions.\n",
    "    - evaluator (object): The evaluation object to compute BLEU, ROUGE, etc.\n",
    "    - semscore (object): The semantic score computation object.\n",
    "    - model_name (str): Name of the model for semantic similarity scoring.\n",
    "    - columns_to_drop (list): List of columns to drop from the evaluated DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with evaluation metrics and semantic scores.\n",
    "    \"\"\"\n",
    "    # Step 1: Evaluate responses\n",
    "    eval_df = evaluate_responses(data_frame, evaluator)\n",
    "    \n",
    "    # Step 2: Drop unnecessary columns\n",
    "    eval_df = eval_df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    # Step 3: Compute semantic scores\n",
    "    reference = \"answer\"\n",
    "    response = \"generated_answer\"\n",
    "    cosine_similarities_bge, _ = semscore.compute_sem_score(data_frame, model_name=model_name, reference=reference, response=response)\n",
    "    eval_df[\"SemScore\"] = cosine_similarities_bge[\"Cosine_Similarity\"]\n",
    "\n",
    "    # Step 4: Merge original DataFrame with evaluation metrics\n",
    "    merged_df = pd.concat([data_frame, eval_df], axis=1)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = process_evaluation_and_metrics(\n",
    "    data_frame=df, \n",
    "    model_name='BAAI/bge-m3'\n",
    ")\n",
    "\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the filtered dataframe to a CSV file\n",
    "eval_df.to_csv('ResultsOnTestset/GeminiBaselineRecursiveChunkingSimplePreprocessing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_average_value(df, output_file):\n",
    "    # Compute the averages\n",
    "    mean_rouge = df['ROUGE-1'].mean()\n",
    "    mean_bert = df['BERT F1'].mean()\n",
    "    mean_sem = df['SemScore'].mean()\n",
    "\n",
    "    # Get model and other details\n",
    "    model = df['model'].unique()\n",
    "    chunking = df[\"chunking\"].unique()\n",
    "    preprocessing = df['preprocessing'].unique()\n",
    "    retriever = df['retriever'].unique()\n",
    "    advanced_techniques = df[\"advanced_techniques\"].unique()\n",
    "\n",
    "    # Create a dictionary of the results\n",
    "    results = {\n",
    "        'Model': model,\n",
    "        'Chunking': chunking,\n",
    "        'Preprocessing': preprocessing,\n",
    "        'Retriever': retriever,\n",
    "        'Advanced Techniques': advanced_techniques,\n",
    "        'Mean ROUGE-1': mean_rouge,\n",
    "        'Mean BERT F1': mean_bert,\n",
    "        'Mean SemScore': mean_sem\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    results_df = pd.DataFrame([results])\n",
    "\n",
    "    # Append the results to the CSV file (if it exists, otherwise create a new one)\n",
    "    results_df.to_csv(output_file, mode='a', header=not pd.io.common.file_exists(output_file), index=False)\n",
    "\n",
    "    # Print the results (optional)\n",
    "    print(\"Model:\", model, \"with chunking of type:\", chunking, \"that uses\", \n",
    "          preprocessing, retriever, advanced_techniques)\n",
    "    print(f\"Mean ROUGE-1: {mean_rouge}\")\n",
    "    print(f\"Mean BERT F1: {mean_bert}\")s\n",
    "    print(f\"Mean SemScore: {mean_sem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_value(eval_df, \"ResultsMeanScore/Improved.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
