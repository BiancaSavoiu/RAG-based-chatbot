{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:15.237706Z",
     "start_time": "2024-10-22T13:04:15.235237Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:15.360719Z",
     "start_time": "2024-10-22T13:04:15.277078Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/final_dataset.csv\"\n",
    "\n",
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:16.734661Z",
     "start_time": "2024-10-22T13:04:15.482849Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import ollama\n",
    "\n",
    "# Load BERT multilingual model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:43.882771Z",
     "start_time": "2024-10-22T13:04:16.754080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try on a single document \n",
    "# Create a list with all the values in the column 'text'\n",
    "text_list = data_df['text'].tolist()\n",
    "text = text_list[0]\n",
    "\n",
    "# Tokenize the input and generate embeddings\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the embeddings from the last hidden state\n",
    "embeddings = outputs.last_hidden_state\n",
    "# Average the embeddings across the sequence if you want a single vector for the entire input\n",
    "mean_embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "# Use Ollama's LLaMA 3.2 model\n",
    "# Example LLaMA prompt using the embeddings\n",
    "#prompt = f'Il testo da riassumere è: --{mean_embeddings.numpy().tolist()}--'\n",
    "prompt = f\"\"\"Fornisci un riassunto dettagliato della documentazione fornita. \n",
    "    Documentazione:\n",
    "    {text}\"\"\"\n",
    "\n",
    "response = ollama.generate(model=\"llama3.2\", prompt=prompt, options={\"temperature\": 0})\n",
    "\n",
    "# Print the response generated by LLaMA\n",
    "print(\"Risposta:\", response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:52.334874Z",
     "start_time": "2024-10-22T13:04:43.926924Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Fornisci un riassunto dettagliato della documentazione fornita. \n",
    "    Documentazione:: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:52.381220Z",
     "start_time": "2024-10-22T13:04:52.378741Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:52.420639Z",
     "start_time": "2024-10-22T13:04:52.410367Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"text\")\n",
    "docs_data = loader.load()\n",
    "docs_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:53.586443Z",
     "start_time": "2024-10-22T13:04:52.452638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "# Possible improvements - future hypertuning of chunk_size and chunk_overlap to improve results and try different slitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)\n",
    "splits = text_splitter.split_documents(docs_data)\n",
    "pprint.pprint(splits[0:6])\n",
    "pprint.pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:59.798961Z",
     "start_time": "2024-10-22T13:04:53.605410Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:59.819322Z",
     "start_time": "2024-10-22T13:04:59.816407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate embeddings using the BERT model\n",
    "def bert_embed(texts):\n",
    "    # Tokenize the input text (list of texts)\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=3000)\n",
    "    \n",
    "    # Generate embeddings using BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "    \n",
    "    # Optionally, we average the token embeddings across the sequence to get a single vector for each input\n",
    "    # mean_embeddings = torch.mean(embeddings, dim=1)  # Shape: [batch_size, hidden_size]\n",
    "    \n",
    "    return mean_embeddings.cpu().numpy()  # Convert to numpy array for Chroma\n",
    "\n",
    "class BertEmbedding:\n",
    "    def embed_documents(self, texts):\n",
    "        return model.encode(texts)\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:59.838957Z",
     "start_time": "2024-10-22T13:04:59.836875Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "#vectorstore = FAISS.from_documents(splits, BertEmbedding())\n",
    "#vectorstore.save_local(\"local_model_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:59.890457Z",
     "start_time": "2024-10-22T13:04:59.858042Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"local_model_index\", BertEmbedding(), allow_dangerous_deserialization=True)\n",
    "vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:59.938379Z",
     "start_time": "2024-10-22T13:04:59.935443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of vectors stored\n",
    "faiss_index = vectorstore.index\n",
    "num_vectors = faiss_index.ntotal\n",
    "dimension = faiss_index.d\n",
    "print(f\"Number of Vectors: {num_vectors}\")\n",
    "print(f\"Dimension of Vectors:{dimension }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:04:59.976012Z",
     "start_time": "2024-10-22T13:04:59.973912Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:05:00.126099Z",
     "start_time": "2024-10-22T13:04:59.997347Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "retrieved_documents = retriever.invoke(question)\n",
    "\n",
    "pprint.pprint(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:05:00.180046Z",
     "start_time": "2024-10-22T13:05:00.145297Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_llama = ChatOllama(\n",
    "    model=\"llama3.2\", \n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:05:28.942774Z",
     "start_time": "2024-10-22T13:05:00.197029Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "response_text = rag_chain.invoke(question)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "vectorstore_fp16 = FAISS.load_local(\"local_model_index\", M3EmbeddingFP16(), allow_dangerous_deserialization=True)\n",
    "vectorstore_fp16.index.ntotal\n",
    "retriever_fp16 = vectorstore_fp16.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever_fp16 | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# \n",
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "response_text = rag_chain.invoke(question)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-22T13:05:28.995158Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "question_out_of_scope = \"Quando è morto Giulio Cesare?\"\n",
    "pprint.pprint(rag_chain.invoke(question_out_of_scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:58:27.697734Z",
     "start_time": "2024-10-22T09:57:59.959067Z"
    }
   },
   "outputs": [],
   "source": [
    "multiple_questions = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione? Chi è Giulio Cesare?\"\n",
    "pprint.pprint(rag_chain.invoke(multiple_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:58:51.395268Z",
     "start_time": "2024-10-22T09:58:27.783929Z"
    }
   },
   "outputs": [],
   "source": [
    "multiple_valid_questions = \"Cosa significa che una fattura è in mancata consegna? Il cliente ha ricevuto la fattura?\"\n",
    "pprint.pprint(rag_chain.invoke(multiple_valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:58:51.603655Z",
     "start_time": "2024-10-22T09:58:51.478206Z"
    }
   },
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(multiple_valid_questions)\n",
    "\n",
    "pprint.pprint(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:20.246260Z",
     "start_time": "2024-10-22T09:58:51.690944Z"
    }
   },
   "outputs": [],
   "source": [
    "fastupdate_question = \"Che novità ci sono relative al workflow nel fast update 5.0.03?\"\n",
    "\n",
    "pprint.pprint(rag_chain.invoke(fastupdate_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:20.456624Z",
     "start_time": "2024-10-22T09:59:20.335216Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever.invoke(fastupdate_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:38.172269Z",
     "start_time": "2024-10-22T09:59:20.570685Z"
    }
   },
   "outputs": [],
   "source": [
    "q_client = \"Addebito bollo su nota credito. Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_rewritten = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\"\n",
    "\n",
    "pprint.pprint(rag_chain.invoke(q_rewritten))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:38.285769Z",
     "start_time": "2024-10-22T09:59:38.212692Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:38.620649Z",
     "start_time": "2024-10-22T09:59:38.300131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k =  4  # Retrieve top 2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:00:15.265129Z",
     "start_time": "2024-10-22T09:59:38.636697Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt - we removed a part of the prompt and it seems to give better answers, answering also to parts it didn't answered before\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain_bm25 = (\n",
    "    {\"context\": bm25_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "response_text = rag_chain_bm25.invoke(question)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:00:41.837987Z",
     "start_time": "2024-10-22T10:00:15.351382Z"
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_valid_questions)\n",
    "response_text = rag_chain_bm25.invoke(multiple_valid_questions)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Retriever - BM25 + Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:00:42.073184Z",
     "start_time": "2024-10-22T10:00:41.881506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever], weights=[0.4, 0.6])\n",
    "\n",
    "# Retrieve relevant documents/products\n",
    "pprint.pprint(question)\n",
    "docs = ensemble_retriever.get_relevant_documents(question)\n",
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:01:13.562629Z",
     "start_time": "2024-10-22T10:00:42.207701Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt - we removed a part of the prompt and it seems to give better answers, answering also to parts it didn't answered before\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain_ensemble = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "response_text = rag_chain_ensemble.invoke(question)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:01:36.295140Z",
     "start_time": "2024-10-22T10:01:13.647846Z"
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_valid_questions)\n",
    "response_text = rag_chain_ensemble.invoke(multiple_valid_questions)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:02:05.111990Z",
     "start_time": "2024-10-22T10:01:36.333301Z"
    }
   },
   "outputs": [],
   "source": [
    "q_rewritten = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\"\n",
    "pprint.pprint(rag_chain_ensemble.invoke(q_rewritten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:02:24.983637Z",
     "start_time": "2024-10-22T10:02:05.186438Z"
    }
   },
   "outputs": [],
   "source": [
    "q_client = \"Addebito bollo su nota credito. Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "pprint.pprint(rag_chain_ensemble.invoke(q_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:02:35.261594Z",
     "start_time": "2024-10-22T10:02:25.048951Z"
    }
   },
   "outputs": [],
   "source": [
    "question_out_of_scope = \"Quando è morto Giulio Cesare?\"\n",
    "pprint.pprint(rag_chain_ensemble.invoke(question_out_of_scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:03:02.354669Z",
     "start_time": "2024-10-22T10:02:35.331568Z"
    }
   },
   "outputs": [],
   "source": [
    "multiple_questions = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione? Chi è Giulio Cesare?\"\n",
    "pprint.pprint(rag_chain_ensemble.invoke(multiple_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:03:27.930523Z",
     "start_time": "2024-10-22T10:03:02.426210Z"
    }
   },
   "outputs": [],
   "source": [
    "fastupdate_question = \"Che novità ci sono relative al workflow nel fast update 5.0.03?\"\n",
    "pprint.pprint(rag_chain_ensemble.invoke(fastupdate_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated system evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding-Based Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:28:00.405751Z",
     "start_time": "2024-10-22T12:27:12.880543Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "def evaluate_embedding_similarity(query, retrieved_docs, generated_answer):\n",
    "    # Embed the query, generated answer, and retrieved documents\n",
    "    query_embedding = model.encode(query).reshape(1, -1)  # Reshape to (1, embedding_size)\n",
    "    answer_embedding = model.encode(generated_answer).reshape(1, -1)  # Reshape to (1, embedding_size)\n",
    "    doc_embeddings = [model.encode(doc).reshape(1, -1) for doc in retrieved_docs]  # Reshape each to (1, embedding_size)\n",
    "\n",
    "    # Compute cosine similarity between query and generated answer\n",
    "    query_answer_similarity = cosine_similarity(query_embedding, answer_embedding)[0][0]\n",
    "\n",
    "    # Compute cosine similarity between generated answer and each retrieved document\n",
    "    doc_answer_similarities = [cosine_similarity(answer_embedding, doc_emb)[0][0] for doc_emb in doc_embeddings]\n",
    "    \n",
    "    # Average similarity between generated answer and retrieved documents\n",
    "    avg_doc_answer_similarity = np.mean(doc_answer_similarities)\n",
    "\n",
    "    return query_answer_similarity, avg_doc_answer_similarity\n",
    "\n",
    "# Example usage\n",
    "docs = ensemble_retriever.get_relevant_documents(question)\n",
    "retrieved_docs = [context.page_content for context in docs]\n",
    "generated_answer = rag_chain_ensemble.invoke(question)\n",
    "\n",
    "# Run the evaluation\n",
    "query_answer_similarity, avg_doc_answer_similarity = evaluate_embedding_similarity(question, retrieved_docs, generated_answer)\n",
    "\n",
    "print(f\"Query-Answer Similarity: {query_answer_similarity}\")\n",
    "print(f\"Average Answer-Documents Similarity: {avg_doc_answer_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query-Answer Similarity: 0.7175188660621643:**\n",
    "\n",
    "Strength: The generated answer is likely relevant and on-topic, addressing the core aspect of the query.\n",
    "\n",
    "Potential Weakness: Some minor discrepancies or nuances in the query may not be fully captured in the answer. For example, the answer might be general or missing specific details from the query, which could be why the similarity is not closer to 1.\n",
    "\n",
    "**Average Answer-Documents Similarity: 0.5526096224784851**\n",
    "\n",
    "Strength: The generated answer seems to make use of the retrieved information, but not in a very strong or comprehensive way.\n",
    "\n",
    "Potential Weakness: The answer may either:\n",
    "- Use information that is not fully represented in the retrieved documents (potentially introducing hallucinations or unsupported facts).\n",
    "- Be loosely based on the retrieved documents, but not drawing directly or strongly from the key information within them.\n",
    "\n",
    "**Possibilities to improve the results:**\n",
    "\n",
    "Query-Answer Similarity:\n",
    "\n",
    "Fine-tune the model to generate answers that more precisely match the intent and specifics of the query.\n",
    "Ensure that the answer directly addresses the main points or information asked in the query.\n",
    "\n",
    "Answer-Documents Similarity:\n",
    "\n",
    "Improve document retrieval system to fetch more relevant or diverse documents that better support the generated answer.\n",
    "Ensure that the model generates answers more closely based on the retrieved content, reducing the chances of hallucinations or answers based on information not found in the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:50.575111Z",
     "start_time": "2024-10-22T10:44:07.892154Z"
    }
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def measure_self_consistency(query, num_trials=10, threshold=0.9):\n",
    "    generated_answers = []\n",
    "    \n",
    "    # Run the retrieval and generation steps multiple times\n",
    "    for _ in range(num_trials):\n",
    "        generated_answer = rag_chain_ensemble.invoke(query)\n",
    "        generated_answers.append(generated_answer)\n",
    "\n",
    "    # Compare answers with each other\n",
    "    consistent_pairs = 0\n",
    "    total_pairs = 0\n",
    "    for i in range(len(generated_answers)):\n",
    "        for j in range(i+1, len(generated_answers)):\n",
    "            total_pairs += 1\n",
    "            if calculate_similarity(generated_answers[i], generated_answers[j]) > threshold:\n",
    "                consistent_pairs += 1\n",
    "\n",
    "    # Consistency score: how many times answers are semantically similar\n",
    "    consistency_score = consistent_pairs / total_pairs\n",
    "    return consistency_score\n",
    "\n",
    "measure_self_consistency(multiple_valid_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions, in Italian, of the given user question to retrieve relevant documents from a vector database. \n",
    "The context of our application is related to Enterprise Resource Planning (ERP) software's technical manuals (specifically Panthera software) or, more generally, topics related to computer science, including system configuration,\n",
    "module functionality, troubleshooting, and implementation guidelines.\n",
    "Your goal is to generate multiple perspectives on the question to help the user overcome limitations of distance-based similarity search while focusing strictly on the context of ERP software documentation\n",
    "or relevant computer science topics.\n",
    "In cases where the user provides multiple questions, only respond to the relevant ones related to ERP documentation or computer science. Provide these alternative questions separated by newlines.\n",
    "Before generating alternatives, ensure the user's question is related to ERP technical documentation or relevant computer science topics. \n",
    "If any of the questions are out of scope or irrelevant to ERP manuals or computer science topics, disregard them entirely. \n",
    "You don't need to ignore all the questions, but only the ones that are out of scope.\n",
    "\n",
    "Use the ERP context only as information, but do not mention it in the rewritten questions.\n",
    "Provide the created alternative questions separated by newlines, and structure the output to contain only the rewritten questions in a bullet list.\n",
    "Output only the bullet list of the rewritten questions.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Sei un assistente modello linguistico AI. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software di Enterprise Resource Planning (ERP).\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni.\n",
    "2. Ignora le domande che non sono pertinenti ai manuali ERP o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model_llama\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "pprint.pprint(question)\n",
    "rewritten_question = generate_queries.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(generate_queries.invoke({\"question\": multiple_valid_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto. Evita troppe ripetizioni nella risposta fornita.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rewriting_rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"), #\"context\" : retrieval_chain\n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint.pprint(rewriting_rag_chain.invoke({\"context\": chain.invoke(question), \"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rewriting_rag_chain.invoke({\"question\":question_out_of_scope}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rewriting_rag_chain.invoke({\"question\":multiple_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rewriting_rag_chain.invoke({\"question\":multiple_valid_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rewriting_rag_chain.invoke({\"context\": retrieval_chain.invoke(multiple_valid_questions), \"question\": multiple_valid_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rewriting_rag_chain.invoke({\"question\":q_rewritten}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            # k is a constant smoothing factor that prevents documents from being overly penalized for being far down the list\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results[:3]\n",
    "\n",
    "reranking_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "retrieval_chain_rag_fusion = generate_queries | reranking_retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint.pprint(question)\n",
    "pprint.pprint(rerank_rag_chain.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question_out_of_scope)\n",
    "pprint.pprint(rerank_rag_chain.invoke({\"question\": question_out_of_scope}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_questions)\n",
    "pprint.pprint(rerank_rag_chain.invoke({\"question\": multiple_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rerank_rag_chain.invoke({\"question\":multiple_valid_questions}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
