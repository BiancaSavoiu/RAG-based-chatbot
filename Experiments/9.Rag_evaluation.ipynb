{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset = Dataset.load_from_disk(\"eval_dataset\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df = eval_dataset.to_pandas()\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG evaluation on eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index = random.randint(0, len(eval_df))\n",
    "element = eval_df.loc[index]\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "question = element[\"question\"]\n",
    "response = element[\"answer\"]\n",
    "referenced_context = element[\"context\"]\n",
    "\n",
    "pprint.pprint(question)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset for RAG\n",
    "import pandas as pd\n",
    "filename_all_data_dict = \"./Files/final_dataset.csv\"\n",
    "\n",
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"text\")\n",
    "docs_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)\n",
    "splits = text_splitter.split_documents(docs_data)\n",
    "pprint.pprint(splits[0:6])\n",
    "pprint.pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.load_local(\"local_model_index\", M3EmbeddingFP16(), allow_dangerous_deserialization=True)\n",
    "vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_llama = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame using `iterrows` to access both index and row data\n",
    "for idx, row in eval_df.iterrows():\n",
    "    question = row[\"question\"]  # Get the question from the current row\n",
    "    \n",
    "    # Invoke the model to get the response\n",
    "    response = rag_chain.invoke(question)  # Assuming invoke expects a dict with \"question\" key\n",
    "    \n",
    "    # Store the response in the \"model_response\" column for the corresponding row\n",
    "    eval_df.at[idx, \"model_response\"] = response\n",
    "\n",
    "# Display the updated DataFrame to verify\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single data poin to test on from the evaluation set\n",
    "import random\n",
    "index = random.randint(0, len(eval_df))\n",
    "element = eval_df.loc[index]\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = element[\"question\"]\n",
    "reference = element[\"answer\"]\n",
    "response = element[\"model_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)\n",
    "pprint.pprint(reference)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG with RAG-evaluator dataset\n",
    "from rag_evaluator import RAGEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = RAGEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the response\n",
    "pprint.pprint(question)\n",
    "metrics = evaluator.evaluate_all(question, response, reference)\n",
    "\n",
    "# Print the results\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain_gpt = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame using `iterrows` to access both index and row data\n",
    "for idx, row in eval_df.iterrows():\n",
    "    question = row[\"question\"]  # Get the question from the current row\n",
    "    \n",
    "    # Invoke the model to get the response\n",
    "    response = rag_chain_gpt.invoke(question)  # Assuming invoke expects a dict with \"question\" key\n",
    "    \n",
    "    # Store the response in the \"model_response\" column for the corresponding row\n",
    "    eval_df.at[idx, \"model_response_gpt\"] = response\n",
    "\n",
    "# Display the updated DataFrame to verify\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = eval_df.loc[index]\n",
    "response_gpt = element[\"model_response_gpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the response\n",
    "pprint.pprint(question)\n",
    "metrics_gpt = evaluator.evaluate_all(question, response_gpt, reference)\n",
    "\n",
    "# Print the results\n",
    "pprint.pprint(metrics_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "eval_df.to_csv('eval_dataset_llama3.2_against_gpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compare models metrics:\")\n",
    "print(\"\")\n",
    "pprint.pprint(\"Metrics llama3.2 generator:\")\n",
    "pprint.pprint(metrics)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Metrics gpt-4o generator:\")\n",
    "pprint.pprint(metrics_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of an evaluation of a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and RAG types\n",
    "model_data = [\n",
    "    {'Model': 'llama3.2', 'RAG Type': 'Basic RAG', 'Question_rewriting' : False, **metrics},\n",
    "    {'Model': 'gpt-4o', 'RAG Type': 'Basic RAG', 'Question_rewriting': False,  **metrics_gpt}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(model_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on all available data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each entry of llama3.2 - (question, reference, response)\n",
    "results = []\n",
    "for index, row in eval_df.iterrows():\n",
    "    question = row['question']\n",
    "    response = row['model_response']\n",
    "    reference = row['answer']\n",
    "    \n",
    "    # Evaluate and store the results\n",
    "    evaluation = evaluator.evaluate_all(question, response, reference)\n",
    "    results.append(evaluation)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the final DataFrame with evaluations\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics to get a single evaluation for the model\n",
    "aggregated_results = {\n",
    "    \"BLEU\": results_df[\"BLEU\"].mean(),\n",
    "    \"ROUGE-1\": results_df[\"ROUGE-1\"].mean(),\n",
    "    \"BERT P\": results_df[\"BERT P\"].mean(),\n",
    "    \"BERT R\": results_df[\"BERT R\"].mean(),\n",
    "    \"BERT F1\": results_df[\"BERT F1\"].mean(),\n",
    "    \"Perplexity\": results_df[\"Perplexity\"].mean(),\n",
    "    \"Diversity\": results_df[\"Diversity\"].mean(),\n",
    "    \"Racial Bias\": results_df[\"Racial Bias\"].mean()\n",
    "}\n",
    "\n",
    "# Convert aggregated results to a DataFrame for better readability\n",
    "aggregated_results_df = pd.DataFrame(aggregated_results, index=[0])\n",
    "\n",
    "# Display the aggregated results\n",
    "display(aggregated_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each entry of llama3.2 - (question, reference, response)\n",
    "results = []\n",
    "for index, row in eval_df.iterrows():\n",
    "    question = row['question']\n",
    "    response = row['model_response_gpt']\n",
    "    reference = row['answer']\n",
    "    \n",
    "    # Evaluate and store the results\n",
    "    evaluation = evaluator.evaluate_all(question, response, reference)\n",
    "    results.append(evaluation)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df_gpt = pd.DataFrame(results)\n",
    "\n",
    "# Display the final DataFrame with evaluations\n",
    "display(results_df_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics to get a single evaluation for the model\n",
    "aggregated_results_gpt = {\n",
    "    \"BLEU\": results_df_gpt[\"BLEU\"].mean(),\n",
    "    \"ROUGE-1\": results_df_gpt[\"ROUGE-1\"].mean(),\n",
    "    \"BERT P\": results_df_gpt[\"BERT P\"].mean(),\n",
    "    \"BERT R\": results_df_gpt[\"BERT R\"].mean(),\n",
    "    \"BERT F1\": results_df_gpt[\"BERT F1\"].mean(),\n",
    "    \"Perplexity\": results_df_gpt[\"Perplexity\"].mean(),\n",
    "    \"Diversity\": results_df_gpt[\"Diversity\"].mean(),\n",
    "    \"Racial Bias\": results_df_gpt[\"Racial Bias\"].mean()\n",
    "}\n",
    "\n",
    "# Convert aggregated results to a DataFrame for better readability\n",
    "aggregated_results_df_gpt = pd.DataFrame(aggregated_results_gpt, index=[0])\n",
    "\n",
    "# Display the aggregated results\n",
    "display(aggregated_results_df_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(aggregated_results_df)\n",
    "display((aggregated_results_df_gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and RAG types\n",
    "model_data_aggregated_eval = [\n",
    "    {'Model': 'llama3.2', 'RAG Type': 'Basic RAG', 'Question_rewriting' : False, **aggregated_results},\n",
    "    {'Model': 'gpt-4o', 'RAG Type': 'Basic RAG', 'Question_rewriting': False,  **aggregated_results_gpt}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics_aggregated_data = pd.DataFrame(model_data_aggregated_eval)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_metrics_aggregated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation for a single data point\")\n",
    "display(df_metrics)\n",
    "\n",
    "print(\"Evaluation for more data points (14) - mean evaluation\")\n",
    "display(df_metrics_aggregated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_metrics = {\n",
    "    \"BLEU\": 0,\n",
    "    \"ROUGE-1\": 0,\n",
    "    \"BERT P\": 0,\n",
    "    \"BERT R\": 0,\n",
    "    \"BERT F1\": 0,\n",
    "    \"Perplexity\": 0,\n",
    "    \"Diversity\": 0,\n",
    "}\n",
    "\n",
    "# List to store running means\n",
    "running_means = []\n",
    "\n",
    "# Evaluate each entry\n",
    "for index, row in eval_df.iterrows():\n",
    "    question = row['question']\n",
    "    response = row['model_response']\n",
    "    reference = row['answer']\n",
    "    \n",
    "    # Evaluate and store the results\n",
    "    evaluation = evaluator.evaluate_all(question, response, reference)\n",
    "    results.append(evaluation)\n",
    "    \n",
    "    # Update cumulative sums\n",
    "    for metric in cumulative_metrics.keys():\n",
    "        cumulative_metrics[metric] += evaluation[metric]\n",
    "    \n",
    "    # Compute the current means\n",
    "    current_means = {metric: cumulative_metrics[metric] / (index + 1) for metric in cumulative_metrics}\n",
    "    running_means.append(current_means)\n",
    "\n",
    "    # Create a DataFrame for running means\n",
    "running_means_df = pd.DataFrame(running_means)\n",
    "\n",
    "# Plotting the evolution of metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for metric in cumulative_metrics.keys():\n",
    "    plt.plot(running_means_df.index + 1, running_means_df[metric], marker='o', label=metric)\n",
    "\n",
    "plt.title('Progression of Evaluation Metrics With more data for Llama3.2')\n",
    "plt.xlabel('Number of Evaluation Points (N)')\n",
    "plt.ylabel('Mean Metric Value')\n",
    "plt.axhline(y=0, color='grey', linestyle='--')  # Optional: Add a horizontal line at y=0 for reference\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_metrics = {\n",
    "    \"BLEU\": 0,\n",
    "    \"ROUGE-1\": 0,\n",
    "    \"BERT P\": 0,\n",
    "    \"BERT R\": 0,\n",
    "    \"BERT F1\": 0,\n",
    "    \"Perplexity\": 0,\n",
    "    \"Diversity\": 0,\n",
    "}\n",
    "\n",
    "# List to store running means\n",
    "running_means = []\n",
    "\n",
    "# Evaluate each entry\n",
    "for index, row in eval_df.iterrows():\n",
    "    question = row['question']\n",
    "    response = row['model_response_gpt']\n",
    "    reference = row['answer']\n",
    "    \n",
    "    # Evaluate and store the results\n",
    "    evaluation = evaluator.evaluate_all(question, response, reference)\n",
    "    results.append(evaluation)\n",
    "    \n",
    "    # Update cumulative sums\n",
    "    for metric in cumulative_metrics.keys():\n",
    "        cumulative_metrics[metric] += evaluation[metric]\n",
    "    \n",
    "    # Compute the current means\n",
    "    current_means = {metric: cumulative_metrics[metric] / (index + 1) for metric in cumulative_metrics}\n",
    "    running_means.append(current_means)\n",
    "\n",
    "    # Create a DataFrame for running means\n",
    "running_means_df = pd.DataFrame(running_means)\n",
    "\n",
    "# Plotting the evolution of metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for metric in cumulative_metrics.keys():\n",
    "    plt.plot(running_means_df.index + 1, running_means_df[metric], marker='o', label=metric)\n",
    "\n",
    "plt.title('Progression of Evaluation Metrics With more data for GPT-4o')\n",
    "plt.xlabel('Number of Evaluation Points (N)')\n",
    "plt.ylabel('Mean Metric Value')\n",
    "plt.axhline(y=0, color='grey', linestyle='--')  # Optional: Add a horizontal line at y=0 for reference\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
