{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510e0b43ecdb8c39",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe1de032178147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:04.019830Z",
     "start_time": "2024-09-26T13:13:03.826634Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import ctypes\n",
    "from ctypes.util import find_library\n",
    "import camelot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b74819733bfce",
   "metadata": {},
   "source": [
    "# Create a txt file with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-23T15:33:47.518381Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"C:/Users/bsavoiumarinas/Documents/Tesi/PyProjects/Doc_Panthera/\"\n",
    "texts_pages = []\n",
    "text = \"\"\n",
    "\n",
    "# In data extract each folder one by one\n",
    "for folder in os.listdir(filepath):\n",
    "    folder_path = os.path.join(filepath, folder)\n",
    "    \n",
    "    # Check if the folder_path is actually a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # For each file in the folder, extract the file name\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                filename_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Use 'with' to automatically close the file after reading\n",
    "                with pdfplumber.open(filename_path) as pdf:\n",
    "                    # Extract text from each page with error handling for None\n",
    "                    texts_pages = [page.extract_text(x_tolerance=1) or \"\" for page in pdf.pages]\n",
    "                    # Concatenate all the text together in a single string\n",
    "                    text += \"  \\n\\n\".join(texts_pages)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bfecf6724bf88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:29:08.729795Z",
     "start_time": "2024-09-17T14:29:08.653159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write on file the text string\n",
    "filename_all_data = \"./Files/all_data.txt\"\n",
    "\n",
    "if not os.path.exists(filename_all_data):\n",
    "  with open(filename_all_data, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe7be2c6d29723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:39:02.933045Z",
     "start_time": "2024-09-17T14:39:02.792303Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca35b952a98b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:39:21.768728Z",
     "start_time": "2024-09-17T14:39:21.759447Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc750c3da5f307a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:57:03.001611Z",
     "start_time": "2024-09-17T14:40:58.183568Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open(filename_all_data, \"r\", encoding=\"utf-8\")\n",
    "string_data = \"\"\n",
    "\n",
    "for line in file.readlines():\n",
    "    string_data += line\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643dff74b6de945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:57:34.811126Z",
     "start_time": "2024-09-17T14:57:34.797051Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(string_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db932ce311ddc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:57:14.528662Z",
     "start_time": "2024-09-17T14:57:14.394114Z"
    }
   },
   "outputs": [],
   "source": [
    "print(string_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026f2c1430fb1d0",
   "metadata": {},
   "source": [
    "# Trial for only a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47c4e2590d23fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:02:06.246780Z",
     "start_time": "2024-09-17T13:01:20.175263Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"./Doc_Panthera/Fast update\"\n",
    "# Create a dictionary called texts\n",
    "texts_dict = {}\n",
    "\n",
    "for filename in os.listdir(filepath):\n",
    "  if filename.endswith(\".pdf\"):\n",
    "    filename_path = os.path.join(filepath, filename)\n",
    "    pdf = pdfplumber.open(filename_path)   \n",
    "    \n",
    "    pages_text_for_file = [page.extract_text(x_tolerance=1) for page in pdf.pages]\n",
    "    # Can concatenate all the text together in a single string\n",
    "    text_for_file = \"  \\n\\n\".join(pages_text_for_file)\n",
    "      \n",
    "    # Populate the texts dictionary, having as key the name of the document and as value the string of all the text\n",
    "    texts_dict[filename] = text_for_file\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ce9d5acf606ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:03:21.610119Z",
     "start_time": "2024-09-17T13:03:21.598656Z"
    }
   },
   "outputs": [],
   "source": [
    "print(texts_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfd6a5273cc719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:03:51.921253Z",
     "start_time": "2024-09-17T13:03:51.904373Z"
    }
   },
   "outputs": [],
   "source": [
    "print(texts_dict['Fastupdate A&C 5_0_03.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4b936c393d853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:04:59.345586Z",
     "start_time": "2024-09-17T13:04:59.310860Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_trial = \"./Files/trial_dict.csv\"\n",
    "with open(filename_all_data_trial, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in texts_dict.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28a03617c26d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:07:42.226624Z",
     "start_time": "2024-09-17T13:07:42.203016Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename_all_data_trial, names = ['file', 'text'], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273ebbfe539fc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:07:46.807083Z",
     "start_time": "2024-09-17T13:07:46.788288Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fc001544f2e01",
   "metadata": {},
   "source": [
    "# Create a csv file with all data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69925620020f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:30:11.131165Z",
     "start_time": "2024-09-17T13:10:01.978634Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"./Doc_Panthera/\"\n",
    "# Create a dictionary called texts\n",
    "texts_dict = {}\n",
    "\n",
    "# In data extract each folder one by one\n",
    "for folder in os.listdir(filepath):\n",
    "  # For each file in the filepath extract the file name\n",
    "  folder_path = os.path.join(filepath, folder)\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "      filename_path = os.path.join(folder_path, filename)\n",
    "      pdf = pdfplumber.open(filename_path)   \n",
    "    \n",
    "      pages_text_for_file = [page.extract_text(x_tolerance=1) for page in pdf.pages]\n",
    "      # Can concatenate all the text together in a single string\n",
    "      text_for_file = \"  \\n\\n\".join(pages_text_for_file)\n",
    "      \n",
    "      # Populate the texts dictionary, having as key the name of the document and as value the string of all the text\n",
    "      texts_dict[filename] = text_for_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376d8539f5404d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:38:35.074863Z",
     "start_time": "2024-09-17T13:38:34.802733Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/all_data_dict.csv\"\n",
    "with open(filename_all_data_dict, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in texts_dict.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd4c5459f87ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:38:42.652220Z",
     "start_time": "2024-09-17T13:38:42.482838Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7bc96ebf0614d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:07.663961Z",
     "start_time": "2024-09-17T13:39:07.641091Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d008e41e3f302f7",
   "metadata": {},
   "source": [
    "# Create the csv file with a dictionary {file_name: text} importing directly the pdf text without headers and footers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c032670ae23d8",
   "metadata": {},
   "source": [
    "Reference: https://pypdf2.readthedocs.io/en/3.0.0/user/extract-text.html#example-1-ignore-header-and-footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea39df44237809b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:52:52.058869Z",
     "start_time": "2024-09-26T16:52:52.031069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exclude headers and footer directly when importing the pdf \n",
    "# Use the y-coordinates to extract text on the page\n",
    "def extract_text_by_y_coordinate(page, min_y=50, max_y=750, parts=None):\n",
    "    \"\"\"\n",
    "    Extracts text from a page based on Y-coordinate range and appends it to parts.\n",
    "\n",
    "    Args:\n",
    "    - page (pdfplumber.page.Page): The page object to extract text from.\n",
    "    - min_y (float): Minimum Y-coordinate for filtering text.\n",
    "    - max_y (float): Maximum Y-coordinate for filtering text.\n",
    "    - parts (list): List to append filtered text parts to.\n",
    "    \"\"\"\n",
    "    if parts is None:\n",
    "        parts = []\n",
    "\n",
    "    for char in page.chars:\n",
    "        y = char['top']\n",
    "        if min_y < y < max_y:\n",
    "            parts.append(char['text'])\n",
    "\n",
    "    return parts\n",
    "\n",
    "def extract_text_from_all_pages(file_path, min_y=50, max_y=750):\n",
    "    \"\"\"\n",
    "    Extracts text from all pages of a PDF file based on Y-coordinate range. We take Y-coordinates between 50 and 720 to exclude the heading and footing of the page, given that a typical PDF document may have a page height of 792 units. The ranges we use ar the ones suggested by the pdfplumber documentation.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the PDF file.\n",
    "    - min_y (float): Minimum Y-coordinate for filtering text.\n",
    "    - max_y (float): Maximum Y-coordinate for filtering text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Extracted text from all pages and coordinate range.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        parts = [extract_text_by_y_coordinate(page, min_y, max_y, parts) for page in pdf.pages]\n",
    "        # Can concatenate all the text together in a single string\n",
    "        text_for_file = \"  \\n\\n\".join(pages_text_for_file)\n",
    "        #parts = extract_text_by_y_coordinate(page, min_y, max_y, parts)\n",
    "\n",
    "    return \"\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fdeb651faf929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:52:58.447216Z",
     "start_time": "2024-09-26T16:52:58.411628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define your coordinate-based text extraction function\n",
    "def extract_text_by_coordinate(page, min_y=50, max_y=750):\n",
    "    \"\"\"\n",
    "    Extracts text from a page based on Y-coordinate range and returns a string.\n",
    "\n",
    "    Args:\n",
    "    - page (pdfplumber.page.Page): The page object to extract text from.\n",
    "    - min_y (float): Minimum Y-coordinate for filtering text.\n",
    "    - max_y (float): Maximum Y-coordinate for filtering text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Extracted text from the page.\n",
    "    \"\"\"\n",
    "    text_lines = []\n",
    "    last_y = None\n",
    "\n",
    "    for char in page.chars:\n",
    "        y = char['top']\n",
    "        text = char['text']\n",
    "        \n",
    "        if min_y < y < max_y:\n",
    "            if last_y is not None and abs(y - last_y) > 10:  # Adjust threshold for line breaks\n",
    "                text_lines.append('\\n')  # Insert a new line for vertical gaps\n",
    "            text_lines.append(text)\n",
    "        \n",
    "        last_y = y\n",
    "\n",
    "    # Join text lines into a single string and return\n",
    "    return \"\".join(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8823a6e14906f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:04:26.827630Z",
     "start_time": "2024-09-26T16:53:06.922502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing PDF files\n",
    "filepath = \"./Doc_Panthera/\"\n",
    "\n",
    "# Create a dictionary to store text from PDFs\n",
    "texts_dict = {}\n",
    "\n",
    "# Loop through each folder in the specified filepath\n",
    "for folder in os.listdir(filepath):\n",
    "    # Get the folder path\n",
    "    folder_path = os.path.join(filepath, folder)\n",
    "    \n",
    "    # Loop through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filename_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(filename_path) as pdf:\n",
    "                # Extract text from each page and combine\n",
    "                parts = [extract_text_by_coordinate(page) for page in pdf.pages]\n",
    "                text_for_file = \"\\n\\n\".join(parts)  # Combine text from all pages\n",
    "\n",
    "            # Add to dictionary, using the filename as the key\n",
    "            texts_dict[filename] = text_for_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913b43d608b4ca2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:05:57.046873Z",
     "start_time": "2024-09-26T17:05:56.913150Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/data_imported_by_pdf_coordinates.csv\"\n",
    "with open(filename_all_data_dict, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in texts_dict.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b45fbf50331a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:04:27.078392Z",
     "start_time": "2024-09-26T17:04:26.990205Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df_only_text = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9b10849625212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:04:27.112113Z",
     "start_time": "2024-09-26T17:04:27.090899Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5f9c903fb6599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:04:27.156197Z",
     "start_time": "2024-09-26T17:04:27.150363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'file'\n",
    "file_list = data_df_only_text['file'].tolist()\n",
    "\n",
    "# Create a list with all the values in the column 'text'\n",
    "text_list = data_df_only_text['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc85770137910ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:05:44.474573Z",
     "start_time": "2024-09-26T17:05:44.464128Z"
    }
   },
   "outputs": [],
   "source": [
    "text_list[56]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4255c99597749",
   "metadata": {},
   "source": [
    "# Complementar import - check not imported page elements to tune the 'y' coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d91ecb41deebc",
   "metadata": {},
   "source": [
    "PDF headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda38e567388bad4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:33:24.896233Z",
     "start_time": "2024-09-26T16:19:27.310656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing PDF files\n",
    "filepath = \"./Doc_Panthera/\"\n",
    "\n",
    "# Create a dictionary to store text from PDFs\n",
    "texts_dict = {}\n",
    "\n",
    "# Loop through each folder in the specified filepath\n",
    "for folder in os.listdir(filepath):\n",
    "    # Get the folder path\n",
    "    folder_path = os.path.join(filepath, folder)\n",
    "    \n",
    "    # Loop through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filename_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(filename_path) as pdf:\n",
    "                # Extract text from each page and combine\n",
    "                parts = [extract_text_by_coordinate(page, 0, 50) for page in pdf.pages]\n",
    "                text_for_file = \"\\n\\n\".join(parts)  # Combine text from all pages\n",
    "\n",
    "            # Add to dictionary, using the filename as the key\n",
    "            texts_dict[filename] = text_for_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4441c293c0d771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:33:37.388294Z",
     "start_time": "2024-09-26T16:33:37.361911Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/headings.csv\"\n",
    "with open(filename_all_data_dict, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in texts_dict.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13759947a4a1c365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:33:40.113864Z",
     "start_time": "2024-09-26T16:33:40.087212Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/headings.csv\"\n",
    "data_df_only_text = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf96427498e885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:33:43.902489Z",
     "start_time": "2024-09-26T16:33:43.829889Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df_only_text[data_df_only_text['text'].str.contains(r'\\w')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929237fe3d8b3f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:33:49.354348Z",
     "start_time": "2024-09-26T16:33:49.340289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'text'\n",
    "text_list = data_df_only_text['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb776129d19e9330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:33:55.225175Z",
     "start_time": "2024-09-26T16:33:55.199183Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'text'\n",
    "import re\n",
    "def clean_output_print(text):\n",
    "    pattern = r\"\\n\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return cleaned_text\n",
    "\n",
    "text_list = data_df_only_text['text'].tolist()\n",
    "for i in range(len(text_list)):\n",
    "    text_list[i] = clean_output_print(text_list[i])\n",
    "\n",
    "text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea8e8f8fa2b31f",
   "metadata": {},
   "source": [
    "PDF footers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28969d33e7e53f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:50:30.654399Z",
     "start_time": "2024-09-26T16:37:14.153143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing PDF files\n",
    "filepath = \"./Doc_Panthera/\"\n",
    "\n",
    "# Create a dictionary to store text from PDFs\n",
    "texts_dict = {}\n",
    "\n",
    "# Loop through each folder in the specified filepath\n",
    "for folder in os.listdir(filepath):\n",
    "    # Get the folder path\n",
    "    folder_path = os.path.join(filepath, folder)\n",
    "    \n",
    "    # Loop through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filename_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(filename_path) as pdf:\n",
    "                # Extract text from each page and combine\n",
    "                parts = [extract_text_by_coordinate(page, 750, 800) for page in pdf.pages]\n",
    "                text_for_file = \"\\n\\n\".join(parts)  # Combine text from all pages\n",
    "\n",
    "            # Add to dictionary, using the filename as the key\n",
    "            texts_dict[filename] = text_for_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72b076918581d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:51:30.859024Z",
     "start_time": "2024-09-26T16:51:30.827410Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/footing.csv\"\n",
    "with open(filename_all_data_dict, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, value in texts_dict.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8b04f9fdd70f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:51:35.191815Z",
     "start_time": "2024-09-26T16:51:35.152002Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/footing.csv\"\n",
    "data_df_only_text = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28061b7978777b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:51:39.077749Z",
     "start_time": "2024-09-26T16:51:39.051530Z"
    }
   },
   "outputs": [],
   "source": [
    "regex_letter = r\"\\w\"\n",
    "data_df_only_text[data_df_only_text['text'].str.contains(regex_letter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f8a92eaacfebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:51:44.430761Z",
     "start_time": "2024-09-26T16:51:44.422551Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_output_print(text):\n",
    "    pattern = r\"\\n\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c794ed1ad97657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:51:49.249457Z",
     "start_time": "2024-09-26T16:51:49.235669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'text'\n",
    "text_list = data_df_only_text['text'].tolist()\n",
    "for i in range(len(text_list)):\n",
    "    text_list[i] = clean_output_print(text_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9cc5d619e0f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:51:55.596896Z",
     "start_time": "2024-09-26T16:51:55.567269Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae836ef75a0fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T07:05:39.445454Z",
     "start_time": "2024-09-26T07:05:38.974363Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./Caricamenti massa/CaricamentMassa_Excel.pdf\"\n",
    "with pdfplumber.open(path) as pdf:\n",
    "    text = [extract_text_by_coordinate(page, 50, 750) for page in pdf.pages]\n",
    "    text_for_file = \"\\n\\n\".join(text)  # Combine text from all pages\n",
    "    \n",
    "text_for_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201bfb7ef3e3a58c",
   "metadata": {},
   "source": [
    "# Extract the tables from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191804ad8a15d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T15:24:06.024106Z",
     "start_time": "2024-09-26T15:24:06.008214Z"
    }
   },
   "outputs": [],
   "source": [
    "find_library(r\"C:\\Program Files\\gs\\gs10.04.0\\bin\\gsdll64.dll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c88b51f55bb646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T15:34:27.415932Z",
     "start_time": "2024-09-26T15:34:26.098158Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./Documentazione tecnica/GEN_Runtimetools.pdf\"\n",
    "tables = camelot.read_pdf(path, \"13\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc304cbe9787d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T15:34:28.644375Z",
     "start_time": "2024-09-26T15:34:28.627841Z"
    }
   },
   "outputs": [],
   "source": [
    "tables[0].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beedad2862b47ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_from_pdfs(input_folder, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop over all PDF files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            \n",
    "            # Extract tables from all pages of the PDF\n",
    "            tables = camelot.read_pdf(pdf_path, pages=\"all\")\n",
    "            \n",
    "            # Create an output subfolder for this PDF file (using filename without extension)\n",
    "            pdf_output_folder = os.path.join(output_folder, os.path.splitext(filename)[0])\n",
    "            if not os.path.exists(pdf_output_folder):\n",
    "                os.makedirs(pdf_output_folder)\n",
    "            \n",
    "            # Loop through all extracted tables\n",
    "            for i, table in enumerate(tables):\n",
    "                df = table.df\n",
    "                \n",
    "                # Remove rows with all NaN or empty values\n",
    "                df_cleaned = df.replace(\"\", pd.NA).dropna(how='all')\n",
    "                \n",
    "                # Skip the table if it's empty after cleaning (no non-null rows)\n",
    "                if not df_cleaned.empty:\n",
    "                    # Get page number for the current table\n",
    "                    page_num = table.page\n",
    "                    \n",
    "                    # Define the CSV path (use page number and index to avoid overwriting)\n",
    "                    csv_filename = f\"table_page_{page_num}_table_{i+1}.csv\"\n",
    "                    csv_path = os.path.join(pdf_output_folder, csv_filename)\n",
    "                    \n",
    "                    # Save the cleaned table to a CSV file\n",
    "                    df_cleaned.to_csv(csv_path, index=False)\n",
    "                    print(f\"Saved table {i+1} from page {page_num} to {csv_path}\")\n",
    "                else:\n",
    "                    print(f\"Table {i+1} on page {table.page} in {filename} is empty, skipping.\")\n",
    "\n",
    "input_folder = \"./Doc_Panthera/Fast update\"\n",
    "output_folder = \"./Experiments/Tables\"\n",
    "\n",
    "extract_tables_from_pdfs(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221e087e263e97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T14:54:23.988562Z",
     "start_time": "2024-09-30T14:54:23.893347Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_and_remove_empty_folders(directory):\n",
    "    # Iterate over all subdirectories and files\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):  # bottom-up to ensure empty parent directories can also be removed\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            if not os.listdir(folder_path):  # Check if the folder is empty\n",
    "                os.rmdir(folder_path)  # Remove the empty folder\n",
    "                print(f\"Removed empty folder: {folder_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_directory_path' with the path of the folder you want to check\n",
    "find_and_remove_empty_folders('./Experiments/Tables')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
