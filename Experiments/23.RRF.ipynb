{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "Se nessuna delle domande è rilevante per il contesto del software fornisci come output solamente il tag: \"OUT-OF SCOPE\".\n",
    "\n",
    "**Esempi:**\n",
    "1. Domanda originale: \"Come posso specificare il tipo costo da assegnare al nuovo ambiente di commessa?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per definire il tipo di costo da applicare a un nuovo ambiente di commessa?\n",
    "   - Come posso configurare il tipo di costo per un nuovo ambiente di commessa nel sistema?\n",
    "   - Quali sono i passaggi per assegnare un tipo di costo a un ambiente di commessa appena creato?\n",
    "   - Dove posso trovare l'opzione per impostare il tipo di costo di un nuovo ambiente di commessa?\n",
    "   - Come posso scegliere il tipo di costo da associare a un ambiente di commessa nel mio sistema?\n",
    "\n",
    "2. Domanda originale: \"Come si fa la carbonara?\"\n",
    "   Risposta: OUT-OF SCOPE\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico. \n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model_gpt = ChatOpenAI(temperature=0, model=\"gpt-4o\") \n",
    "\n",
    "generate_queries = (\n",
    "    prompt \n",
    "    | model_gpt\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Buonasera, avrei la necessità di applicare per alcuni Clienti un Extra\n",
    "sconto per un periodo di tempo limitato (Mese di Novembre). Come posso procedere (se esiste la possibilità)?\"\"\"\n",
    "\n",
    "rewritten_question = generate_queries.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking of rewritten questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def compute_cosine_similarity(embedding, embeddings):\n",
    "    \"\"\"Computes cosine similarity between one vector and a set of vectors.\"\"\"\n",
    "    # Compute the dot product between the single vector and each vector in embeddings\n",
    "    dot_products = np.dot(embeddings, embedding)\n",
    "    \n",
    "    # Compute the norms\n",
    "    norm_embedding = np.linalg.norm(embedding)\n",
    "    norms_embeddings = np.linalg.norm(embeddings, axis=1)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = dot_products / (norm_embedding * norms_embeddings)\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def compute_bm25_score(original, rewrites):\n",
    "    \"\"\"Computes BM25 scores for original question against rewritten questions.\"\"\"\n",
    "    # Prepare documents for BM25\n",
    "    documents = [original] + rewrites\n",
    "    tokenized_documents = [doc.split(\" \") for doc in documents]\n",
    "    \n",
    "    # Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    \n",
    "    # Get BM25 scores for the original question against all rewrites\n",
    "    scores = bm25.get_scores(tokenized_documents[0])\n",
    "    \n",
    "    return scores[1:]  # Exclude the score for the original question\n",
    "\n",
    "def rerank_questions(original_question, alpha=0.5, threshold=1):\n",
    "    \"\"\"Rerank the rewritten questions based on cosine similarity and BM25 scores.\"\"\"\n",
    "    \n",
    "    # Load a pre-trained model\n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    original_embedding = model.encode(original_question)\n",
    "    rewritten_questions = generate_queries.invoke({\"question\": original_question})\n",
    "    rewritten_embeddings = model.encode(rewritten_questions)\n",
    "\n",
    "    # Convert list of embeddings to a numpy array\n",
    "    rewritten_embeddings = np.vstack(rewritten_embeddings)  # Stack into a single 2D array\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = compute_cosine_similarity(original_embedding, rewritten_embeddings)\n",
    "    \n",
    "    # Compute BM25 scores\n",
    "    bm25_scores = compute_bm25_score(original_question, rewritten_questions)\n",
    "\n",
    "    # Combine scores\n",
    "    weighted_scores = alpha * cosine_similarities + (1 - alpha) * bm25_scores\n",
    "    \n",
    "    # Create a ranking of rewritten questions\n",
    "    ranked_indices = np.argsort(weighted_scores)[::-1]  # Sort in descending order\n",
    "    \n",
    "    # Filter questions based on the threshold\n",
    "    filtered_questions = [(rewritten_questions[i], weighted_scores[i]) for i in ranked_indices if weighted_scores[i] >= threshold]\n",
    "    # EXPERIMENT IF IT WORKS BETTER WITH OR WITHOUT THE ORIGINAL QUESTION\n",
    "    questions = []\n",
    "    # questions = [original_question]\n",
    "    for question, score in filtered_questions:\n",
    "        print(question, score)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "    \n",
    "embd = M3EmbeddingFP16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"augmented_faiss_index\", embd, allow_dangerous_deserialization=True)\n",
    "vectorstore, vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            # k is a constant smoothing factor that prevents documents from being overly penalized for being far down the list\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative part of the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template_RAG_generation = \"\"\"   \n",
    "Comportati come un assistente che risponde alle domande del cliente.   \n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}.\n",
    "\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.   \n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata. \n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento,\n",
    "specificando, invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question} \n",
    "\"\"\"\n",
    "\n",
    "generation_prompt = ChatPromptTemplate.from_template(template_RAG_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline has to be: question rewriting, question ranking and filtering, then document retrieval for each question with reciprocal rank fusion algorithm, to rank all the extracted documents by the rrf computed score. We then use the documents, weighting them by their score, to use the context, the original question and the rewritten questions for the prompting for the generative part of the RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc[0].page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": rerank_questions | retriever.map() | reciprocal_rank_fusion | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generation_prompt\n",
    "    | model_gpt\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "pprint(question)\n",
    "pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Cosa significa che una fattura è in mancata consegna? Il cliente ha ricevuto la fattura?\"\n",
    "pprint(question)\n",
    "pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\"\n",
    "pprint(question)\n",
    "pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quando è morto Giulio Cesare?\"\n",
    "pprint(question)\n",
    "pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different chunking methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "csv_input_path = os.path.dirname(path) + \"/Doc_Panthera_Augmented/augmented_dataset_final_outputs.csv\"\n",
    "csv_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_input_path, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the contents\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "docs_data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs_data)\n",
    "pprint.pprint(splits[0:6])\n",
    "pprint.pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorstore = FAISS.from_documents(documents=splits, embedding=embd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorstore.save_local(\"recursive_augmented_faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retriever = new_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc[0].page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "new_rag_chain = (\n",
    "    {\"context\": rerank_questions | new_retriever.map() | reciprocal_rank_fusion | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generation_prompt\n",
    "    | model_gpt\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "pprint.pprint(question)\n",
    "pprint.pprint(new_rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Cosa significa che una fattura è in mancata consegna? Il cliente ha ricevuto la fattura?\"\n",
    "pprint.pprint(new_rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\"\n",
    "pprint.pprint(new_rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def baseline_format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "basic_rag_chain = (\n",
    "    {\"context\": new_retriever | baseline_format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generation_prompt\n",
    "    | model_gpt\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the two pipelines on a small testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_outputs_from_csv(csv_file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame with semicolon as the delimiter\n",
    "        df = pd.read_csv(csv_file_path, sep=';', encoding='utf-8')\n",
    "        print(f\"Data successfully loaded from {csv_file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No CSV file found at {csv_file_path}. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=[\"question\", \"answer\"])\n",
    "\n",
    "# Usage example - change the input file as needed\n",
    "eval_df = load_outputs_from_csv(\"real_evaluation_set.csv\")\n",
    "print(len(eval_df))\n",
    "display(eval_df.head())  # Display the first few rows of the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import E2E_Evaluation_metrics\n",
    "importlib.reload(E2E_Evaluation_metrics)\n",
    "from E2E_Evaluation_metrics import RAGEvaluator\n",
    "from E2E_Evaluation_metrics import SemScoreQueryRewriting\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "semscore = SemScoreQueryRewriting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_answers(generation_chain, df, model_name, chunking_type):\n",
    "    # Create a copy of the original dataframe to avoid modifying it\n",
    "    new_df = df.copy()\n",
    "    new_df['generated_answer'] = None\n",
    "    new_df['model'] = None\n",
    "    new_df['chunking'] = None\n",
    "\n",
    "    # Iterate through the dataframe and generate answers\n",
    "    for idx, elem in new_df.iterrows():\n",
    "        question = elem[\"question\"]\n",
    "        new_df.at[idx, 'generated_answer'] = generation_chain.invoke(question) \n",
    "        new_df.at[idx, 'model'] = model_name\n",
    "        new_df.at[idx, 'chunking'] = chunking_type\n",
    "\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = generate_answers(rag_chain, eval_df, 'gpt-4o', 'Semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = generate_answers(new_rag_chain, eval_df, 'gpt-4o', 'Recursive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(eval_df, evaluator):\n",
    "    results = []\n",
    "    for _, row in eval_df.iterrows():\n",
    "        response = row['generated_answer']\n",
    "        reference = row['answer']\n",
    "        \n",
    "        # Check if either response or reference is empty, and skip this row\n",
    "        if not response or not reference:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate and store the results\n",
    "        evaluation = evaluator.evaluate_all(response, reference)\n",
    "        results.append(evaluation)\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    eval_df = pd.DataFrame(results)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['BLEU', 'ROUGE-2', 'ROUGE-L', 'BERT P', 'BERT R', 'Perplexity', 'Diversity']\n",
    "\n",
    "def process_evaluation_and_metrics(data_frame, model_name, evaluator = evaluator, semscore = semscore, columns_to_drop = columns_to_drop):\n",
    "    \"\"\"\n",
    "    Evaluate responses, compute semantic scores, and merge results into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data_frame (pd.DataFrame): The input DataFrame with original and rewritten questions.\n",
    "    - evaluator (object): The evaluation object to compute BLEU, ROUGE, etc.\n",
    "    - semscore (object): The semantic score computation object.\n",
    "    - model_name (str): Name of the model for semantic similarity scoring.\n",
    "    - columns_to_drop (list): List of columns to drop from the evaluated DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with evaluation metrics and semantic scores.\n",
    "    \"\"\"\n",
    "    # Step 1: Evaluate responses\n",
    "    eval_df = evaluate_responses(data_frame, evaluator)\n",
    "    \n",
    "    # Step 2: Drop unnecessary columns\n",
    "    eval_df = eval_df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    # Step 3: Compute semantic scores\n",
    "    reference = \"answer\"\n",
    "    response = \"generated_answer\"\n",
    "    cosine_similarities_bge, _ = semscore.compute_sem_score(data_frame, model_name=model_name, reference=reference, response=response)\n",
    "    eval_df[\"SemScore\"] = cosine_similarities_bge[\"Cosine_Similarity\"]\n",
    "\n",
    "    # Step 4: Merge original DataFrame with evaluation metrics\n",
    "    merged_df = pd.concat([data_frame, eval_df], axis=1)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_1 = process_evaluation_and_metrics(\n",
    "    data_frame=df_1, \n",
    "    model_name='BAAI/bge-m3'\n",
    ")\n",
    "\n",
    "display(eval_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_2 = process_evaluation_and_metrics(\n",
    "    data_frame=df_2, \n",
    "    model_name='BAAI/bge-m3'\n",
    ")\n",
    "\n",
    "display(eval_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_value(df):\n",
    "    # Example: Assuming you have a dataframe named df\n",
    "    mean_rouge = df['ROUGE-1'].mean()\n",
    "    mean_bert = df['BERT F1'].mean()\n",
    "    mean_sem = df['SemScore'].mean()\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model:\", df['model'].unique(), \"with chunking of type:\", df[\"chunking\"].unique())\n",
    "    print(f\"Mean ROUGE-1: {mean_rouge}\")\n",
    "    print(f\"Mean BERT F1: {mean_bert}\")\n",
    "    print(f\"Mean SemScore: {mean_sem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_value(eval_df_1)\n",
    "print(\"\\n\")\n",
    "compute_average_value(eval_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = generate_answers(basic_rag_chain, eval_df, 'gpt-4o', 'Recursive with Basic Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_baseline = process_evaluation_and_metrics(\n",
    "    data_frame=df_baseline, \n",
    "    model_name='BAAI/bge-m3'\n",
    ")\n",
    "\n",
    "display(eval_df_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_value(eval_df_baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
