{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/final_dataset.csv\"\n",
    "\n",
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'text'\n",
    "text_list = data_df['text'].tolist()\n",
    "\n",
    "pprint.pprint(text_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "# Use this funciton to check for any tokenizer we decide to use\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens_in_documents(documents, token_limit=8192):\n",
    "    \"\"\"\n",
    "    Count the tokens in a list of documents and check if each document exceeds the token limit.\n",
    "\n",
    "    Args:\n",
    "        documents (list of str): List of documents (strings) to process.\n",
    "        token_limit (int): Maximum allowed tokens per document. Defaults to 8192.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the document index, token count, and a message\n",
    "              indicating if it exceeds the token limit.\n",
    "    \"\"\"\n",
    "    token_counts = []\n",
    "    count = 0\n",
    "    \n",
    "    # Process each document in the list\n",
    "    for idx, doc in enumerate(documents):\n",
    "        # Tokenize the document\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        # Check if the document exceeds the token limit\n",
    "        if num_tokens > token_limit:\n",
    "            message = f\"Document {idx + 1} has {num_tokens} tokens, exceeding the limit of {token_limit} tokens.\"\n",
    "            count += 1\n",
    "        else:\n",
    "            message = f\"Document {idx + 1} has {num_tokens} tokens, within the limit.\"\n",
    "        \n",
    "        # Append the result to the list\n",
    "        token_counts.append({\n",
    "            'document_index': idx + 1,\n",
    "            'token_count': num_tokens,\n",
    "            'message': message\n",
    "        })\n",
    "    \n",
    "    return token_counts, count\n",
    "\n",
    "token_info, count = count_tokens_in_documents(data_df['text'])\n",
    "\n",
    "# Print the result for each document\n",
    "print(f\"On a total of {data_df.shape[0]} documents, we have {count} documents, that exceed the maximum token limit.\")\n",
    "for info in token_info:\n",
    "    print(info['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI()\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "\n",
    "def chunk_text(text, max_tokens=8000):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    # Split into chunks that are within the max token limit\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_embedding_for_long_text(text, model=\"text-embedding-ada-002\", max_tokens=8000):\n",
    "    chunks = chunk_text(text, max_tokens)\n",
    "    embeddings = [client.embeddings.create(input=[chunk], model=model).data[0].embedding for chunk in chunks]\n",
    "    \n",
    "    # You can aggregate these embeddings, e.g., by averaging\n",
    "    avg_embedding = [sum(x)/len(x) for x in zip(*embeddings)]\n",
    "    \n",
    "    return avg_embedding\n",
    "\n",
    "data_df['embedding'] = data_df['text'].apply(lambda x: get_embedding_for_long_text(x))\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_embeddings(user_prompt, embedding_context, max_tokens=1024):\n",
    "    # Combine user prompt with relevant information from embeddings\n",
    "    prompt = f\"Contesto: {embedding_context}\\n\\n Prompt dell'user: {user_prompt}\\n\\n Risposta:\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "def find_most_similar_embedding(user_input, embeddings_df, model=\"text-embedding-ada-002\"):\n",
    "    user_embedding = get_embedding_for_long_text(user_input, model=model)\n",
    "    similarities = []\n",
    "    \n",
    "    for index, row in embeddings_df.iterrows():\n",
    "        embedding = row['embedding']  # Assuming the embeddings are stored as lists\n",
    "        similarity = cosine_similarity(user_embedding, embedding)\n",
    "        similarities.append((index, similarity))\n",
    "\n",
    "    most_similar_index, highest_similarity = max(similarities, key=lambda x: x[1])\n",
    "    return embeddings_df.iloc[most_similar_index], highest_similarity\n",
    "\n",
    "user_input = \"Cosa è un caricamento di massa per panthera? E come si può effettuare?\"\n",
    "most_similar_embedding, similarity_score = find_most_similar_embedding(user_input, data_df)\n",
    "\n",
    "generated_text = generate_text_with_embeddings(user_input, most_similar_embedding)\n",
    "print(\"Given the input: \", user_input)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "matrix = np.vstack(data_df.embedding.values)\n",
    "n_clusters = 5\n",
    "\n",
    "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "data_df['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\n",
    "vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "x = [x for x, y in vis_dims2]\n",
    "y = [y for x, y in vis_dims2]\n",
    "\n",
    "for category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\", 'gray']):\n",
    "    xs = np.array(x)[data_df.cluster == category]\n",
    "    ys = np.array(y)[data_df.cluster == category]\n",
    "    plt.scatter(xs, ys, color=color, alpha=0.3)\n",
    "\n",
    "    avg_x = xs.mean()\n",
    "    avg_y = ys.mean()\n",
    "\n",
    "    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "plt.title(\"Clusters identified visualized in language 2d using t-SNE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Reading a review which belongs to each group.\n",
    "rev_per_cluster = 5\n",
    "max_tokens_per_review = 1024  # Limit characters per review\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"Cluster {i} Theme:\", end=\" \")\n",
    "\n",
    "    # Sample reviews and truncate each review if necessary\n",
    "    reviews = data_df['text'][data_df.cluster == i].replace(\"Title: \", \"\").replace(\"\\n\\nContent: \", \":  \")\n",
    "    \n",
    "    # Sample reviews while ensuring the maximum character count is respected\n",
    "    sampled_reviews = reviews.sample(rev_per_cluster, random_state=42).values\n",
    "    truncated_reviews = [review[:max_tokens_per_review] for review in sampled_reviews]\n",
    "\n",
    "    reviews_text = \"\\n\".join(truncated_reviews)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f'Cosa hanno in comune i seguenti documenti?\\n\\nDocumenti Panthera:\\n\"\"\"\\n{reviews_text}\\n\"\"\"\\n\\nTema:'}\n",
    "    ]\n",
    "\n",
    "    # Check the token count before making the request\n",
    "    num_tokens = sum(len(review.split()) for review in truncated_reviews) + len(messages[0]['content'].split())\n",
    "    \n",
    "    if num_tokens > 8192:\n",
    "        print(f\"Warning: Message exceeds maximum token length: {num_tokens} tokens. Reduce review count or length.\")\n",
    "        continue\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "\n",
    "    sample_cluster_rows = data_df[data_df.cluster == i].sample(rev_per_cluster, random_state=42)\n",
    "\n",
    "    for j in range(rev_per_cluster):\n",
    "        print(sample_cluster_rows.file.values[j], ':', sample_cluster_rows.text.str[:100].values[j])\n",
    "\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation on cluster number and application on best clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matrix = np.vstack(data_df.embedding.values)\n",
    "\n",
    "# Define the range of n_clusters to try\n",
    "cluster_range = range(5, 16)  # Adjust this range based on your dataset\n",
    "silhouette_scores = []\n",
    "\n",
    "# Loop through each value of n_clusters\n",
    "for n_clusters in cluster_range:\n",
    "    # Initialize and fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "    kmeans.fit(matrix)\n",
    "    \n",
    "    # Compute silhouette score\n",
    "    score = silhouette_score(matrix, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f'For n_clusters = {n_clusters}, silhouette score = {score:.4f}')\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(cluster_range)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Determine the best n_clusters based on the highest silhouette score\n",
    "best_n_clusters = cluster_range[np.argmax(silhouette_scores)]\n",
    "print(f'Best number of clusters: {best_n_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "matrix = np.vstack(data_df.embedding.values)\n",
    "n_clusters = 7\n",
    "\n",
    "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "data_df['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'matrix' and 'data_df' are already defined\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components= 2, perplexity=15, random_state=42, init=\"pca\", learning_rate=200)\n",
    "vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "# Extract the x and y coordinates from the t-SNE result\n",
    "x = vis_dims2[:, 0]\n",
    "y = vis_dims2[:, 1]\n",
    "\n",
    "# Determine unique clusters in the data\n",
    "unique_clusters = np.unique(data_df.cluster)\n",
    "n_clusters = len(unique_clusters)  # Get the number of unique clusters\n",
    "\n",
    "colors = plt.cm.get_cmap(\"tab10\", n_clusters)  # You can change 'tab10' to other colormaps\n",
    "\n",
    "# Plot each cluster\n",
    "for category in unique_clusters:\n",
    "    xs = x[data_df.cluster == category]\n",
    "    ys = y[data_df.cluster == category]\n",
    "    \n",
    "    plt.scatter(xs, ys, color=colors(category), alpha=0.3)\n",
    "\n",
    "    # Compute and plot the average position for each cluster\n",
    "    avg_x = xs.mean()\n",
    "    avg_y = ys.mean()\n",
    "    plt.scatter(avg_x, avg_y, marker=\"x\", color=colors(category), s=100)\n",
    "\n",
    "plt.title(\"Clusters identified visualized in 2D using t-SNE\")\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting toolkit\n",
    "\n",
    "# Assuming 'matrix' and 'data_df' are already defined\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=3, perplexity=15, random_state=42, init=\"pca\", learning_rate=200)\n",
    "vis_dims3 = tsne.fit_transform(matrix)  # Get 3D embeddings\n",
    "\n",
    "# Extract the x, y, and z coordinates from the t-SNE result\n",
    "x = vis_dims3[:, 0]\n",
    "y = vis_dims3[:, 1]\n",
    "z = vis_dims3[:, 2]\n",
    "\n",
    "# Determine unique clusters in the data\n",
    "unique_clusters = np.unique(data_df.cluster)\n",
    "n_clusters = len(unique_clusters)  # Get the number of unique clusters\n",
    "\n",
    "# Create a color map for clusters\n",
    "colors = plt.cm.get_cmap(\"tab10\", n_clusters)  # You can change 'tab10' to other colormaps\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each cluster\n",
    "for category in unique_clusters:\n",
    "    xs = x[data_df.cluster == category]\n",
    "    ys = y[data_df.cluster == category]\n",
    "    zs = z[data_df.cluster == category]\n",
    "    \n",
    "    ax.scatter(xs, ys, zs, color=colors(category), alpha=0.5, label=f'Cluster {category}')\n",
    "\n",
    "    # Compute and plot the average position for each cluster\n",
    "    avg_x = xs.mean()\n",
    "    avg_y = ys.mean()\n",
    "    avg_z = zs.mean()\n",
    "    ax.scatter(avg_x, avg_y, avg_z, marker=\"x\", color=colors(category), s=100)\n",
    "\n",
    "# Set titles and labels\n",
    "ax.set_title(\"Clusters identified visualized in 3D using t-SNE\")\n",
    "ax.set_xlabel(\"t-SNE Component 1\")\n",
    "ax.set_ylabel(\"t-SNE Component 2\")\n",
    "ax.set_zlabel(\"t-SNE Component 3\")\n",
    "ax.legend()  # Show legend for clusters\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Reading a review which belongs to each group.\n",
    "rev_per_cluster = 5\n",
    "max_tokens_per_review = 1024  # Limit characters per review\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"Cluster {i} Theme:\", end=\" \")\n",
    "\n",
    "    # Sample reviews and truncate each review if necessary\n",
    "    reviews = data_df['text'][data_df.cluster == i].replace(\"Title: \", \"\").replace(\"\\n\\nContent: \", \":  \")\n",
    "    \n",
    "    # Sample reviews while ensuring the maximum character count is respected\n",
    "    sampled_reviews = reviews.sample(rev_per_cluster, random_state=42).values\n",
    "    truncated_reviews = [review[:max_tokens_per_review] for review in sampled_reviews]\n",
    "\n",
    "    reviews_text = \"\\n\".join(truncated_reviews)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f'Cosa hanno in comune i seguenti documenti?\\n\\nDocumenti Panthera:\\n\"\"\"\\n{reviews_text}\\n\"\"\"\\n\\nTema:'}\n",
    "    ]\n",
    "\n",
    "    # Check the token count before making the request\n",
    "    num_tokens = sum(len(review.split()) for review in truncated_reviews) + len(messages[0]['content'].split())\n",
    "    \n",
    "    if num_tokens > 8192:\n",
    "        print(f\"Warning: Message exceeds maximum token length: {num_tokens} tokens. Reduce review count or length.\")\n",
    "        continue\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "\n",
    "    sample_cluster_rows = data_df[data_df.cluster == i].sample(rev_per_cluster, random_state=42)\n",
    "\n",
    "    for j in range(rev_per_cluster):\n",
    "        print(sample_cluster_rows.file.values[j], ':', sample_cluster_rows.text.str[:100].values[j])\n",
    "\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "rev_per_cluster = 5  # Reviews to sample per cluster\n",
    "max_tokens_per_review = 1024  # Limit characters per review\n",
    "max_tokens_per_prompt = 8192  # GPT-4 max token limit\n",
    "output_text = \"\"\n",
    "\n",
    "# Iterate over clusters\n",
    "for i in range(n_clusters):\n",
    "    output_text += f\"### Cluster {i} Theme:\\n\"\n",
    "    \n",
    "    # Get reviews for the current cluster\n",
    "    reviews = data_df['text'][data_df.cluster == i].replace(\"Title: \", \"\").replace(\"\\n\\nContent: \", \":  \")\n",
    "    sampled_reviews = reviews.sample(rev_per_cluster, random_state=42).values\n",
    "    \n",
    "    # Truncate reviews to respect max token limits\n",
    "    truncated_reviews = [review[:max_tokens_per_review] for review in sampled_reviews]\n",
    "    reviews_text = \"\\n\".join(truncated_reviews)\n",
    "    \n",
    "    # Prepare the GPT prompt\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'Cosa hanno in comune i seguenti documenti?\\n\\nDocumenti Panthera:\\n\"\"\"\\n{reviews_text}\\n\"\"\"\\n\\nTema:'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Calculate token count\n",
    "    num_tokens = sum(len(review.split()) for review in truncated_reviews) + len(messages[0]['content'].split())\n",
    "    \n",
    "    if num_tokens > max_tokens_per_prompt:\n",
    "        print(f\"Warning: Message exceeds maximum token length: {num_tokens} tokens. Reduce review count or length.\")\n",
    "        continue\n",
    "    \n",
    "    # Get theme description using GPT-4\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    theme_description = response.choices[0].message.content.strip().replace(\"\\n\", \" \")\n",
    "    output_text += theme_description + \"\\n\\n\"\n",
    "\n",
    "    all_cluster_rows = data_df[data_df.cluster == i]\n",
    "    \n",
    "    output_text += \"#### Document Titles (All Documents in Cluster):\\n\"\n",
    "    for title in all_cluster_rows.file.values:\n",
    "        output_text += f\"- {title}\\n\"\n",
    "    \n",
    "    output_text += \"\\n\" + \"-\" * 100 + \"\\n\\n\"\n",
    "\n",
    "# Write the output text to a file\n",
    "with open('clustered_documents_summary.txt', 'w') as f:\n",
    "    f.write(output_text)\n",
    "\n",
    "print(\"Document created: 'clustered_documents_summary.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data_df[['text']])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "\n",
    "# Load MBart model and tokenizer\n",
    "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
    "#\"NousResearch/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data_df['text'].tolist()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "retriever_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Generate embeddings for all the documents (texts from your df_data['text'])\n",
    "doc_embeddings = retriever_model.encode(docs)\n",
    "\n",
    "# Build a FAISS index for fast similarity search\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])  # Use L2 distance\n",
    "index.add(doc_embeddings)  # Add the document embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, top_k=5):\n",
    "    # Encode the query using the same model\n",
    "    query_embedding = retriever_model.encode([query])\n",
    "\n",
    "    # Perform the search, retrieving the top_k relevant documents\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Return the relevant document texts and their corresponding files\n",
    "    retrieved_texts = [docs[i] for i in indices[0]]\n",
    "    retrieved_files = [data_df['file'].iloc[i] for i in indices[0]]  # File names\n",
    "    return retrieved_texts, retrieved_files\n",
    "\n",
    "# Example usage\n",
    "query = \"Come funziona il fast update in Panthera?\"\n",
    "retrieved_texts, retrieved_files = retrieve_docs(query)\n",
    "pprint.pprint(f\"Documenti rilevanti: {retrieved_files}\")\n",
    "pprint.pprint(f\"Contenuto estratto: {retrieved_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, retrieved_texts):\n",
    "    # Combine the user query and the retrieved documents\n",
    "    input_text = query + \" \" + \" \".join(retrieved_texts)  # Limit to top 3 texts for context\n",
    "    \n",
    "    # Tokenize and generate the response\n",
    "    tokenizer.src_lang = \"it_IT\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the response with improved generation settings\n",
    "    output = model.generate(**inputs, \n",
    "                            forced_bos_token_id=tokenizer.lang_code_to_id[\"it_IT\"],\n",
    "                            max_length=100, \n",
    "                            num_beams=2, \n",
    "                            length_penalty=2.0, \n",
    "                            no_repeat_ngram_size=3, \n",
    "                            top_k=50, \n",
    "                            top_p=0.95, \n",
    "                            temperature=0.2)\n",
    "    \n",
    "    # Decode and return the generated response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "answer = generate_answer(query, retrieved_texts)\n",
    "pprint.pprint(f\"Risposta: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on a single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "cluster = pd.DataFrame(data_df[data_df.cluster == 0]['text'])\n",
    "cluster['file'] = data_df[data_df.cluster == 0]['file']\n",
    "loader = DataFrameLoader(cluster, page_content_column=\"text\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=600)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "pprint.pprint(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Come funziona la gestione delle finestre modali in Chrome?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente. \n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se non sai rispondere, di soltanto che non detieni queste informationi e che è necessario contattare l'assistenza del helpdesk.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM - the used model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "response_text = rag_chain.invoke(question)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_out_of_scope = \"Come posso trovare informazioni su Giulio Cesare dalle finestre modali di Chrome?\"\n",
    "pprint.pprint(rag_chain.invoke(question_out_of_scope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query translation - Multi query - rewriting of the query in 5 different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions, in Italian, of the given user question to retrieve relevant documents from a vector database. \n",
    "The context of our application is related to Enterprise Resource Planning (ERP) software's technical manuals (specifically Panthera software) or, more generally, topics related to computer science, including system configuration,\n",
    "module functionality, troubleshooting, and implementation guidelines.\n",
    "Your goal is to generate multiple perspectives on the question to help the user overcome limitations of distance-based similarity search while focusing strictly on the context of ERP software documentation\n",
    "or relevant computer science topics.\n",
    "In cases where the user provides multiple questions, only respond to the relevant ones related to ERP documentation or computer science. Provide these alternative questions separated by newlines.\n",
    "Before generating alternatives, ensure the user's question is related to ERP technical documentation or relevant computer science topics. \n",
    "If any of the questions are out of scope or irrelevant to ERP manuals or computer science topics, disregard them entirely. \n",
    "You don't need to ignore all the questions, but only the ones that are out of scope.\n",
    "Provide the created alternative questions separated by newlines, and structure the output to contain only the rewritten questions in a bullet list.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente. \n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuaziore l'azione desiderata. \n",
    "\n",
    "Se la domanda non è inerente al contesto della documentazione, rispondi soltanto che la domanda non è pertinente al contesto. \n",
    "Non rispondere mai alle domande non inerenti alla documentazione. Rispondere solamente che: \"La domanda non è pertinente in questo contesto.\"\n",
    "\n",
    "Se non sai rispondere alle domande relative alla documentazione, rispondi soltanto che non detieni queste informationi e che è necessario contattare l'assistenza del helpdesk.\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint.pprint(final_rag_chain.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_out_of_scope = \"Entro quando devo liquidare l'iva in Italia?\"\n",
    "pprint.pprint(final_rag_chain.invoke({\"question\":question_out_of_scope}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"Come funziona la gestione delle finestre modali in Chrome?\"\n",
    "pprint.pprint(final_rag_chain.invoke({\"question\" : question1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_out_of_scope = \"Come funziona la gestione delle finestre modali in Chrome? Chi è Giulio Cesare? \"\n",
    "pprint.pprint(final_rag_chain.invoke({\"question\":question_out_of_scope}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_multiple = \"Come funziona la gestione delle finestre modali in Chrome? E come posso fare il rinnovo dei certificati dello sdi? \"\n",
    "pprint.pprint(final_rag_chain.invoke({\"question\":questions_multiple}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query translation - RAG Fusion - rerank the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint.pprint(rerank_rag_chain.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query translation - Decomposition and recursive answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"Sei un assistente utile che genera più sotto-domande relative a una domanda di input. \\n\n",
    "L'obiettivo è suddividere l'input in un insieme di sotto-problemi che possono essere risolti isolatamente, nel contesto del software Panthera. \\n\n",
    "Genera più query di ricerca relative a: {question} \\n\n",
    "Output (3 query):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)\n",
    "pprint.pprint(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Ecco la domanda a cui devi rispondere:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Ecco le eventuali coppie di domande + risposte disponibili:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Ecco il contesto aggiuntivo rilevante per la domanda:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Usa il contesto sopra e le coppie di domande + risposte per rispondere alla domanda: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Formato coppia domanda e risposta: \"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Domanda: {question}\\nRisposta: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "\n",
    "pprint.pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not recursive answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG per ogni sotto-domanda\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente. \n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se non sai rispondere, di soltanto che non detieni queste informationi e che è necessario contattare l'assistenza del helpdesk.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "prompt_rag = ChatPromptTemplate.from_template(template)\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Formato coppia domdanda e risposta:\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Domanda {i}: {question}\\nRisposta {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Ecco un set di domande e risposte sull'argomento:\n",
    "\n",
    "{context}\n",
    "\n",
    "Usale per rispondere alla domanda: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint.pprint(final_rag_chain.invoke({\"context\":context,\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other alternatives of question translation, that could be implemented, are: \n",
    "\n",
    "- Step back prompting: giving some few shot examples of abstraction of the given query, it takes the question of the user and rewrites it in a more abstract way, enlarging the context and the response - but, I think this only confuses the model and doesn't give an on point answer to the customer\n",
    "    \n",
    "- HyDE: giving the question in asks to a model, accurately prompting it, to create a document for the question, and this hypothetical document is then compared to the documents retrieved from the dataset, to get the most similar ones, based on which we generate the response - but, given that the subject of interest is very specific to our software, the creation of the hypothetical document is difficult and could confuse the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possible useful extensions:\n",
    "\n",
    "- Logical Routing (to different clusters), saved in different vector stores, so route an user query to the most relevant data source (one per cluster)\n",
    "\n",
    "- Semantic Routing, based on similarity of the query to the prompting, differentiating it based on the cluster \n",
    "\n",
    "- Query structuring, using the metadata (ex. version of Panthera) with different vectorestores, and the metadata could be used only if explicitly specified, or if extracted every time from the user request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing - Multi representation Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "cluster = pd.DataFrame(data_df[data_df.cluster == 0]['text'])\n",
    "cluster['file'] = data_df[data_df.cluster == 0]['file']\n",
    "loader = DataFrameLoader(cluster, page_content_column=\"text\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Riassumi dettagliatamente i seguenti documenti:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\",max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(query,k=3)\n",
    "sub_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
