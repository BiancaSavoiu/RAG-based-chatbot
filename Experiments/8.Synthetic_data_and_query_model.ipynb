{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "question_out_of_scope = \"Quando è morto Giulio Cesare?\"\n",
    "multiple_questions = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione? Chi è Giulio Cesare?\"\n",
    "multiple_valid_questions = \"Cosa significa che una fattura è in mancata consegna? Il cliente ha ricevuto la fattura?\"\n",
    "q_client = \"Addebito bollo su nota credito. Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_client_without_object = \"Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_rewritten = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3.2-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Riformula la frase fornita in 5 modi diversi, mantenendo il senso della frase e rendendola più chiara. \n",
    "In output mostra solo la lista delle domande riformulate, senza altro testo o commenti.\n",
    "\n",
    "Domanda originale: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "\n",
    "response = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries_gpt = (\n",
    "    prompt_perspectives\n",
    "    | ChatOpenAI(temperature=0, model=\"gpt-4o\") \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Llama3.2-instruct question rewriting with basic prompt\")\n",
    "pprint.pprint(response.invoke({\"question\": question}))\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"GPT-4o question rewriting with basic prompt\")\n",
    "pprint.pprint(generate_queries_gpt.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(q_client)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Llama3.2-instruct question rewriting with basic prompt\")\n",
    "pprint.pprint(response.invoke({\"question\": q_client}))\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"GPT-4o question rewriting with basic prompt\")\n",
    "pprint.pprint(generate_queries_gpt.invoke({\"question\": q_client}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics to compare the models for query re-writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def clarity_score(text: str) -> float:\n",
    "    \"\"\"Evaluate clarity based on Italian readability scores using the Gulpease Index.\"\"\"\n",
    "    # Calculate the Gulpease Index for Italian text\n",
    "    readability = textstat.gulpease_index(text)\n",
    "    # Translate score to a scale of 1-5\n",
    "    if readability >= 60:\n",
    "        return 5\n",
    "    elif readability >= 40:\n",
    "        return 4\n",
    "    elif readability >= 20:\n",
    "        return 3\n",
    "    elif readability >= 10:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def variety_score(rewrites: List[str]) -> float:\n",
    "    \"\"\"Evaluate variety among a list of rewrites in Italian.\"\"\"\n",
    "    vectorizer = CountVectorizer(analyzer='word').fit_transform(rewrites)\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    # Compute pairwise cosine similarities\n",
    "    cos_sim_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Compute variety as inverse of average similarity\n",
    "    avg_similarity = (np.sum(cos_sim_matrix) - len(rewrites)) / (len(rewrites) * (len(rewrites) - 1))\n",
    "    variety = 1 - avg_similarity  # Higher value indicates more variety\n",
    "    \n",
    "    # Scale to 1-5\n",
    "    if variety >= 0.8:\n",
    "        return 5\n",
    "    elif variety >= 0.6:\n",
    "        return 4\n",
    "    elif variety >= 0.4:\n",
    "        return 3\n",
    "    elif variety >= 0.2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def relevance_score(original: str, rewrite: str) -> float:\n",
    "    \"\"\"Evaluate relevance for Italian text based on keyword retention.\"\"\"\n",
    "    # Extract keywords from the original text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    original_vector = vectorizer.fit_transform([original])\n",
    "    rewrite_vector = vectorizer.transform([rewrite])\n",
    "    \n",
    "    # Compute cosine similarity on the keyword vector\n",
    "    similarity = cosine_similarity(original_vector, rewrite_vector).item()\n",
    "    # Translate similarity to a scale of 1-5\n",
    "    if similarity >= 0.75:\n",
    "        return 5\n",
    "    elif similarity >= 0.5:\n",
    "        return 4\n",
    "    elif similarity >= 0.25:\n",
    "        return 3\n",
    "    elif similarity >= 0.1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "# Initialize the language tool for Italian\n",
    "tool = language_tool_python.LanguageTool('it-IT')\n",
    "\n",
    "def fluency_score(text: str) -> float:\n",
    "    \"\"\"Evaluate fluency based on grammar and syntax errors in Italian.\"\"\"\n",
    "    # Get list of errors from language tool\n",
    "    errors = tool.check(text)\n",
    "    num_errors = len(errors)\n",
    "    # Scale inversely with the number of errors (fewer errors = higher score)\n",
    "    if num_errors == 0:\n",
    "        return 5\n",
    "    elif num_errors <= 1:\n",
    "        return 4\n",
    "    elif num_errors <= 3:\n",
    "        return 3\n",
    "    elif num_errors <= 5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a Sentence-BERT model optimized for Italian\n",
    "model = SentenceTransformer('dbmdz/bert-base-italian-xxl-cased')\n",
    "\n",
    "def concept_retention_score(original: str, rewrite: str) -> float:\n",
    "    \"\"\"Evaluate concept retention for Italian text based on semantic similarity.\"\"\"\n",
    "    # Generate embeddings\n",
    "    original_embedding = model.encode(original, convert_to_tensor=True)\n",
    "    rewrite_embedding = model.encode(rewrite, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = util.pytorch_cos_sim(original_embedding, rewrite_embedding).item()\n",
    "    # Translate similarity to a scale of 1-5\n",
    "    if similarity >= 0.9:\n",
    "        return 5\n",
    "    elif similarity >= 0.75:\n",
    "        return 4\n",
    "    elif similarity >= 0.6:\n",
    "        return 3\n",
    "    elif similarity >= 0.45:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rewrites(original: str, rewrites: List[str]) -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple rewrites of a given original text.\n",
    "    \n",
    "    Args:\n",
    "    - original (str): The original query.\n",
    "    - rewrites (List[str]): A list of rewritten queries.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Dict[str, float]]: A list of scores for each rewrite.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for rewrite in rewrites:\n",
    "        score = {\n",
    "            'clarity': clarity_score(rewrite),\n",
    "            'concept_retention': concept_retention_score(original, rewrite),\n",
    "            'variety': variety_score(rewrites),\n",
    "            'fluency': fluency_score(rewrite),\n",
    "            'relevance': relevance_score(original, rewrite)\n",
    "        }\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_scores(scores: List[Dict[str, float]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Aggregate scores from individual evaluations.\n",
    "    \n",
    "    Args:\n",
    "    - scores (List[Dict[str, float]]): A list of score dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "    - List[float]: A list of aggregated scores for each rewrite.\n",
    "    \"\"\"\n",
    "    aggregated_scores = []\n",
    "    \n",
    "    for score in scores:\n",
    "        avg_score = sum(score.values()) / len(score)\n",
    "        aggregated_scores.append(avg_score)\n",
    "    \n",
    "    return aggregated_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = q_client\n",
    "\n",
    "# Assuming `model_rewrites` is a dictionary containing rewrites from different models\n",
    "model_rewrites = {\n",
    "    \"Llama 3.2-Instruct\": {\n",
    "        \"prompt_language\": \"Italian\",\n",
    "        \"outputs\": [response.invoke({\"question\": original_query})],\n",
    "    },\n",
    "    \"GPT-4o\": {\n",
    "        \"prompt_language\": \"Italian\",\n",
    "        \"outputs\": [generate_queries_gpt.invoke({\"question\": original_query})],\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, rewrites in model_rewrites.items():\n",
    "    scores = evaluate_rewrites(original_query, rewrites)\n",
    "    aggregated_scores = aggregate_scores(scores)\n",
    "    results[model_name] = aggregated_scores\n",
    "\n",
    "# Now `results` contains average scores for each model\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Initialize tools and models for Italian\n",
    "grammar_tool = language_tool_python.LanguageTool('it')  # Specify Italian language\n",
    "similarity_model = SentenceTransformer('stsb-xlm-r-multilingual')  # A multilingual model for sentence similarity\n",
    "\n",
    "# Metric 1: Grammatical Correctness (Italian)\n",
    "def evaluate_grammar_italian(text):\n",
    "    matches = grammar_tool.check(text)\n",
    "    error_count = len(matches)\n",
    "    word_count = len(text.split())\n",
    "    grammar_score = max(1 - (error_count / word_count), 0)  # Score between 0 and 1\n",
    "    return grammar_score\n",
    "\n",
    "# Metric 2: Clarity (Italian)\n",
    "def evaluate_clarity_italian(text):\n",
    "    # Readability indices are often language-specific, and textstat doesn’t directly support Italian.\n",
    "    # For simplicity, we can use sentence and word length as a proxy for readability.\n",
    "    # An alternative is the Gulpease Index, specifically for Italian.\n",
    "    def gulpease_index(text):\n",
    "        words = len(text.split())\n",
    "        sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "        letters = sum(1 for c in text if c.isalpha())\n",
    "        if words == 0 or sentences == 0: return 0  # Avoid division by zero\n",
    "        return (89 - (10 * letters / words) + (300 * sentences / words))\n",
    "\n",
    "    gulpease_score = gulpease_index(text)\n",
    "    clarity_score = min(max((gulpease_score - 20) / 70, 0), 1)  # Normalize to 0-1 range\n",
    "    return clarity_score\n",
    "\n",
    "# Metric 3: Conciseness (Italian)\n",
    "def evaluate_conciseness(text, max_word_count=50):\n",
    "    word_count = len(text.split())\n",
    "    conciseness_score = max(1 - (word_count / max_word_count), 0) if word_count > max_word_count else 1\n",
    "    return conciseness_score\n",
    "\n",
    "# Metric 4: Relevance (Italian)\n",
    "def evaluate_relevance(response, question):\n",
    "    # Compute embeddings and similarity for Italian using multilingual model\n",
    "    embeddings = similarity_model.encode([response, question], convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    relevance_score = float(similarity[0][0])  # Cosine similarity score between 0 and 1\n",
    "    return relevance_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def evaluate_rewrites(original: str, rewrites: List[str]) -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple rewrites of a given original text.\n",
    "    \n",
    "    Args:\n",
    "    - original (str): The original query.\n",
    "    - rewrites (List[str]): A list of rewritten queries.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Dict[str, float]]: A list of scores for each rewrite.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for rewrite in rewrites:\n",
    "        score = {\n",
    "            'grammar': evaluate_grammar_italian(rewrite),   # Grammatical Correctness\n",
    "            'clarity': evaluate_clarity_italian(rewrite),     # Clarity\n",
    "            'conciseness': evaluate_conciseness(rewrite),      # Conciseness\n",
    "            'relevance': evaluate_relevance(rewrite, original)  # Relevance to the original question\n",
    "        }\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_scores(scores: List[Dict[str, float]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Aggregate scores from individual evaluations.\n",
    "    \n",
    "    Args:\n",
    "    - scores (List[Dict[str, float]]): A list of score dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "    - List[float]: A list of aggregated scores for each rewrite.\n",
    "    \"\"\"\n",
    "    aggregated_scores = []\n",
    "    \n",
    "    for score in scores:\n",
    "        avg_score = sum(score.values()) / len(score)\n",
    "        aggregated_scores.append(avg_score)\n",
    "    \n",
    "    return aggregated_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = q_client\n",
    "\n",
    "# Assuming `model_rewrites` is a dictionary containing rewrites from different models\n",
    "model_rewrites = {\n",
    "    \"Llama 3.2-Instruct\": {\n",
    "        \"prompt_language\": \"Italian\",\n",
    "        \"outputs\": [response.invoke({\"question\": original_query})],\n",
    "    },\n",
    "    \"GPT-4o\": {\n",
    "        \"prompt_language\": \"Italian\",\n",
    "        \"outputs\": [generate_queries_gpt.invoke({\"question\": original_query})],\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, rewrites in model_rewrites.items():\n",
    "    scores = evaluate_rewrites(original_query, rewrites)\n",
    "    aggregated_scores = aggregate_scores(scores)\n",
    "    results[model_name] = aggregated_scores\n",
    "\n",
    "# Now `results` contains average scores for each model\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a synthetic dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename_all_data_dict = \"./Files/final_dataset.csv\"\n",
    "\n",
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"text\")\n",
    "docs_data = loader.load()\n",
    "docs_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split\n",
    "# Possible improvements - future hypertuning of chunk_size and chunk_overlap to improve results and try different slitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = []\n",
    "for doc in docs_data:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "docs_processed[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "access_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "token_pro = os.getenv('HUGGINGFACE_TOKEN')\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=token_pro\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1024},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Il tuo compito è scrivere una domanda e una risposta data un contesto.\n",
    "La tua domanda deve essere rispondibile con un'informazione specifica dal contesto. Se nel contesto ci sono errori grammaticali o morfologici correggili nell'output fornito.\n",
    "La tua domanda deve essere formulata nello stesso stile delle domande che gli utenti potrebbero porre ad un helpdesk, che si occupa di assistenza clienti per un software aziendale.\n",
    "Questo significa che la tua domanda NON deve menzionare frasi come \"secondo il passaggio\" o \"nel contesto\". \n",
    "La tua domanda può menzionare frasi come \"Ho un errore\" o \"Come posso sistemare il problema\".\n",
    "\n",
    "Domanda e risposta devono essere generate in italiano.\n",
    "\n",
    "Fornisci la tua risposta come segue:\n",
    "\n",
    "Output:::\n",
    "Domanda: (la tua domanda)\n",
    "Risposta: (la tua risposta alla domanda)\n",
    "\n",
    "Ora ecco il contesto.\n",
    "\n",
    "Contesto: {context}\\n\n",
    "Output:::\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "document = random.sample(docs_processed, 1)\n",
    "pprint.pprint(document)\n",
    "for sampled_document in document:\n",
    "    pprint.pprint(call_llm(llm_client, QA_generation_prompt.format(context=sampled_document.page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_GENERATIONS = 100\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Domanda: \")[-1].split(\"Risposta: \")[0]\n",
    "        answer = output_QA_couple.split(\"Risposta: \")[-1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"source_doc\": sampled_context.metadata[\"file\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_next_run_filename(base_name='generated_QA_couples', directory='.', extension='csv'):\n",
    "    # Find all files with the base name and extension in the specified directory\n",
    "    existing_files = [f for f in os.listdir(directory) if f.startswith(base_name) and f.endswith(f'.{extension}')]\n",
    "    \n",
    "    # Extract run numbers from file names and find the maximum\n",
    "    run_numbers = [int(re.search(rf'{base_name}_(\\d+)\\.{extension}', f).group(1)) for f in existing_files if re.search(rf'{base_name}_(\\d+)\\.{extension}', f)]\n",
    "    next_run_number = max(run_numbers) + 1 if run_numbers else 1\n",
    "    \n",
    "    # Construct the new file name\n",
    "    return os.path.join(directory, f\"{base_name}_{next_run_number}.{extension}\")\n",
    "\n",
    "def save_outputs_to_csv(outputs, base_name='generated_QA_couples'):\n",
    "    # Generate the next run file name\n",
    "    csv_file_path = get_next_run_filename(base_name=base_name)\n",
    "    \n",
    "    # Define the column headers\n",
    "    fieldnames = [\"question\", \"answer\", \"context\", \"source_doc\"]\n",
    "\n",
    "    # Write only the new outputs to a new CSV file with headers\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames, delimiter=';')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(outputs)\n",
    "    \n",
    "    print(f\"Data successfully saved to {csv_file_path}\")\n",
    "\n",
    "# Usage example for saving new outputs\n",
    "save_outputs_to_csv(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a critique agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "Sarà fornito un contesto e una domanda.\n",
    "Il tuo compito è di fornire una valutazione per indicare quanto bene si possa rispondere in modo univoco alla domanda data con il contesto fornito.\n",
    "Dai la tua risposta su una scala da 1 a 5, dove 1 significa che la domanda non è affatto rispondibile con il contesto, \n",
    "e 5 significa che la domanda è chiaramente e univocamente rispondibile con il contesto.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, come numero tra 1 e 5)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda e il contesto.\n",
    "\n",
    "Domanda: {question}\n",
    "Contesto: {context}\n",
    "\n",
    "Output:::\n",
    "\"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "Ti sarà fornita una domanda.\n",
    "Il tuo compito è di fornire una \"valutazione totale\" che rappresenti quanto utile possa essere questa domanda per gli utenti che chiedono assistenza all'help desk riguardo a specifiche funzionalità\n",
    "del software gestionale e la relativa documentazione.\n",
    "Dai la tua risposta su una scala da 1 a 5, dove 1 significa che la domanda non è per nulla utile, e 5 significa che la domanda è estremamente utile.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, come numero tra 1 e 5)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda.\n",
    "\n",
    "Domanda: {question}\n",
    "\n",
    "Output:::\n",
    "\"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "Ti sarà fornita una domanda.\n",
    "Il tuo compito è di fornire una \"valutazione totale\" che rappresenti quanto questa domanda sia indipendente dal contesto.\n",
    "Dai la tua risposta su una scala da 1 a 5, dove 1 significa che la domanda dipende da informazioni aggiuntive per essere compresa, e 5 significa che la domanda ha senso da sola.\n",
    "Ad esempio, se la domanda si riferisce a un contesto particolare, come \"nel contesto\" o \"nel documento\", la valutazione deve essere 1.\n",
    "Le domande possono contenere termini tecnici o acronimi e ricevere comunque una valutazione di 5: deve semplicemente essere chiaro per un operatore con accesso alla documentazione di cosa tratta la domanda.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, come numero tra 1 e 5)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda.\n",
    "\n",
    "Domanda: {question}\n",
    "\n",
    "Output:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"groundedness_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"relevance_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"standalone_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Create DataFrame after ensuring all columns are initialized\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "# Calculate the average score across the three columns\n",
    "generated_questions[\"average_score\"] = (\n",
    "    generated_questions[\"groundedness_score\"] \n",
    "    + generated_questions[\"relevance_score\"]\n",
    "    + generated_questions[ \"standalone_score\"]\n",
    ")/3\n",
    "\n",
    "# Filter to keep rows where the average score is greater than 4\n",
    "generated_questions = generated_questions.loc[generated_questions[\"average_score\"] > 4]\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "            \"average_score\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the dataset from filtered DataFrame\n",
    "new_eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append new data in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing dataset\n",
    "existing_dataset = Dataset.load_from_disk(\"eval_dataset\")\n",
    "old_elem = pd.DataFrame(existing_dataset)\n",
    "new_elem = pd.DataFrame(new_eval_dataset)\n",
    "\n",
    "# Concatenate the old and new DataFrames\n",
    "combined_df = pd.concat([old_elem, new_elem], ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Convert the combined DataFrame back to a Dataset\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataset to disk again\n",
    "combined_dataset.save_to_disk(\"eval_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
