{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing questions set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "question_out_of_scope = \"Quando è morto Giulio Cesare?\"\n",
    "multiple_valid_questions = \"Cosa significa che una fattura è in mancata consegna? Il cliente ha ricevuto la fattura?\"\n",
    "q_client = \"Addebito bollo su nota credito. Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_client_without_object = \"Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_rewritten = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'file_path.csv' with the path to your CSV file\n",
    "file_path = 'real_evaluation_set.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, sep = ';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [question, question_out_of_scope, multiple_valid_questions, q_client, q_client_without_object, q_rewritten]\n",
    "questions_list = df['question'].tolist()\n",
    "for q in questions_list:\n",
    "    questions.append(q)\n",
    "pprint.pprint(questions)\n",
    "pprint.pprint(len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different promptings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_zs_eng = \"\"\"You are an AI language model acting as a customer support assistant for Panthera software.  \n",
    "Your task is to generate five different versions of the Italian question provided by the user, in order to retrieve relevant documents from a vector database.  \n",
    "The context involves technical manuals for business management software.  \n",
    "\n",
    "**IMPORTANT:** Only process questions directly related to the context of business management software, technical manuals, or IT-related topics.  \n",
    "If the question is unrelated to these contexts, **do not rewrite it**. In such cases, provide only the tag: \"OUT-OF SCOPE\".\n",
    "If the original question contains multiple questions, rewrite only the ones relevant to the context and ignore entirely the unrelated ones, without giving the tag \"OUT-OF SCOPE\", which\n",
    "is used only when no part of the original question is relevant to the software context.\n",
    "\n",
    "**Instructions:**  \n",
    "1. Check if the question is relevant to management software manuals or IT-related topics. If not, output only: \"OUT-OF SCOPE\".  \n",
    "2. If the question is relevant, generate rewritten questions in Italian that maintain the original meaning, exploring different formulations and angles.  \n",
    "3. Provide the rewritten questions in a bullet-point list separated by new lines.  \n",
    "4. The output must contain only the rewritten questions, without explanations or comments.  \n",
    "\n",
    "Perform the task only for questions relevant to the software context.  \n",
    "Try to improve the original wording by exploring different angles that help clarify the problem or request, making the questions clearer and more readable for a general user.  \n",
    "\n",
    "Original question: {question}  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_zs_ita = \"\"\"Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "Se nessuna delle domande è rilevante per il contesto del software fornisci come output solamente il tag: \"OUT-OF SCOPE\".\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico.\n",
    "\n",
    "Domanda originale: {question} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_zs_eng = ChatPromptTemplate.from_template(template_zs_eng)\n",
    "prompt_zs_ita = ChatPromptTemplate.from_template(template_zs_ita)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_llama = ChatOllama(temperature= 0, model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3.2 Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model_llama_instruct = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model_gpt_old = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model_gpt = ChatOpenAI(temperature=0, model=\"gpt-4o\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question re-writing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from E2E_Evaluation_metrics import RAGEvaluator\n",
    "from E2E_Evaluation_metrics import SemScoreQueryRewriting\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "semscore = SemScoreQueryRewriting()\n",
    "columns_to_drop = ['BLEU', 'ROUGE-2', 'ROUGE-L', 'BERT P', 'BERT R', 'Perplexity', 'Diversity']\n",
    "metrics_columns = ['BERT F1', 'ROUGE-1', 'SemScore']\n",
    "summary_csv_path = \"model_performance_summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rewritten_questions(questions, model, language_prompting, prompting, data_frame, generator):\n",
    "    \"\"\"\n",
    "    Generate rewritten questions for a list of questions and add them to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - questions (list): List of original questions to rewrite.\n",
    "    - model (str): Name of the model used for rewriting.\n",
    "    - language_prompting (str): Language of prompting (e.g., \"Ita\").\n",
    "    - prompting (str): Type of prompting (e.g., \"Zero-shot\").\n",
    "    - data_frame (pd.DataFrame): The DataFrame to which new rows will be added.\n",
    "    - generator (object): The generator object to invoke rewritten questions.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with rewritten questions.\n",
    "    \"\"\"\n",
    "    for question in questions:\n",
    "        rewritten_questions = generator.invoke({\"question\": question})\n",
    "\n",
    "        # If the question is out of scope, skip further processing\n",
    "        if \"OUT-OF SCOPE\" in rewritten_questions:\n",
    "            continue  # Skip out-of-scope questions entirely\n",
    "\n",
    "        for alternative in rewritten_questions:\n",
    "            # Create a new row with the necessary details\n",
    "            new_row = pd.DataFrame([{\n",
    "                \"Original_Question\": question,\n",
    "                \"Model\": model,\n",
    "                \"Language-prompting\": language_prompting, \n",
    "                \"Prompting\": prompting,\n",
    "                \"Rewritten_Question\": alternative\n",
    "            }])\n",
    "\n",
    "            # Add the new row to the DataFrame\n",
    "            data_frame = pd.concat([data_frame, new_row], ignore_index=True)\n",
    "            data_frame = data_frame.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_responses(eval_df, evaluator):\n",
    "    results = []\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        response = row['Rewritten_Question']\n",
    "        reference = row['Original_Question']\n",
    "        \n",
    "        # Check if either response or reference is empty, and skip this row\n",
    "        if not response or not reference:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate and store the results\n",
    "        evaluation = evaluator.evaluate_all(response, reference)\n",
    "        results.append(evaluation)\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    eval_df = pd.DataFrame(results)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_evaluation_and_metrics(data_frame, evaluator, semscore, model_name, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Evaluate responses, compute semantic scores, and merge results into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data_frame (pd.DataFrame): The input DataFrame with original and rewritten questions.\n",
    "    - evaluator (object): The evaluation object to compute BLEU, ROUGE, etc.\n",
    "    - semscore (object): The semantic score computation object.\n",
    "    - model_name (str): Name of the model for semantic similarity scoring.\n",
    "    - columns_to_drop (list): List of columns to drop from the evaluated DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with evaluation metrics and semantic scores.\n",
    "    \"\"\"\n",
    "    # Step 1: Evaluate responses\n",
    "    eval_df = evaluate_responses(data_frame, evaluator)\n",
    "    \n",
    "    # Step 2: Drop unnecessary columns\n",
    "    eval_df = eval_df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    # Step 3: Compute semantic scores\n",
    "    cosine_similarities_bge, _ = semscore.compute_sem_score(data_frame, model_name=model_name)\n",
    "    eval_df[\"SemScore\"] = cosine_similarities_bge[\"Cosine_Similarity\"]\n",
    "\n",
    "    # Step 4: Merge original DataFrame with evaluation metrics\n",
    "    merged_df = pd.concat([data_frame, eval_df], axis=1)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aggregate_model_metrics(df, metrics_columns, group_column='Original_Question'):\n",
    "    \"\"\"\n",
    "    This function aggregates model metrics by a specified group column, calculates the overall score,\n",
    "    and computes the overall model performance across all groups (questions).\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing model metrics.\n",
    "    - metrics_columns (list): A list of column names corresponding to the metrics to be aggregated.\n",
    "    - group_column (str): The column name to group by (default is 'Original_Question').\n",
    "\n",
    "    Returns:\n",
    "    - aggregated_metrics (pd.DataFrame): A DataFrame with aggregated metrics for each group.\n",
    "    - overall_model_performance (pd.Series): A Series with the overall model performance for each metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Group by the specified column and calculate the mean for each metric\n",
    "    aggregated_metrics = df.groupby(group_column).agg({metric: 'mean' for metric in metrics_columns}).reset_index()\n",
    "\n",
    "    # Step 2: Calculate an overall score for each group (optional)\n",
    "    # You can customize how these metrics are combined; here, we'll take their average.\n",
    "    aggregated_metrics['Overall_Score'] = aggregated_metrics[metrics_columns].mean(axis=1)\n",
    "\n",
    "    # Step 3: Compute the overall model performance across all groups\n",
    "    overall_model_performance = aggregated_metrics[metrics_columns].mean()\n",
    "\n",
    "    return aggregated_metrics, overall_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def full_pipeline(\n",
    "    questions, model, language_prompting, prompting, data_frame, generator,\n",
    "    evaluator, semscore, model_name, columns_to_drop, metrics_columns,\n",
    "    summary_csv_path\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline for generating rewritten questions, evaluating metrics, and aggregating results.\n",
    "\n",
    "    Parameters:\n",
    "    - questions (list): List of original questions.\n",
    "    - model (str): Model name to annotate in the data.\n",
    "    - language_prompting (str): Language for prompting annotation.\n",
    "    - prompting (str): Prompting strategy annotation.\n",
    "    - data_frame (pd.DataFrame): Initial DataFrame to append results.\n",
    "    - generator (callable): Function or object to generate rewritten questions.\n",
    "    - evaluator (object): Evaluation object for computing BLEU, ROUGE, etc.\n",
    "    - semscore (object): Semantic score computation object.\n",
    "    - model_name (str): Model name for semantic similarity scoring.\n",
    "    - columns_to_drop (list): Columns to drop after evaluation.\n",
    "    - metrics_columns (list): List of columns to use for aggregated metrics.\n",
    "    - summary_csv_path (str): Path to the CSV file to store overall performance metrics.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (final DataFrame with all metrics, aggregated metrics DataFrame, overall model performance)\n",
    "    \"\"\"\n",
    "    # Step 1: Generate rewritten questions\n",
    "    updated_data_frame = generate_rewritten_questions(\n",
    "        questions=questions,\n",
    "        model=model,\n",
    "        language_prompting=language_prompting,\n",
    "        prompting=prompting,\n",
    "        data_frame=data_frame,\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    # Step 2: Evaluate metrics and add semantic similarity scores\n",
    "    evaluated_data_frame = process_evaluation_and_metrics(\n",
    "        data_frame=updated_data_frame,\n",
    "        evaluator=evaluator,\n",
    "        semscore=semscore,\n",
    "        model_name=model_name,\n",
    "        columns_to_drop=columns_to_drop\n",
    "    )\n",
    "\n",
    "    # Step 3: Aggregate metrics and compute overall performance\n",
    "    aggregated_metrics, overall_model_performance = aggregate_model_metrics(\n",
    "        evaluated_data_frame, metrics_columns\n",
    "    )\n",
    "\n",
    "    # Add model details to the overall performance\n",
    "    overall_performance_row = overall_model_performance.to_frame().T\n",
    "    overall_performance_row[\"Model\"] = model\n",
    "    overall_performance_row[\"Language_Prompting\"] = language_prompting\n",
    "    overall_performance_row[\"Prompting\"] = prompting\n",
    "\n",
    "    # Step 4: Save or update summary CSV\n",
    "    if os.path.exists(summary_csv_path):\n",
    "        # Load existing summary and append new results\n",
    "        existing_summary = pd.read_csv(summary_csv_path)\n",
    "        updated_summary = pd.concat([existing_summary, overall_performance_row], ignore_index=True)\n",
    "        updated_summary = updated_summary.drop_duplicates(ignore_index=True)\n",
    "    else:\n",
    "        # Create new summary file\n",
    "        updated_summary = overall_performance_row\n",
    "\n",
    "    # Save updated summary to CSV\n",
    "    updated_summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    return evaluated_data_frame, aggregated_metrics, overall_model_performance, updated_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a DataFrame\n",
    "columns = [\n",
    "    \"Original_Question\",    # The original question\n",
    "    \"Model\",                # Model name\n",
    "    \"Language-prompting\",   # Language\n",
    "    \"Prompting\",            # Type of prompting\n",
    "    \"Rewritten_Question\"   # Rewritten version of the question\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame\n",
    "data_llama_eng = pd.DataFrame(columns=columns)\n",
    "data_llama_ita = pd.DataFrame(columns=columns)\n",
    "\n",
    "data_llama_instruct_eng = pd.DataFrame(columns=columns)\n",
    "data_llama_instruct_ita = pd.DataFrame(columns=columns)\n",
    "\n",
    "data_gpt_old_eng = pd.DataFrame(columns=columns)\n",
    "data_gpt_old_ita = pd.DataFrame(columns=columns)\n",
    "\n",
    "data_gpt_eng = pd.DataFrame(columns=columns)\n",
    "data_gpt_ita = pd.DataFrame(columns=columns)\n",
    "\n",
    "data_gemini_eng = pd.DataFrame(columns=columns)\n",
    "data_gemini_ita = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries_llama_eng = (prompt_zs_eng | model_llama | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "generate_queries_llama_ita = (prompt_zs_ita | model_llama | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "generate_queries_llama_instruct_eng = (prompt_zs_eng | model_llama_instruct | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "generate_queries_llama_instruct_ita = (prompt_zs_ita | model_llama_instruct | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "generate_queries_gpt_old_eng = (prompt_zs_eng | model_gpt_old | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "generate_queries_gpt_old_ita = (prompt_zs_ita | model_gpt_old | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "generate_queries_gpt_eng = (prompt_zs_eng | model_gpt | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "generate_queries_gpt_ita = (prompt_zs_ita | model_gpt | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "generate_queries_gemini_eng = (prompt_zs_eng | model_gemini | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "generate_queries_gemini_ita = (prompt_zs_ita | model_gemini | StrOutputParser() | (lambda x: x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all models question rewriting capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate open source models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_llama_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"llama3.2\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_llama_ita,\n",
    "    generator=generate_queries_llama_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_llama_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_llama_instruct_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"llama3.2-instruct\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_llama_instruct_ita,\n",
    "    generator=generate_queries_llama_instruct_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_llama_instruct_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_llama_eng, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"llama3.2\",\n",
    "    language_prompting=\"Eng\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_llama_eng,\n",
    "    generator=generate_queries_llama_eng,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_llama_eng.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_llama_instruct_eng, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"llama3.2-instruct\",\n",
    "    language_prompting=\"Eng\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_llama_instruct_eng,\n",
    "    generator=generate_queries_llama_instruct_eng,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_llama_instruct_eng.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate proprietary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gpt_old_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_gpt_old_ita,\n",
    "    generator=generate_queries_gpt_old_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gpt_old_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gpt_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-4o\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_gpt_ita,\n",
    "    generator=generate_queries_gpt_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gpt_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gpt_old_eng, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    language_prompting=\"Eng\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_gpt_old_eng,\n",
    "    generator=generate_queries_gpt_old_eng,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gpt_old_eng.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gpt_eng, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-4o\",\n",
    "    language_prompting=\"Eng\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_gpt_eng,\n",
    "    generator=generate_queries_gpt_eng,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gpt_eng.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gemini_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_gemini_ita,\n",
    "    generator=generate_queries_gemini_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gemini_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gemini_eng, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    language_prompting=\"Eng\",\n",
    "    prompting=\"Zero-shot\",\n",
    "    data_frame=data_gemini_eng,\n",
    "    generator=generate_queries_gemini_eng,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=summary_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gemini_eng.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "plot_path = os.getcwd() + \"/Plots\"\n",
    "plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the plot with title-based filename and specified save path\n",
    "def save_plot(fig, title, save_path=plot_path):\n",
    "    \"\"\"\n",
    "    Save the plot to the specified directory with a filename based on the plot title.\n",
    "\n",
    "    Args:\n",
    "    fig (matplotlib.figure.Figure): The figure object of the plot.\n",
    "    title (str): The title of the plot, used to generate the filename.\n",
    "    save_path (str): Directory path where the plot should be saved (optional).\n",
    "    \"\"\"\n",
    "    if save_path:\n",
    "        # Sanitize the title to create a valid filename\n",
    "        sanitized_title = re.sub(r\"[^\\w\\s-]\", \"\", title).replace(\" \", \"_\")\n",
    "        filename = f\"{sanitized_title}.jpg\"\n",
    "        \n",
    "        # Ensure the save_path exists, create if necessary\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # Combine save_path with the filename\n",
    "        full_save_path = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Save the plot\n",
    "        fig.savefig(full_save_path, format=\"jpg\", dpi=300)\n",
    "        print(f\"Plot saved to {full_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = updated_summary.sort_values(by=\"Model\")\n",
    "\n",
    "# Combine Model and Language for x-tick labels\n",
    "df[\"Model_Language\"] = df[\"Model\"] + \" (\" + df[\"Language_Prompting\"] + \")\"\n",
    "\n",
    "# Set up the plot\n",
    "x = np.arange(len(df[\"Model_Language\"]))  # X-axis positions\n",
    "width = 0.25  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot each metric\n",
    "ax.bar(x - width, df[\"BERT F1\"], width, label=\"BERT F1\", color='blue')\n",
    "ax.bar(x, df[\"ROUGE-1\"], width, label=\"ROUGE-1\", color='orange')\n",
    "ax.bar(x + width, df[\"SemScore\"], width, label=\"SemScore\", color='green')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Model (Language)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Comparison of Metrics Across Models and Languages\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df[\"Model_Language\"], rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plot(fig, \"Comparison of Metrics Across Models and Languages\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a barplot for a specific metric\n",
    "def plot_metric(dataframe, metric, title, ylabel, color, save_path = plot_path):\n",
    "    \"\"\"\n",
    "    Create a barplot for the specified metric.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The input DataFrame.\n",
    "    metric (str): The column name for the metric to plot.\n",
    "    title (str): The title of the plot.\n",
    "    ylabel (str): The label for the y-axis.\n",
    "    color (str): The color of the bars.\n",
    "    \"\"\"\n",
    "    x = np.arange(len(dataframe[\"Model_Language\"]))  # X-axis positions\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bars=ax.bar(x, dataframe[metric], color=color, label=metric)\n",
    "\n",
    "    # Add text annotations on bars\n",
    "    for bar, value in zip(bars, dataframe[metric]):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "                f\"{value:.3f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "        \n",
    "    ax.set_xlabel(\"Model (Language)\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dataframe[\"Model_Language\"], rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # If a save_path is provided, combine it with the dynamically generated filename\n",
    "    if save_path:\n",
    "        # Sanitize the title to create a valid filename\n",
    "        sanitized_title = re.sub(r\"[^\\w\\s-]\", \"\", title).replace(\" \", \"_\")\n",
    "        filename = f\"{sanitized_title}.jpg\"\n",
    "        \n",
    "        # Ensure the save_path exists, create if necessary\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # Combine save_path with the filename\n",
    "        full_save_path = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(full_save_path, format=\"jpg\", dpi=300)\n",
    "        print(f\"Plot saved to {full_save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create separate plots for each metric\n",
    "plot_metric(df, \"BERT F1\", \"General Comparison of BERT F1 Across Models\", \"BERT F1 Score\", \"blue\")\n",
    "plot_metric(df, \"ROUGE-1\", \"General Comparison of ROUGE-1 Across Models\", \"ROUGE-1 Score\", \"orange\")\n",
    "plot_metric(df, \"SemScore\", \"General Comparison of SemScore Across Models\", \"SemScore\", \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a barplot for a specific metric and language\n",
    "def plot_metric_by_language(dataframe, metric, title, ylabel, color, language, save_path = plot_path):\n",
    "    \"\"\"\n",
    "    Create a barplot for the specified metric filtered by language.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The input DataFrame.\n",
    "    metric (str): The column name for the metric to plot.\n",
    "    title (str): The title of the plot.\n",
    "    ylabel (str): The label for the y-axis.\n",
    "    color (str): The color of the bars.\n",
    "    language (str): The language to filter by (e.g., \"Ita\" or \"Eng\").\n",
    "    \"\"\"\n",
    "    # Filter by language\n",
    "    df_language = dataframe[dataframe[\"Language_Prompting\"] == language]\n",
    "    x = np.arange(len(df_language[\"Model_Language\"]))  # X-axis positions\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bars = ax.bar(x, df_language[metric], color=color, label=f\"{metric} ({language})\")\n",
    "\n",
    "    # Add text annotations on bars\n",
    "    for bar, value in zip(bars, df_language[metric]):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "                f\"{value:.3f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "        \n",
    "    ax.set_xlabel(\"Model (Language)\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f\"{title} - {language.upper()} Models\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_language[\"Model_Language\"], rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # If a save_path is provided, combine it with the dynamically generated filename\n",
    "    if save_path:\n",
    "        # Sanitize the title to create a valid filename\n",
    "        sanitized_title = re.sub(r\"[^\\w\\s-]\", \"\", title).replace(\" \", \"_\")\n",
    "        filename = f\"{sanitized_title}_{language}.jpg\"\n",
    "        \n",
    "        # Ensure the save_path exists, create if necessary\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # Combine save_path with the filename\n",
    "        full_save_path = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(full_save_path, format=\"jpg\", dpi=300)\n",
    "        print(f\"Plot saved to {full_save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create separate plots for each metric and language\n",
    "for language, color in [(\"Ita\", \"blue\"), (\"Eng\", \"orange\")]:\n",
    "    plot_metric_by_language(df, \"BERT F1\", \"Comparison of BERT F1 Across Models\", \"BERT F1 Score\", color, language)\n",
    "    plot_metric_by_language(df, \"ROUGE-1\", \"Comparison of ROUGE-1 Across Models\", \"ROUGE-1 Score\", color, language)\n",
    "    plot_metric_by_language(df, \"SemScore\", \"Comparison of SemScore Across Models\", \"SemScore\", color, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare zero-shot, one-shot and few-shot prompting with gpt-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze some possible questions, and manually create the rewritten versions, in order to use them as examples in the one-shot and few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset = Dataset.load_from_disk(\"eval_dataset\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df_syntethic = eval_dataset.to_pandas()\n",
    "eval_df_syntethic = eval_df_syntethic[[\"question\", \"answer\"]]\n",
    "print(len(eval_df_syntethic))\n",
    "display(eval_df_syntethic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "generated_questions = eval_df_syntethic['question']\n",
    "sample = random.randint(0, len(generated_questions))\n",
    "pprint.pprint(generated_questions[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_os_ita = \"\"\"\n",
    "Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "Se nessuna delle domande è rilevante per il contesto del software gestionale fornisci come output solamente il tag: \"OUT-OF SCOPE\".\n",
    "\n",
    "**Esempi:**\n",
    "1. Domanda originale: \"Come posso specificare il tipo costo da assegnare al nuovo ambiente di commessa?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per definire il tipo di costo da applicare a un nuovo ambiente di commessa?\n",
    "   - Come posso configurare il tipo di costo per un nuovo ambiente di commessa nel sistema?\n",
    "   - Quali sono i passaggi per assegnare un tipo di costo a un ambiente di commessa appena creato?\n",
    "   - Dove posso trovare l'opzione per impostare il tipo di costo di un nuovo ambiente di commessa?\n",
    "   - Come posso scegliere il tipo di costo da associare a un ambiente di commessa nel mio sistema?\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico. \n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_os_ita = ChatPromptTemplate.from_template(template_os_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gpt_os_ita = pd.DataFrame(columns = columns)\n",
    "generate_queries_gpt_os_ita = (prompt_os_ita | model_gpt | StrOutputParser() | (lambda x: x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "prompting_type_csv_path = \"prompting_type_csv_path_summary.csv\"\n",
    "df_gpt_os_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-4o\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"One-shot\",\n",
    "    data_frame=data_gpt_os_ita,\n",
    "    generator=generate_queries_gpt_os_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=prompting_type_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gpt_os_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we won't generate only examples of acceptable questions and the rewritings, but we have also to include examples with not acceptable questions to which the model doesn't answer, but just retrurns the [OUT-OF SCOPE] tag; and we also consider multiple questions rewriting, with more valid questions, but also multiple questions with some valid ones and some not valid ones, even if we don't use these cases in the evaluation test-set, to not mislead the semantic scores due to some invalid question's parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "generated_questions = eval_df_syntethic['question']\n",
    "sample = random.randint(0, len(generated_questions))\n",
    "pprint.pprint(generated_questions[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_fs_ita = \"\"\"\n",
    "Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "Se nessuna delle domande è rilevante per il contesto del software fornisci come output solamente il tag: \"OUT-OF SCOPE\".\n",
    "\n",
    "**Esempi:**\n",
    "1. Domanda originale: \"Come posso specificare il tipo costo da assegnare al nuovo ambiente di commessa?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per definire il tipo di costo da applicare a un nuovo ambiente di commessa?\n",
    "   - Come posso configurare il tipo di costo per un nuovo ambiente di commessa nel sistema?\n",
    "   - Quali sono i passaggi per assegnare un tipo di costo a un ambiente di commessa appena creato?\n",
    "   - Dove posso trovare l'opzione per impostare il tipo di costo di un nuovo ambiente di commessa?\n",
    "   - Come posso scegliere il tipo di costo da associare a un ambiente di commessa nel mio sistema?\n",
    "\n",
    "2. Domanda originale: \"Come si fa la carbonara?\"\n",
    "   Risposta: OUT-OF SCOPE\n",
    "\n",
    "3. Domanda originale: \"Come posso definire lo stato di un articolo in questo software?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per impostare lo stato di un articolo in questo software?\n",
    "   - Come posso configurare lo stato di un articolo all'interno di questo software?\n",
    "   - Dove posso definire o modificare lo stato di un articolo nel software?\n",
    "   - Quali sono le opzioni per determinare lo stato di un articolo in questo software?\n",
    "   - Come posso aggiornare lo stato di un articolo nel sistema?\n",
    "\n",
    "4. Domanda originale: \"Ho un errore, come posso risolvere il problema delle schermate nere all'avvio del software? E come posso fare la carbonara?\"\n",
    "   Risposte: \n",
    "   - Come posso risolvere il problema delle schermate nere che si verificano all'avvio del software?\n",
    "   - Cosa devo fare per correggere l'errore delle schermate nere durante l'avvio del programma?\n",
    "   - Quali sono le possibili cause delle schermate nere all'avvio del software e come posso risolvere il problema?\n",
    "   - Come posso risolvere il malfunzionamento delle schermate nere che si presentano quando avvio il software?\n",
    "   - Perché vedo delle schermate nere all'avvio del software e quali sono le soluzioni disponibili?\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico. \n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_fs_ita = ChatPromptTemplate.from_template(template_fs_ita)\n",
    "data_gpt_fs_ita = pd.DataFrame(columns = columns)\n",
    "generate_queries_gpt_fs_ita = (prompt_fs_ita | model_gpt | StrOutputParser() | (lambda x: x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "df_gpt_fs_ita, aggregated_metrics, overall_model_performance, updated_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-4o\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Few-shot\",\n",
    "    data_frame=data_gpt_fs_ita,\n",
    "    generator=generate_queries_gpt_fs_ita,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=prompting_type_csv_path\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Single Metrics for Each Question and Rewritten Question:\")\n",
    "display(df_gpt_fs_ita.head())\n",
    "\n",
    "print(\"Aggregated Metrics for Each Question:\")\n",
    "display(aggregated_metrics)\n",
    "\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "display(overall_model_performance)\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(updated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_gpt_ita), len(df_gpt_os_ita), len(df_gpt_fs_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows to drop based on the condition\n",
    "rows_to_drop = df_gpt_ita[~df_gpt_ita['Original_Question'].isin(df_gpt_fs_ita['Original_Question'].unique())].index\n",
    "\n",
    "# Drop the rows from df_gpt_ita\n",
    "df_gpt_ita_new = df_gpt_ita.drop(rows_to_drop).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 3: Aggregate metrics and compute overall performance\n",
    "aggregated_metrics, overall_model_performance = aggregate_model_metrics(\n",
    "    df_gpt_ita_new, metrics_columns\n",
    ")\n",
    "\n",
    " # Add model details to the overall performance\n",
    "overall_performance_row = overall_model_performance.to_frame().T\n",
    "overall_performance_row[\"Model\"] = \"gpt-4o\"\n",
    "overall_performance_row[\"Language_Prompting\"] = \"Ita\"\n",
    "overall_performance_row[\"Prompting\"] = \"Zero-shot\"\n",
    "\n",
    "updated_summary = pd.concat([updated_summary, overall_performance_row], ignore_index=True)\n",
    "updated_summary = updated_summary.drop_duplicates(ignore_index=True)\n",
    "updated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df is already defined and contains the data\n",
    "df = updated_summary\n",
    "\n",
    "# Filter data for gpt-4o models and Language_Prompting 'Ita'\n",
    "filtered_df = df[(df[\"Model\"] == \"gpt-4o\") & (df[\"Language_Prompting\"] == \"Ita\")]\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "melted_df = filtered_df.melt(id_vars=[\"Prompting\"], value_vars=[\"BERT F1\", \"ROUGE-1\", \"SemScore\"], \n",
    "                             var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Define the desired order of prompting types\n",
    "prompting_order = [\"Zero-shot\", \"One-shot\", \"Few-shot\"]\n",
    "\n",
    "# Plot using seaborn with the specified order\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=melted_df, x=\"Prompting\", y=\"Score\", hue=\"Metric\", palette=\"viridis\", order=prompting_order)\n",
    "\n",
    "# Add text annotations for each bar\n",
    "for container in ax.containers:\n",
    "    for bar in container:\n",
    "        bar_height = bar.get_height()\n",
    "        if bar_height > 0:  # Only annotate if the bar's height is greater than 0\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,  # X-coordinate (bar center)\n",
    "                bar_height,  # Y-coordinate (just above the bar)\n",
    "                f'{bar_height:.3f}',  # Text to display (score with 3 decimals)\n",
    "                ha='center', va='bottom', fontsize=9, color='black'\n",
    "            )\n",
    "\n",
    "# Customize plot\n",
    "plot_filename = \"Scores_for_gpt-4o_Models_with_Ita_Language_Prompting.jpg\"  # Name of the file\n",
    "plt.title(\"Scores for gpt-4o Models with Ita Language_Prompting\", fontsize=14)\n",
    "plt.xlabel(\"Prompting Type\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.legend(title=\"Metrics\", fontsize=10)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save the plot to a file\n",
    "output_path = f\"{plot_path}/{plot_filename}\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved successfully to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's analyze also the number of useful few-shot elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_i = \"\"\"\n",
    "Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "Se nessuna delle domande è rilevante per il contesto del software fornisci come output solamente il tag: \"OUT-OF SCOPE\".\n",
    "\n",
    "**Esempi:**\n",
    "1. Domanda originale: \"Come posso specificare il tipo costo da assegnare al nuovo ambiente di commessa?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per definire il tipo di costo da applicare a un nuovo ambiente di commessa?\n",
    "   - Come posso configurare il tipo di costo per un nuovo ambiente di commessa nel sistema?\n",
    "   - Quali sono i passaggi per assegnare un tipo di costo a un ambiente di commessa appena creato?\n",
    "   - Dove posso trovare l'opzione per impostare il tipo di costo di un nuovo ambiente di commessa?\n",
    "   - Come posso scegliere il tipo di costo da associare a un ambiente di commessa nel mio sistema?\n",
    "\n",
    "2. Domanda originale: \"Come si fa la carbonara?\"\n",
    "   Risposta: OUT-OF SCOPE\n",
    "\n",
    "3. Domanda originale: \"Come posso definire lo stato di un articolo in questo software?\"\n",
    "   Risposte:\n",
    "   - Qual è il procedimento per impostare lo stato di un articolo in questo software?\n",
    "   - Come posso configurare lo stato di un articolo all'interno di questo software?\n",
    "   - Dove posso definire o modificare lo stato di un articolo nel software?\n",
    "   - Quali sono le opzioni per determinare lo stato di un articolo in questo software?\n",
    "   - Come posso aggiornare lo stato di un articolo nel sistema?\n",
    "\n",
    "4. Domanda originale: \"Ho un errore, come posso risolvere il problema delle schermate nere all'avvio del software? E come posso fare la carbonara?\"\n",
    "   Risposte: \n",
    "   - Come posso risolvere il problema delle schermate nere che si verificano all'avvio del software?\n",
    "   - Cosa devo fare per correggere l'errore delle schermate nere durante l'avvio del programma?\n",
    "   - Quali sono le possibili cause delle schermate nere all'avvio del software e come posso risolvere il problema?\n",
    "   - Come posso risolvere il malfunzionamento delle schermate nere che si presentano quando avvio il software?\n",
    "   - Perché vedo delle schermate nere all'avvio del software e quali sono le soluzioni disponibili?\n",
    "\n",
    "5. Domanda originale: \"Quali informazioni posso visualizzare riguardo ai beni ammortizzabili?\"\n",
    "   Risposte:\n",
    "   - Quali dettagli sono disponibili nel sistema sui beni ammortizzabili?\n",
    "   - Che tipo di informazioni posso consultare sui beni ammortizzabili nel software?\n",
    "   - Quali dati relativi ai beni ammortizzabili possono essere visualizzati o gestiti?\n",
    "   - Dove posso trovare le informazioni disponibili sui beni ammortizzabili?\n",
    "   - Quali informazioni sui beni ammortizzabili sono accessibili attraverso il sistema?\n",
    "\n",
    "6. Domanda originale: \"Come posso gestire le quantità in unità di misura intere nel magazzino? Dove si trova fisicamente il magazzino?\"\n",
    "   Risposte:\n",
    "   - Come posso gestire le quantità in unità di misura intere nel sistema di magazzino?\n",
    "   - Qual è il metodo per gestire le quantità in unità intere nel magazzino?\n",
    "   - Come posso configurare il magazzino per gestire le quantità in unità di misura intere?\n",
    "   - Quali sono le opzioni per gestire le quantità in unità intere nel magazzino?\n",
    "   - Come posso impostare la gestione delle quantità in unità intere nel magazzino per il mio sistema?\n",
    "\n",
    "7. Domanda originale: \"Buonasera, qual è il fattore di scarto considerato dal sistema durante la fase di estazione pso? Si può cambiare in qualche modo?\"\n",
    "   Risposte:\n",
    "   - Qual è il fattore di scarto utilizzato dal sistema durante la fase di estazione PSO e come posso modificarlo?\n",
    "   - Come viene calcolato il fattore di scarto nella fase di estazione PSO e c'è la possibilità di cambiarlo?\n",
    "   - In che modo il sistema considera il fattore di scarto durante la fase di estazione PSO e può essere personalizzato?\n",
    "   - Qual è il valore predefinito del fattore di scarto nella fase di estazione PSO e come posso cambiarlo?\n",
    "   - È possibile modificare il fattore di scarto usato dal sistema nella fase di estazione PSO? Se sì, come?\n",
    "\n",
    "8. Domanda originale: \"Voglio copiare le configurazioni di un articolo in un altro con lo stesso schema di configurazione?\"\n",
    "   Risposte:\n",
    "   - Come posso copiare le configurazioni di un articolo e applicarle a un altro con lo stesso schema di configurazione?\n",
    "   - Esiste un metodo per trasferire le configurazioni di un articolo su un altro mantenendo lo stesso schema?\n",
    "   - Qual è la procedura per duplicare le configurazioni di un articolo e usarle per un altro con lo stesso schema?\n",
    "   - Come posso replicare le impostazioni di un articolo in un altro, mantenendo invariato lo schema di configurazione?\n",
    "   - È possibile copiare e incollare le configurazioni di un articolo in un altro con lo stesso schema di configurazione?\n",
    "\n",
    "9. Domanda originale: \"Mi puoi aiutare ad integrare una fattura ricevuta con la relativa aliquota e imposta?\"\n",
    "   Risposte:\n",
    "   - Come posso integrare una fattura ricevuta nel sistema includendo l'aliquota e l'imposta applicata?\n",
    "   - Qual è la procedura per registrare una fattura ricevuta e calcolare correttamente l'aliquota e l'imposta?\n",
    "   - Come posso associare l'aliquota e l'imposta a una fattura ricevuta nel mio sistema?\n",
    "   - C'è un modo per aggiungere l'aliquota e l'imposta a una fattura ricevuta in modo automatico nel software?\n",
    "   - In che modo posso integrare una fattura con l'aliquota e l'imposta per un calcolo corretto?\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico. \n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_i = ChatPromptTemplate.from_template(template_i)\n",
    "data_gpt_prompt_i = pd.DataFrame(columns = columns)\n",
    "generate_queries_prompt_i = (prompt_i | model_gpt | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Execute the full pipeline\n",
    "prompting_i_examples = \"prompting_i_examples.csv\"\n",
    "df_prompt_i, aggregated_metrics, overall_model_performance, examples_i_summary = full_pipeline(\n",
    "    questions=questions,\n",
    "    model=\"gpt-4o\",\n",
    "    language_prompting=\"Ita\",\n",
    "    prompting=\"Few-shot-9\",\n",
    "    data_frame=data_gpt_prompt_i,\n",
    "    generator=generate_queries_prompt_i,\n",
    "    evaluator=evaluator,\n",
    "    semscore=semscore,\n",
    "    model_name='BAAI/bge-m3',\n",
    "    columns_to_drop=columns_to_drop,\n",
    "    metrics_columns=metrics_columns, \n",
    "    summary_csv_path=prompting_i_examples\n",
    ")\n",
    "\n",
    "print(\"Updated Model Performance Summary:\")\n",
    "display(examples_i_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming examples_i_summary is a predefined data structure\n",
    "df = pd.DataFrame(examples_i_summary)\n",
    "\n",
    "# Create subplots for each metric\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 18))  # 3 rows, 1 column\n",
    "\n",
    "# Plotting BERT F1\n",
    "axs[0].plot(df[\"Prompting\"], df[\"BERT F1\"], label=\"BERT F1\", marker='o', color='blue')\n",
    "axs[0].set_title(\"BERT F1 Performance\")\n",
    "axs[0].set_xlabel(\"Number of Few-shot Examples\")\n",
    "axs[0].set_ylabel(\"BERT F1 Score\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "axs[0].set_ylim([df[\"BERT F1\"].min() - 0.1, df[\"BERT F1\"].max() + 0.1])  # Zoom on BERT F1 values\n",
    "\n",
    "# Annotate all BERT F1 points\n",
    "for i, val in enumerate(df[\"BERT F1\"]):\n",
    "    if i == df[\"BERT F1\"].idxmax():\n",
    "        axs[0].annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, -15), ha='center', color='blue')\n",
    "    else:\n",
    "        axs[0].annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, 10), ha='center', color='black')\n",
    "\n",
    "# Plotting ROUGE-1\n",
    "axs[1].plot(df[\"Prompting\"], df[\"ROUGE-1\"], label=\"ROUGE-1\", marker='o', color='green')\n",
    "axs[1].set_title(\"ROUGE-1 Performance\")\n",
    "axs[1].set_xlabel(\"Number of Few-shot Examples\")\n",
    "axs[1].set_ylabel(\"ROUGE-1 Score\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "axs[1].set_ylim([df[\"ROUGE-1\"].min() - 0.1, df[\"ROUGE-1\"].max() + 0.1])  # Zoom on ROUGE-1 values\n",
    "\n",
    "# Annotate all ROUGE-1 points\n",
    "for i, val in enumerate(df[\"ROUGE-1\"]):\n",
    "    if i == df[\"ROUGE-1\"].idxmax():\n",
    "        axs[1].annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, -15), ha='center', color='green')\n",
    "    else:\n",
    "        axs[1].annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, 10), ha='center', color='black')\n",
    "\n",
    "# Plotting Semantic Score\n",
    "axs[2].plot(df[\"Prompting\"], df[\"SemScore\"], label=\"Semantic Score\", marker='o', color='red')\n",
    "axs[2].set_title(\"Semantic Score Performance\")\n",
    "axs[2].set_xlabel(\"Number of Few-shot Examples\")\n",
    "axs[2].set_ylabel(\"Semantic Score\")\n",
    "axs[2].legend()\n",
    "axs[2].grid(True)\n",
    "axs[2].set_ylim([df[\"SemScore\"].min() - 0.1, df[\"SemScore\"].max() + 0.1])  # Zoom on Semantic Score values\n",
    "\n",
    "# Annotate all Semantic Score points\n",
    "for i, val in enumerate(df[\"SemScore\"]):\n",
    "    if i == df[\"SemScore\"].idxmax():\n",
    "        axs[2].annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, -15), ha='center', color='red')\n",
    "    else:\n",
    "        axs[2].annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, 10), ha='center', color='black')\n",
    "\n",
    "# Save the Semantic Score plot\n",
    "axs[2].figure.savefig(f\"{plot_path}/Metrics_Performance_Few_shot.jpg\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming examples_i_summary is a predefined data structure\n",
    "df = pd.DataFrame(examples_i_summary)\n",
    "\n",
    "# Calculate the average of the three metrics\n",
    "df[\"Average\"] = df[[\"BERT F1\", \"ROUGE-1\", \"SemScore\"]].mean(axis=1)\n",
    "\n",
    "# Find the index of the maximum average value\n",
    "max_avg_idx = df[\"Average\"].idxmax()\n",
    "\n",
    "# Plotting the scores for analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the individual metrics\n",
    "plt.plot(df[\"Prompting\"], df[\"BERT F1\"], label=\"BERT F1\", marker='o', color='blue')\n",
    "plt.plot(df[\"Prompting\"], df[\"ROUGE-1\"], label=\"ROUGE-1\", marker='o', color='green')\n",
    "plt.plot(df[\"Prompting\"], df[\"SemScore\"], label=\"Semantic Score\", marker='o', color='red')\n",
    "\n",
    "# Plot the average values\n",
    "plt.plot(df[\"Prompting\"], df[\"Average\"], label=\"Average\", marker='o', color='orange', linestyle='--')\n",
    "\n",
    "# Annotate the average values\n",
    "for i, val in enumerate(df[\"Average\"]):\n",
    "    if i == max_avg_idx:\n",
    "        # Annotate max average in a distinct color\n",
    "        plt.annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, 10), \n",
    "                     ha='center', color='orange', fontsize=10, weight='bold')\n",
    "    else:\n",
    "        # Annotate other averages in black\n",
    "        plt.annotate(f\"{val:.4f}\", (df[\"Prompting\"][i], val), textcoords=\"offset points\", xytext=(10, -10), \n",
    "                     ha='center', color='black', fontsize=8)\n",
    "\n",
    "# Add a vertical line for the `Prompting` value with max average\n",
    "plt.axvline(x=df[\"Prompting\"][max_avg_idx], color='grey', linestyle='--', label='Max Avg Prompting')\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title(\"Performance of GPT-4o Model with Few-shot Examples\")\n",
    "plt.xlabel(\"Number of Few-shot Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Save the plot with the title as the filename\n",
    "plt.savefig(f\"{plot_path}/Compare_Performance_GPT-4o_Different_Few-Shot_Examples.jpg\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
