{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9421f09e5df019a",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48231c46a0de8a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T06:54:30.982656Z",
     "start_time": "2024-10-02T06:54:09.415996Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from jupyter_core.version import pattern\n",
    "from nltk.corpus import stopwords\n",
    "#from sqlalchemy.testing import not_in\n",
    "#from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import contractions\n",
    "import random\n",
    "import pandas as pd\n",
    "import language_tool_python\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02c78151d7a649",
   "metadata": {},
   "source": [
    "# Load obtained dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T06:54:31.726998Z",
     "start_time": "2024-10-02T06:54:31.049239Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filename_all_data_dict = \"C:/Users/bsavoiumarinas/Documents/Tesi/PyProjects/Experiments/Files/data_imported_by_pdf_coordinates.csv\"\n",
    "filename_all_data_dict = \"./Files/data_imported_by_pdf_coordinates.csv\"\n",
    "\n",
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe03b217746c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:00.143061Z",
     "start_time": "2024-09-30T17:14:00.138693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'file'\n",
    "file_list = data_df['file'].tolist()\n",
    "\n",
    "# Create a list with all the values in the column 'text'\n",
    "text_list = data_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53496253aa0eaed8",
   "metadata": {},
   "source": [
    "# Data preprocessing steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7ab72105cd49b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:00.758965Z",
     "start_time": "2024-09-30T17:14:00.172347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Italian stopwords from nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "# Load SpaCy Italian model\n",
    "nlp = spacy.load('it_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2185c452dba4a0",
   "metadata": {},
   "source": [
    "### Explore files with different format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58923b255939b1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:00.852017Z",
     "start_time": "2024-09-30T17:14:00.848516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's first remove the index from the document\n",
    "def remove_index(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    \n",
    "    # Flag to detect if we are in the \"Indice\" section\n",
    "    in_index = False\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Detect the start of the index section by looking for \"Indice\"\n",
    "        if re.search(r'\\bIndice\\b', line, re.IGNORECASE):\n",
    "            in_index = True\n",
    "            continue  # Skip the \"Indice\" line itself\n",
    "        \n",
    "        # If we are in the index section, check if the line is part of the index\n",
    "        if in_index:\n",
    "            # Check for lines that contain a sentence followed by dots or a number\n",
    "            if re.match(r'^.*\\s*\\.*\\s*\\d+\\s*$', line.strip()):\n",
    "                continue  # Skip this line (it's part of the index)\n",
    "            # Check for lines that only contain a number (page breaks)\n",
    "            elif re.match(r'^\\d+$', line) or re.match(r'^[\\s]*$', line):\n",
    "                continue  # Skip page numbers or empty lines\n",
    "            else:\n",
    "                # If we hit a line that doesn't match the index format, we're past the index section\n",
    "                in_index = False\n",
    "        \n",
    "        # Add the current line to the cleaned_lines if it's not part of the index\n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    # Recombine the cleaned lines into a single string\n",
    "    return \"\\n\".join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67c5938daef078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:00.879263Z",
     "start_time": "2024-09-30T17:14:00.859026Z"
    }
   },
   "outputs": [],
   "source": [
    "def index_dots_removal(text):\n",
    "    # Split the text into individual lines\n",
    "    lines = text.splitlines(\"\\n\")\n",
    "\n",
    "    # Define the regex pattern to match lines with more than 10 dots and ending with a digit\n",
    "    pattern = r\"\\.{10,}\\s*\"\n",
    "\n",
    "    # Filter out lines that match the pattern\n",
    "    filtered_lines = [line for line in lines if not re.search(pattern, line)]\n",
    "\n",
    "    # Join the filtered lines back into a single string\n",
    "    filtered_text = \"\\n\".join(filtered_lines)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fce73b3110b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:00.908583Z",
     "start_time": "2024-09-30T17:14:00.902293Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_remaining_indexes(text):\n",
    "    pattern = r\"(Obiettivi del manuale).*(\\n\\n\\n1\\. )\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, ' ', text, flags=re.DOTALL)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e144bdc4047ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:00.943096Z",
     "start_time": "2024-09-30T17:14:00.938178Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_table_content(text):\n",
    "    \"\"\"\n",
    "    Removes table-like content from the text, defined as sections starting with 'Es.'\n",
    "    and containing at least two '|' symbols, which represent manually written tables.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text from which table content needs to be removed.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The modified text with table-like content removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The pattern looks for blocks that begin with 'Es.' and contain at least two '|' symbols.\n",
    "    pattern = r'Es\\..*?(?:\\|.*?){2,}.*?(?=\\n\\n|\\Z)'\n",
    "    \n",
    "    # Using re.sub to remove matching table-like blocks\n",
    "    modified_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Also clean up any remaining multiple new lines created after table removal\n",
    "    modified_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', modified_text)\n",
    "    \n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99429ba4e0788ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:01.013282Z",
     "start_time": "2024-09-30T17:14:01.002371Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_remaining_table_from_text(text):\n",
    "    # Define the regex pattern\n",
    "    pattern = r\"\\n\\ntabella[\\s\\S]*?\\|.*\\|\\s*\\n\\n\"\n",
    "    \n",
    "    # Use re.sub to substitute the matched table with an empty string\n",
    "    cleaned_text = re.sub(pattern, '\\n', text, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f13c58856eb3fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:01.039299Z",
     "start_time": "2024-09-30T17:14:01.032723Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_table_header(text):\n",
    "    pattern = r\"--+|===+\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72029a2ebabfd5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:01.058045Z",
     "start_time": "2024-09-30T17:14:01.052633Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten_any_list(text):\n",
    "    # Updated regex pattern to match:\n",
    "    # 1. Bullet lists (e.g., *, -, •, \\uf0a7)\n",
    "    # 2. Numbered lists (e.g., 1., 2)\n",
    "    # 3. Uppercase alphabetic markers (e.g., I, U, D)\n",
    "    # 4. Lowercase alphabetic markers (e.g., a., b.)\n",
    "    # 5. Mixed patterns (e.g., I Inserimento, 0 Inserimento/Aggiornamento)\n",
    "    \n",
    "    \n",
    "    #list_pattern = r\"(\\n\\s*[\\*\\-\\•\\–]|\\n\\s*\\d+[\\.\\)]|\\n\\s*[a-zA-Z]{1}|\\n\\s*\\d+|\\uf0a7)\\s+.+(?:\\s+.+)*\"\n",
    "    # 6. Match nested numeration (e.g. 1.1.1., 2.30)\n",
    "    # 7. Match number or alphabetic lists with ')' (e.g. 1) 2) or a) b) )\n",
    "    # 8. Mixed number/letter list (e.g. 1a), 1b) or 1a. 1b. ) \n",
    "    list_pattern = r\"(\\n\\s*[\\*\\-\\•\\–\\▪]|\\n\\s*\\d+[\\.\\)]|\\n\\s*(\\d)*\\w[\\.\\)]|\\n\\s*[a-zA-Z]{1}|\\n\\s*\\d+|\\n\\s*(\\d+[\\.\\)]?)+|\\\\uf0a7)\\s+.+(?:\\s+.+)*\"\n",
    "    \n",
    "\n",
    "    # Function to replace the matched list items\n",
    "    def replace_list_with_commas(match):\n",
    "        # Get the matched list block\n",
    "        list_block = match.group(0)\n",
    "        \n",
    "        # Remove the list markers and flatten the list\n",
    "        # flattened_list = re.sub(r\"(\\n\\s*[\\*\\-\\•\\–]|\\n\\s*\\d+[\\.\\)]|\\n\\s*[a-zA-Z]{1}|\\n\\s*\\d+|\\uf0a7)\\s+\", \", \", list_block)\n",
    "        flattened_list = re.sub(r\"(\\n\\s*[\\*\\-\\•\\–]|\\n\\s*\\d+[\\.\\)]|\\n\\s*(\\d)*\\w[\\.\\)]|\\n\\s*[a-zA-Z]{1}|\\n\\s*\\d+|\\n\\s*(\\d+[\\.\\)]?)+|\\\\uf0a7)\\s+\", \", \", list_block)\n",
    "        \n",
    "        flattened_list = re.sub(r\"\\n\\s*\", \" \", flattened_list)  # Removes extra newlines within list items\n",
    "        \n",
    "        # Clean up spaces\n",
    "        flattened_list = flattened_list.replace(\"  \", \" \").strip()\n",
    "        \n",
    "        \n",
    "        return flattened_list\n",
    "\n",
    "    # Apply the transformation only to list items, leaving other text untouched\n",
    "    normalized_text = re.sub(list_pattern, replace_list_with_commas, text)\n",
    "    normalized_text = re.sub(r\": ,\", \": \", normalized_text)  # Fix for colon-space issues\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d5d04c70bfff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:14:09.039300Z",
     "start_time": "2024-10-02T07:14:09.027998Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Reduces multiple consecutive whitespace characters to a single space.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text with excessive whitespace.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text with reduced whitespace.\n",
    "    \"\"\"\n",
    "    # Replace one or more whitespace characters with a single space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7068c0def60bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:01.137783Z",
     "start_time": "2024-09-30T17:14:01.133590Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_issue_date(text):\n",
    "    # Define the regex pattern to match\n",
    "    pattern = r'Data emissione (\\d+\\/){2}(\\d+)'\n",
    "    \n",
    "    # Remove all matches of the pattern\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def remove_issue_slash(text):\n",
    "    # Define the regex pattern to match\n",
    "    pattern = r'\\/'\n",
    "    \n",
    "    # Remove all matches of the pattern\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbe3bc6bb9bbce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:01.154322Z",
     "start_time": "2024-09-30T17:14:01.141295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create some functions to also remove unwanted tables and equations with special characters\n",
    "# equations = r\"(\\d).*[=+*\\/∑√]{2,}( \\d*|\\d*)|\\d+\\/\\d+|\\^\"\n",
    "# special_elements = r\"\\uf0e6|\\uf0f6|\\uf0e7|\\uf0f7|\\uf0e8|\\uf0f8|\\uf0e5\"\n",
    "# table_elem = r\"([A-Z]{1,3}[\\ ,]){3,}|([0-9]{1,3}[\\ ,]){3,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326c5ea4d1b82af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:14:01.172503Z",
     "start_time": "2024-09-30T17:14:01.167281Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text_template(text):\n",
    "    text = remove_index(text)\n",
    "    text = index_dots_removal(text)\n",
    "    text = remove_remaining_indexes(text)\n",
    "    text = remove_table_content(text)\n",
    "    text = remove_remaining_table_from_text(text)\n",
    "    text = remove_table_header(text)\n",
    "    text = flatten_any_list(text)\n",
    "    text = remove_issue_date(text)\n",
    "    text = remove_issue_slash(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9c5297e7c801a",
   "metadata": {},
   "source": [
    "Use language_tool_python to normalize the text and correct it grammatically and syntactically, removing misspelling errors and other errors identified by the tool and considered as such by our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "# Initialize the LanguageTool objects for both Italian and English\n",
    "tool_it = language_tool_python.LanguageTool('it-IT')\n",
    "tool_en = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Function to check if a word is valid in either Italian or English\n",
    "def is_valid_word(word):\n",
    "    matches_it = tool_it.check(word)\n",
    "    matches_en = tool_en.check(word)\n",
    "    # If no matches, the word is valid in either language\n",
    "    return len(matches_it) == 0 or len(matches_en) == 0\n",
    "\n",
    "# Function to correct errors based on ruleId and specific conditions\n",
    "def correct_errors(text, matches):\n",
    "    # Define the rules we want to correct\n",
    "    valid_rules = {'MORFOLOGIK_RULE_IT_IT', 'WHITESPACE_RULE', 'GR_04_002',\n",
    "                   'ITALIAN_WORD_REPEATED_RULE', 'ARTICOLATA_SOSTANTIVO', 'UNPAIRED_BRACKETS'}\n",
    "    \n",
    "    for match in matches:\n",
    "        if match.ruleId in valid_rules and len(match.replacements) == 1:\n",
    "            replacement = match.replacements[0]\n",
    "            \n",
    "            # Extract the word that is marked as an error\n",
    "            incorrect_word = text[match.offset: match.offset + match.errorLength]\n",
    "            \n",
    "            # Check if the word is valid in either English or Italian\n",
    "            if not is_valid_word(incorrect_word):\n",
    "                # Apply only if MORFOLOGIK_RULE_IT_IT doesn't start with uppercase, others directly\n",
    "                if match.ruleId != 'MORFOLOGIK_RULE_IT_IT' or not replacement[0].isupper():\n",
    "                    text = text[:match.offset] + replacement + text[match.offset + match.errorLength:]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826c1eb6ac60b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:18:09.108745Z",
     "start_time": "2024-09-30T17:14:12.815962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over all text elements in text_list\n",
    "cleaned_text_list = list(range(len(text_list)))\n",
    "\n",
    "for i in range(0, len(text_list)):\n",
    "    # Clean each text element\n",
    "    text = clean_text_template(text_list[i])\n",
    "    matches = tool_it.check(text)\n",
    "    cleaned_text = correct_errors(text, matches)\n",
    "    cleaned_text_list[i] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664069aa6db0fae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:18:09.175089Z",
     "start_time": "2024-09-30T17:18:09.169254Z"
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(cleaned_text_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91591ef901a62ba1",
   "metadata": {},
   "source": [
    "Remove some additional patterns, to normalize the text, and discard elements not identified correctly, such as equations, special elements in equations and unwanted pattern brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e8ba38669b89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T17:18:09.973435Z",
     "start_time": "2024-09-30T17:18:09.414170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to lowercase text and remove punctuation\n",
    "def lowercase_and_remove_punctuation(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    import string\n",
    "    # Regex to identify any punctuation in the text\n",
    "    regex = '[' + string.punctuation + ']' #searching for a match with any of the characters inside the square brackets\n",
    "    result = re.sub(regex,' ',text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply lowercase and punctuation removal to each cleaned text\n",
    "cleaned_text_list_to_save = [normalize_whitespace(lowercase_and_remove_punctuation(text)) for text in cleaned_text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "print('[' + string.punctuation + ']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0a4d046ab9408",
   "metadata": {},
   "source": [
    "# Save the pre-processed dataset on a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b464a4050b4db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:58:32.819169Z",
     "start_time": "2024-09-30T18:58:32.560568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with 'file' and 'text' columns\n",
    "df = pd.DataFrame(columns=['file', 'text'])\n",
    "\n",
    "# Gradually add the data to the DataFrame\n",
    "for i in range(len(file_list)):\n",
    "    df.loc[i] = [file_list[i], cleaned_text_list_to_save[i]]\n",
    "    \n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('./Files/cleaned_dataset.csv', index=False)\n",
    "\n",
    "# Output the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8513c228ab659f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:25:48.019776Z",
     "start_time": "2024-10-02T07:25:47.820153Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_all_data_dict = \"./Files/cleaned_dataset.csv\"\n",
    "\n",
    "cleaned_data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "cleaned_data_df = cleaned_data_df.drop(index = 0)\n",
    "cleaned_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28feb4b44d2b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T06:55:09.768781Z",
     "start_time": "2024-10-02T06:55:09.761051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list with all the values in the column 'text'\n",
    "text_list_preprocessed = cleaned_data_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ca04815031e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T09:46:44.996182Z",
     "start_time": "2024-10-01T09:46:44.973444Z"
    }
   },
   "outputs": [],
   "source": [
    "text_list_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9929cbba92612e2",
   "metadata": {},
   "source": [
    "# Import the new cleaned dataset and process it once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec89fd60a8f2473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T06:55:14.208883Z",
     "start_time": "2024-10-02T06:55:14.189593Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_use_case(text):\n",
    "    # Define the regex pattern to match\n",
    "    pattern = r'(use case seu).*'\n",
    "    \n",
    "    # Remove all matches of the pattern\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "for i in range(len(text_list_preprocessed)):\n",
    "    text_list_preprocessed[i] = remove_use_case(text_list_preprocessed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40741c71d06483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:13:38.980155Z",
     "start_time": "2024-10-02T07:13:37.880068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Before removing any special characters try first to solve contractions\n",
    "# Find all the contractions in a text\n",
    "def find_words_with_apostrophe(text):\n",
    "    \"\"\"\n",
    "    This function finds all the words in the provided text that contain the contraction \"l'\".\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text from which to extract words.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of words containing \"l'\".\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match words containing \"'\"\n",
    "    pattern =r\"\\b\\w*’\\w*\\b|\\b\\w*'\\w*\\b\"\n",
    "    \n",
    "    # Use re.findall to get all matches\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "words_with_apostr = []\n",
    "for text in text_list_preprocessed:\n",
    "    words_with_apostr.extend(find_words_with_apostrophe(text))  # Use extend instead of append\n",
    "\n",
    "# Convert to a set to get unique words\n",
    "unique_words_with_apostr = set(words_with_apostr)\n",
    "\n",
    "print(unique_words_with_apostr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33450f6ded365d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# We can also set the api_key as environment variable\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert to a set to get unique words\n",
    "text = f\"\"\"{set(words_with_apostr)}\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "For each word given as input \n",
    "```{text}``` return the contracted word and the respective extension in italian.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "\n",
    "response_text = f'''{response}'''\n",
    "\n",
    "prompt_rewrite = f\"\"\"\n",
    "Given the '''{response_text}''' in the form **contraction** -- extended contraction  \n",
    "return it as \"contraction\" : \"extended contraction\" \n",
    "\"\"\"\n",
    "\n",
    "rewrite = get_completion(prompt_rewrite)\n",
    "\n",
    "# Regular expression pattern and replacement\n",
    "pattern = r\"^\\d+\\.\\s*\\*\\*(.*?)\\*\\*\\s*:\\s*\\*\\*(.*?)\\*\\*\"\n",
    "replacement = r'\"\\1\": \"\\2\",'\n",
    "\n",
    "# Apply regex to each line\n",
    "modified_text = re.sub(pattern, replacement, rewrite, flags=re.MULTILINE)\n",
    "\n",
    "# Output the modified text - which we checked and saved on a txt file in the format 'key':'value', for 'contraction':'extended contraction'\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe0de4e559b791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:12:49.803873Z",
     "start_time": "2024-10-02T07:12:49.782258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "contraction_dictionary = {}\n",
    "\n",
    "# Read the contents of the file\n",
    "with open('italian_contractions.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read each line in the file\n",
    "    for line in file:\n",
    "        # Strip any leading/trailing whitespace and trailing commas\n",
    "        line = line.strip().rstrip(',')\n",
    "        \n",
    "        # Split the line into key and value based on the colon\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)  # Split only at the first colon\n",
    "            # Remove extra quotes and whitespace\n",
    "            \n",
    "            key = key.strip().strip('\"')\n",
    "            value = value.strip().strip('\"')\n",
    "            \n",
    "            # Add to the dictionary\n",
    "            contraction_dictionary[key] = value\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(len(contraction_dictionary),contraction_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763c87100e3db5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:13:08.271107Z",
     "start_time": "2024-10-02T07:12:55.414791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to replace contractions\n",
    "def expand_contractions(text, contraction_dict):\n",
    "    for contraction, expansion in contraction_dict.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "# Iterate through each document in text_list and replace contractions\n",
    "expanded_texts = [expand_contractions(text, contraction_dictionary) for text in text_list_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841373a53c08d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:13:47.929249Z",
     "start_time": "2024-10-02T07:13:46.658989Z"
    }
   },
   "outputs": [],
   "source": [
    "remaining_words_with_apostr = []\n",
    "for text in expanded_texts:\n",
    "    remaining_words_with_apostr.extend(find_words_with_apostrophe(text))  # Use extend instead of append\n",
    "\n",
    "# Convert to a set to get unique words\n",
    "remaining_unique_words_with_apostr = set(remaining_words_with_apostr)\n",
    "\n",
    "print(remaining_unique_words_with_apostr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c75a299cbb39c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:17:59.680900Z",
     "start_time": "2024-10-02T07:17:59.148051Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_any_special_characters(text):\n",
    "    regex = r\"[^a-zA-Z0-9\\s]\"\n",
    "    \n",
    "    cleaned_text = re.sub(regex, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "for i in range(len(expanded_texts)):\n",
    "    expanded_texts[i] = remove_any_special_characters(expanded_texts[i])\n",
    "    expanded_texts[i] = normalize_whitespace(expanded_texts[i])\n",
    "    \n",
    "text_list_preprocessed[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60f550524c66e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:22:05.294797Z",
     "start_time": "2024-10-02T07:22:05.264943Z"
    }
   },
   "outputs": [],
   "source": [
    "num = random.randint(0, len(text_list_preprocessed) - 1)\n",
    "text_list_preprocessed[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2efbb28f4237d4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-30T19:03:47.078033Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the spaCy Italian model\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "# Initialize the spell checker for the Italian language\n",
    "spell = SpellChecker(language='it')\n",
    "\n",
    "# Process each text in the list\n",
    "for i, text in enumerate(tqdm(text_list_preprocessed, desc = \"Checking text: \")):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    \n",
    "    # Use spaCy to process the text (assuming no punctuation)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize the text into words (tokens)\n",
    "    words = [token.text for token in doc]\n",
    "    \n",
    "    # Find the misspelled words using pyspellchecker\n",
    "    misspelled = spell.unknown(words)\n",
    "    \n",
    "    # Correct the misspelled words\n",
    "    for word in misspelled:\n",
    "        # Get the most likely correction\n",
    "        correction = spell.correction(word)\n",
    "        # Get other suggestions (optional)\n",
    "        suggestions = spell.candidates(word)\n",
    "        print(f\"  Misspelled: {word}, Correction: {correction}, Suggestions: {suggestions}\")\n",
    "    \n",
    "    print()  # Add a blank line between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bca253c18a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove other special characters\n",
    "# re.sub('[^a-zA-Z0-9\\\\s]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf34d2b60c8fdd9",
   "metadata": {},
   "source": [
    "# Analyze word frequency and elements present in the resulting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62fcd497ce1ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.810793400Z",
     "start_time": "2024-09-30T12:55:23.641840Z"
    }
   },
   "outputs": [],
   "source": [
    "result = cleaned_text_list_to_save[252]\n",
    "splitted_text = result.lower().split()\n",
    "set_text = set(splitted_text)\n",
    "len(splitted_text), len(set_text), splitted_text, set_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde7603e02a1f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.810793400Z",
     "start_time": "2024-09-30T12:55:23.805854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze vocabulary\n",
    "sorted_text = sorted(set_text)\n",
    "print(sorted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54599529e8064795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.810793400Z",
     "start_time": "2024-09-30T12:55:23.923186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove punctuation before splitting the text\n",
    "import string\n",
    "# Regex to identify any punctuation in the text\n",
    "regex = '[' + string.punctuation + ']' #searching for a match with any of the characters inside the square brackets\n",
    "print(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98d2295e9bfd84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.810793400Z",
     "start_time": "2024-09-30T12:55:24.053685Z"
    }
   },
   "outputs": [],
   "source": [
    "no_punctuation_result = re.sub(regex,' ',result)\n",
    "no_punctuation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d358bb197f5b071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:24.293443Z"
    }
   },
   "outputs": [],
   "source": [
    "no_p_splitted_text = no_punctuation_result.lower().split()\n",
    "set_text_no_p = set(no_p_splitted_text)\n",
    "sort_no_p = sorted(set_text_no_p)\n",
    "print(len(sort_no_p)) \n",
    "print(sort_no_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b0a06d8d42d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:24.403355Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "counts = nltk.FreqDist(no_p_splitted_text)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2622734cdcbd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:24.554688Z"
    }
   },
   "outputs": [],
   "source": [
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0197ffc0890aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:24.671871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the least common words\n",
    "least_common = counts.most_common()[-20:]\n",
    "\n",
    "# Print the least common words\n",
    "least_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb6eabdf276bfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:24.812407Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "print('Italian stopwords:')\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca108f4790ebc938",
   "metadata": {},
   "source": [
    "# Data preprocessing final operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c43c7d004176c",
   "metadata": {},
   "source": [
    "### Revise and decide what operations to maintain and in which order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adaebaef9d7ab60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:24.993556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to preprocess Italian text\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Expand contractions (e.g., \"l’italiana\" --> \"l' italiana\")\n",
    "    # The contractions handler is not optimal, so in case we can create a custom one\n",
    "    text = contractions.fix(text, lang='it')  # Handle contractions in Italian\n",
    "    \n",
    "    # 3. Handle hyphenated words (split at hyphen)\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # 4. Handle words split across lines (concatenate words split with a dash)\n",
    "    text = re.sub(r'\\b(\\w+)-\\n(\\w+)\\b', r'\\1\\2', text)\n",
    "    \n",
    "    # 5. Spelling correction (using TextBlob, note: might not be perfect for Italian)\n",
    "    blob = TextBlob(text)\n",
    "    text = str(blob.correct())  # Correct spelling, but TextBlob has limited Italian support\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145e81b50138b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:25.112640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to normalize Italian text (abbreviations, acronyms, dates, numbers)\n",
    "def normalize_text(text):\n",
    "    # Example dictionary for Italian abbreviations and acronyms\n",
    "    abbrev_dict = {\n",
    "        \"N.B.\": \"Nota bene\",\n",
    "        \"A.D.E\": \"Agenzia Delle Entrate\"\n",
    "    }\n",
    "    \n",
    "    # Replace abbreviations with full forms\n",
    "    for abbrev, full_form in abbrev_dict.items():\n",
    "        text = re.sub(r'\\b' + abbrev + r'\\b', full_form, text)\n",
    "    \n",
    "    # Normalize dates (e.g., 10/05/2022 --> \"10 maggio 2022\")\n",
    "    text = re.sub(r'(\\d{1,2})/(\\d{1,2})/(\\d{2,4})', r'\\1 \\2 \\3', text)\n",
    "    \n",
    "    # Normalize numbers (optional, depending on your need)\n",
    "    # text = re.sub(r'\\d+', '<NUMERO>', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3d27d33b34ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T15:49:38.842083500Z",
     "start_time": "2024-09-30T12:55:25.174545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization and Post-processing for Italian text\n",
    "def postprocess_text(text):\n",
    "    # Tokenize text using SpaCy (Italian language model)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization (reduce words to their lemma in Italian)\n",
    "    lem_text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # Remove stopwords (Italian stopwords from NLTK)\n",
    "    tokens = [token for token in lem_text.split() if token not in stop_words]\n",
    "    \n",
    "    # Frequency analysis (optional)\n",
    "    word_freq = Counter(tokens)\n",
    "    \n",
    "    # Remove rare words (words occurring less than 2 times)\n",
    "    rare_words = [word for word, freq in word_freq.items() if freq < 2]\n",
    "    final_tokens = [word for word in tokens if word not in rare_words]\n",
    "    \n",
    "    return ' '.join(final_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
