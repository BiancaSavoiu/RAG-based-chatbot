{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_input_path = os.path.dirname(path) + \"/Doc_Panthera_Augmented/augmented_dataset_final_outputs.csv\"\n",
    "csv_input_path\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data_df = pd.read_csv(csv_input_path, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the contents\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data and Recursive chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"Text\")\n",
    "docs_data = loader.load()\n",
    "docs_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Data_preprocessing\n",
    "importlib.reload(Data_preprocessing)\n",
    "\n",
    "# Initialize the Preprocessing object\n",
    "preprocessing = Data_preprocessing.Preprocessing()\n",
    "\n",
    "# Iterate through each document in docs_data and clean the text\n",
    "for doc in docs_data:\n",
    "    cleaned_content = preprocessing.clean_text_template(doc.page_content)\n",
    "    doc.page_content = cleaned_content\n",
    "\n",
    "pprint.pprint(docs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split\n",
    "# Possible improvements - future hypertuning of chunk_size and chunk_overlap to improve results and try different slitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = []\n",
    "chunk_number = 1  # Initialize chunk number\n",
    "\n",
    "for doc in docs_data:\n",
    "    # Split the document into chunks\n",
    "    split_docs = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Add chunk number as metadata to each split document\n",
    "    for split_doc in split_docs:\n",
    "        split_doc.metadata['chunk_number'] = chunk_number\n",
    "        docs_processed.append(split_doc)\n",
    "        chunk_number += 1  # Increment chunk number for the next chunk\n",
    "\n",
    "# Print the first 6 processed documents and their count\n",
    "pprint.pprint(docs_processed[0:6])\n",
    "print(len(docs_processed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the generation of synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "token_pro = os.getenv('HUGGINGFACE_TOKEN')\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=token_pro\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1028},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Il tuo compito è scrivere una domanda e una risposta in Italiano dato il contesto testuale della documentazione del software aziendale. \n",
    "La tua domanda deve essere rispondibile con un'informazione specifica dal contesto. \n",
    "Se nel contesto ci sono errori grammaticali o morfologici, correggili nell'output fornito.\n",
    "La tua domanda deve essere formulata nello stesso stile delle domande che gli utenti potrebbero porre ad un helpdesk che si occupa di assistenza clienti\n",
    "per un software aziendale. \n",
    "\n",
    "Nella generazione della domanda non devi fare riferimento direttamente alle descrizioni delle immagini \n",
    "e non devi utilizzare informazioni relative a schermate, finestre e testate di tabelle presenti nelle descrizioni delle immagini. \n",
    "La tua domanda NON deve menzionare frasi come \"secondo il passaggio\", \"nel contesto\", \"nella schermata\" o \"perché l'immagine mostra\". \n",
    "Non fare domande che chiedano quanti elementi sono visualizzati o che riguardano dati e valori presi da descrizioni che possono derivare da immagini. \n",
    "\n",
    "Evita di fare riferimento alle descrizioni delle immagini o di usare dettagli visivi nella generazione della domanda, ma utilizza queste informazioni se necessario solo per la risposta,\n",
    "considerando che la risposta deve guidare l'utente nella risoluzione delle problematiche e dei quesiti posti nella domanda.\n",
    "Puoi formulare domande come: \"Ho un errore\", \"Come posso risolvere il problema\", o \"Come posso sistemare questa situazione?\".\n",
    "La risposta dovrà fare riferimento alle informazioni fornite nel contesto, citando correttamente flag, nomi o altre informazioni pertinenti,\n",
    "senza mai rivelare esempi o dati sensibili.\n",
    "\n",
    "Domanda e risposta devono essere generate in Italiano.\n",
    "\n",
    "Fornisci la tua risposta come segue:\n",
    "\n",
    "Output:::\n",
    "Domanda: (la tua domanda)\n",
    "Risposta: (la tua risposta alla domanda)\n",
    "\n",
    "Ora ecco il contesto:\n",
    "\n",
    "Contesto: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "document = random.sample(docs_processed, 1)\n",
    "pprint.pprint(document)\n",
    "for sampled_document in document:\n",
    "    pprint.pprint(call_llm(llm_client, QA_generation_prompt.format(context=sampled_document.page_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Generation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA using only a sampled chunk each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATASET USING ONLY SAMPLED CHUNK\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_GENERATIONS = 100\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Domanda: \")[-1].split(\"Risposta: \")[0]\n",
    "        answer = output_QA_couple.split(\"Risposta: \")[-1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"source_doc\": sampled_context.metadata[\"FileName\"],\n",
    "                \"chunk_num\": sampled_context.metadata[\"chunk_number\"]\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA testset using not only a sampled chunk, but also the adjacent chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATASET USING NOT ONLY THE SAMPLED CHUNK, BUT ALSO THE ADJACENT ONES\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_GENERATIONS = 200\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "\n",
    "def get_adjacent_chunks(sampled_chunk, docs, radius=1):\n",
    "    \"\"\"\n",
    "    Get adjacent chunks within a given radius for the same document.\n",
    "    \"\"\"\n",
    "    source_doc = sampled_chunk.metadata[\"FileName\"]\n",
    "    chunk_num = sampled_chunk.metadata[\"chunk_number\"]\n",
    "    \n",
    "    # Filter all chunks belonging to the same source_doc\n",
    "    doc_chunks = [doc for doc in docs if doc.metadata[\"FileName\"] == source_doc]\n",
    "    \n",
    "    # Sort the chunks by their chunk_number\n",
    "    doc_chunks.sort(key=lambda x: x.metadata[\"chunk_number\"])\n",
    "    \n",
    "    # Identify adjacent chunks\n",
    "    start = max(chunk_num - radius, 0)\n",
    "    end = chunk_num + radius + 1  # Include current and adjacent chunks\n",
    "    return [chunk for chunk in doc_chunks if start <= chunk.metadata[\"chunk_number\"] < end]\n",
    "\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Get the sampled chunk along with its adjacent chunks\n",
    "    adjacent_chunks = get_adjacent_chunks(sampled_context, docs_processed, radius=1)\n",
    "    \n",
    "    # Combine contents of the adjacent chunks\n",
    "    combined_context = \" \".join(chunk.page_content for chunk in adjacent_chunks)\n",
    "\n",
    "    # Record all chunk numbers used\n",
    "    chunk_numbers = [chunk.metadata[\"chunk_number\"] for chunk in adjacent_chunks]\n",
    "    \n",
    "    # Generate QA couple using the combined context\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=combined_context))\n",
    "    \n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Domanda: \")[-1].split(\"Risposta: \")[0]\n",
    "        answer = output_QA_couple.split(\"Risposta: \")[-1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context\": combined_context,\n",
    "                \"source_doc\": sampled_context.metadata[\"FileName\"],\n",
    "                \"chunk_num\": chunk_numbers\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {sampled_context.metadata}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA testset, using more than a chunk and adjacent ones sampled from the same document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATASET USING NOT ONLY THE SAMPLED CHUNK, BUT ALSO OTHER CHUNKS SAMPLED RANDOMLY FROM THE SAME DOCUMENT\n",
    "def get_random_chunks(sampled_chunk, docs, n_random=2, radius=1):\n",
    "    \"\"\"\n",
    "    Get n_random chunks from the same document (excluding the sampled chunk and its adjacent ones),\n",
    "    along with their adjacent chunks, ensuring no duplicate chunk numbers.\n",
    "    \"\"\"\n",
    "    source_doc = sampled_chunk.metadata[\"FileName\"]\n",
    "    chunk_num = sampled_chunk.metadata[\"chunk_number\"]\n",
    "    \n",
    "    # Filter all chunks belonging to the same source_doc\n",
    "    doc_chunks = [doc for doc in docs if doc.metadata[\"FileName\"] == source_doc]\n",
    "    \n",
    "    # Exclude the sampled chunk and its adjacent chunks\n",
    "    start = max(chunk_num - radius, 0)\n",
    "    end = chunk_num + radius + 1\n",
    "    excluded_chunks = {chunk.metadata[\"chunk_number\"] for chunk in doc_chunks if start <= chunk.metadata[\"chunk_number\"] < end}\n",
    "    available_chunks = [chunk for chunk in doc_chunks if chunk.metadata[\"chunk_number\"] not in excluded_chunks]\n",
    "    \n",
    "    # Randomly sample chunks\n",
    "    sampled_random_chunks = random.sample(available_chunks, min(n_random, len(available_chunks)))\n",
    "    \n",
    "    # Include adjacent chunks for each randomly sampled chunk and remove duplicates\n",
    "    all_random_chunks = []\n",
    "    unique_chunk_nums = set()  # Track unique chunk numbers\n",
    "\n",
    "    for chunk in sampled_random_chunks:\n",
    "        # Get adjacent chunks for the randomly sampled chunk\n",
    "        adjacent_chunks = get_adjacent_chunks(chunk, docs, radius)\n",
    "        \n",
    "        # Add chunks to the result only if their chunk_num is unique\n",
    "        for adj_chunk in adjacent_chunks:\n",
    "            if adj_chunk.metadata[\"chunk_number\"] not in unique_chunk_nums:\n",
    "                all_random_chunks.append(adj_chunk)\n",
    "                unique_chunk_nums.add(adj_chunk.metadata[\"chunk_number\"])\n",
    "    \n",
    "    return all_random_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATASET USING NOT ONLY THE SAMPLED CHUNK, BUT ALSO THE ADJACENT ONES\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_GENERATIONS = 200\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    try:\n",
    "        # Get the sampled chunk along with its adjacent chunks\n",
    "        adjacent_chunks = get_adjacent_chunks(sampled_context, docs_processed, radius=1)\n",
    "        \n",
    "        # Get additional random chunks (and their adjacent chunks) from the same document\n",
    "        random_chunks = get_random_chunks(sampled_context, docs_processed, n_random=2, radius=1)\n",
    "        \n",
    "        # Combine contents of all chunks\n",
    "        combined_chunks = adjacent_chunks + random_chunks\n",
    "        combined_context = \" \".join(chunk.page_content for chunk in combined_chunks)\n",
    "\n",
    "        # Record all chunk numbers used\n",
    "        chunk_numbers = [chunk.metadata[\"chunk_number\"] for chunk in combined_chunks]\n",
    "        \n",
    "        # Generate QA couple using the combined context\n",
    "        output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=combined_context))\n",
    "        \n",
    "        question = output_QA_couple.split(\"Domanda: \")[-1].split(\"Risposta: \")[0]\n",
    "        answer = output_QA_couple.split(\"Risposta: \")[-1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context\": combined_context,\n",
    "                \"source_doc\": sampled_context.metadata[\"FileName\"],\n",
    "                \"chunk_num\": chunk_numbers\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {sampled_context.metadata}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critique agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "Sarà fornito un contesto e una domanda.\n",
    "Il tuo compito è di fornire una valutazione per indicare quanto bene si possa rispondere in modo univoco alla domanda data con il contesto fornito.\n",
    "Dai la tua risposta su una scala da 1 a 5, dove 1 significa che la domanda non è affatto rispondibile con il contesto, \n",
    "e 5 significa che la domanda è chiaramente e univocamente rispondibile con il contesto.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, come numero tra 1 e 5)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda e il contesto.\n",
    "\n",
    "Domanda: {question}\n",
    "Contesto: {context}\n",
    "\n",
    "Output:::\n",
    "\"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "Ti sarà fornita una domanda.\n",
    "Il tuo compito è di fornire una \"valutazione totale\" che rappresenti quanto utile possa essere questa domanda per gli utenti che chiedono assistenza all'help desk riguardo a specifiche funzionalità\n",
    "del software gestionale e la relativa documentazione.\n",
    "Dai la tua risposta su una scala da 1 a 5, dove 1 significa che la domanda non è per nulla utile, e 5 significa che la domanda è estremamente utile.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, come numero tra 1 e 5)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda.\n",
    "\n",
    "Domanda: {question}\n",
    "\n",
    "Output:::\n",
    "\"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "Ti sarà fornita una domanda.\n",
    "Il tuo compito è di fornire una \"valutazione totale\" che rappresenti quanto questa domanda sia indipendente dal contesto.\n",
    "Dai la tua risposta su una scala da 1 a 5, dove 1 significa che la domanda dipende da informazioni aggiuntive per essere compresa, e 5 significa che la domanda ha senso da sola.\n",
    "Ad esempio, se la domanda si riferisce a un contesto particolare, come \"nel contesto\" o \"nel documento\", la valutazione deve essere 1.\n",
    "Le domande possono contenere termini tecnici o acronimi e ricevere comunque una valutazione di 5: deve semplicemente essere chiaro per un operatore con accesso alla documentazione di cosa tratta la domanda.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, come numero tra 1 e 5)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda.\n",
    "\n",
    "Domanda: {question}\n",
    "\n",
    "Output:::\n",
    "\"\"\"\n",
    "\n",
    "image_relevance_critique_prompt = \"\"\"\n",
    "Ti sarà fornita una domanda.\n",
    "Il tuo compito è di valutare se la domanda riguarda entità come immagini, schermate, finestre o testate di tabelle che appaiono nelle descrizioni delle immagini.\n",
    "Inoltre, valuta se la domanda potrebbe contenere dati sensibili derivanti dalle descrizioni delle immagini, come informazioni su schermate, immagini o tabelle.\n",
    "Se la domanda è probabilmente derivata da un testo che descrive il contenuto di un'immagine, restituisci un punteggio di 0.\n",
    "Se la domanda non riguarda le immagini o non contiene dati sensibili, restituisci un punteggio di 1.\n",
    "\n",
    "Fornisci la tua risposta esattamente nel seguente formato:\n",
    "\n",
    "Output:::\n",
    "Valutazione totale: (il tuo punteggio, che può essere 0 o 1)\n",
    "Output:::\n",
    "\n",
    "Ora ecco la domanda.\n",
    "\n",
    "Domanda: {question}\n",
    "\n",
    "Output:::\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"groundedness_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,  \n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"standalone_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"relevance_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    time.sleep(1)\n",
    "    evaluations = {\n",
    "        \"image_relevance\": call_llm(\n",
    "            llm_client,\n",
    "            image_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Initialize scores with None as default values\n",
    "    output.update({\n",
    "        \"image_relevance_score\": None,\n",
    "    })\n",
    "\n",
    "    # Example code with regex substitution\n",
    "    for criterion, evaluation in evaluations.items():\n",
    "    \n",
    "        # Use regex to find the score following \"Valutazione totale:\"\n",
    "        match = re.search(r\"Valutazione totale:\\s*(\\d+)\", evaluation)\n",
    "    \n",
    "        # Extract the score if the match is found, else set it to a default value (e.g., 0 or None)\n",
    "        score = int(match.group(1)) if match else 0\n",
    "    \n",
    "        output.update(\n",
    "            {\n",
    "                f\"{criterion}_score\": score\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Create DataFrame after ensuring all columns are initialized\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "# Calculate the average score across the three columns\n",
    "generated_questions[\"average_score\"] = (\n",
    "    generated_questions[\"groundedness_score\"] \n",
    "    + generated_questions[\"relevance_score\"]\n",
    "    + generated_questions[ \"standalone_score\"]\n",
    ")/3\n",
    "\n",
    "# Filter to keep rows where the average score is greater than 4 and image_relevance_score equals 1\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"average_score\"] > 4) & \n",
    "    (generated_questions[\"image_relevance_score\"] == 1) &\n",
    "    (generated_questions[\"groundedness_score\"] > 4)\n",
    "]\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "            \"average_score\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the dataset from filtered DataFrame\n",
    "new_eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing dataset\n",
    "existing_dataset = Dataset.load_from_disk(\"eval_dataset_random_chunks\")\n",
    "old_elem = pd.DataFrame(existing_dataset)\n",
    "new_elem = pd.DataFrame(new_eval_dataset)\n",
    "\n",
    "# Concatenate the old and new DataFrames\n",
    "combined_df = pd.concat([old_elem, new_elem], ignore_index=True)\n",
    "\n",
    "# Identify unhashable columns (columns with lists, dicts, etc.)\n",
    "unhashable_cols = [\n",
    "    col for col in combined_df.columns \n",
    "    if combined_df[col].apply(lambda x: isinstance(x, (list, dict))).any()\n",
    "]\n",
    "\n",
    "# Separate the unhashable columns\n",
    "hashable_df = combined_df.drop(columns=unhashable_cols)\n",
    "unhashable_df = combined_df[unhashable_cols]\n",
    "\n",
    "# Drop duplicates on the hashable part and keep the corresponding indices\n",
    "deduplicated_hashable_df = hashable_df.drop_duplicates()\n",
    "deduplicated_indices = deduplicated_hashable_df.index\n",
    "\n",
    "# Use the indices to filter the unhashable part\n",
    "deduplicated_unhashable_df = unhashable_df.loc[deduplicated_indices]\n",
    "\n",
    "# Combine the deduplicated hashable and unhashable parts\n",
    "combined_df = pd.concat(\n",
    "    [deduplicated_hashable_df.reset_index(drop=True), deduplicated_unhashable_df.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert the combined DataFrame back to a Dataset\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.save_to_disk(\"eval_dataset_random_chunks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
