{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "question_out_of_scope = \"Quando è morto Giulio Cesare?\"\n",
    "multiple_questions = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione? Chi è Giulio Cesare?\"\n",
    "multiple_valid_questions = \"Cosa significa che una fattura è in mancata consegna? Il cliente ha ricevuto la fattura?\"\n",
    "q_client = \"Addebito bollo su nota credito. Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_client_without_object = \"Su nota credito non mette più addebito bollo: precedente nota credito si.\"\n",
    "q_rewritten = \"Perché la nota di credito non sta aggiungendo più il bollo e come risolvere questo problema?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question re-writing to have -> Rewrite - retrieve -read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot - compare english and italian prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import pprint\n",
    "\n",
    "model_llama = ChatOllama(\n",
    "    model=\"llama3.2\", \n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions, in Italian, of the given user question to retrieve relevant documents from a vector database. \n",
    "The context of our application is related to Enterprise Resource Planning (ERP) software's technical manuals (specifically Panthera software) or, more generally, topics related to computer science, including system configuration,\n",
    "module functionality, troubleshooting, and implementation guidelines.\n",
    "Your goal is to generate multiple perspectives on the question to help the user overcome limitations of distance-based similarity search while focusing strictly on the context of ERP software documentation\n",
    "or relevant computer science topics.\n",
    "In cases where the user provides multiple questions, only respond to the relevant ones related to ERP documentation or computer science, generating five different versions of the relevant question. \n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Before generating alternatives, ensure the user's question is related to ERP technical documentation or relevant computer science topics. \n",
    "If any of the questions are out of scope or irrelevant to ERP manuals or computer science topics, disregard them entirely. \n",
    "You don't need to ignore all the questions, but only the ones that are out of scope.\n",
    "\n",
    "Use the ERP context only as information, but do not mention it in the rewritten questions.\n",
    "Provide the created alternative questions separated by newlines, and structure the output to contain only the rewritten questions in a bullet list.\n",
    "Output only the bullet list of the rewritten questions, without any specification about the out of scope parts of the question.\n",
    "\n",
    "**Instructions:**\n",
    "1. Generate rewritten questions that maintain the original meaning, exploring different formulations.\n",
    "2. Ignore questions not relevant to ERP manuals or computer science topics.\n",
    "3. If there are not questions relevant to the context just specify that you cannot answer to out of scope demands.\n",
    "4. If there is any relevant question, provide alternative questions in a bullet-pointed list with new lines separating them, rewriting only the questions relevant to the application.\n",
    "5. The output should contain only the rewritten questions, without explanations or comments. Do not add any comment about the not questions.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model_llama\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_italian = \"\"\"Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse della domanda fornita dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Genera domande riscritte che mantengano il significato originale, esplorando diverse formulazioni e angolazioni. \n",
    "2. Ignora le domande che non sono pertinenti ai manuali di software gestionale o agli argomenti di informatica.\n",
    "3. Fornisci le domande alternative in un elenco puntato separato da nuove righe.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "Svolgi la task solo per le domande rilevanti al contesto del software. In questo caso, cerca di migliorare la formulazione originale \n",
    "esplorando diverse angolazioni che aiutino a comprendere meglio il problema o la richiesta, rendendo più chiare e leggibili le domande per un utente generico.\n",
    "\n",
    "Domanda originale: {question} \"\"\"\n",
    "\n",
    "prompt_perspectives_ita = ChatPromptTemplate.from_template(template_italian)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries_ita = (\n",
    "    prompt_perspectives_ita \n",
    "    | model_llama\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare english and italian prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question_out_of_scope)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_questions)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_valid_questions)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(q_client)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": q_client})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": q_client})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(q_client_without_object)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": q_client_without_object})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": q_client_without_object})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(q_rewritten)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": q_rewritten})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": q_rewritten})\n",
    "pprint.pprint(rewritten_question_ita)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_italian_few_shot = \"\"\"Sei un modello linguistico AI che svolge il ruolo di assistente clienti per il software Panthera. \n",
    "Il tuo compito è generare cinque versioni diverse delle domande fornite dall'utente per recuperare documenti rilevanti da un database vettoriale. \n",
    "Il contesto riguarda manuali tecnici di software per la gestione aziendale. Non fornire la riscrittura della domanda se non è relativa al contesto.\n",
    "\n",
    "**Istruzioni:**\n",
    "1. Valuta attentamente la domanda fornita. Se contiene più interrogativi, identifica quelli pertinenti ai manuali di software gestionale o agli argomenti di informatica e riformula solo quelli.\n",
    "2. Se ci sono domande pertinenti, genera cinque versioni riscritte per ciascuna di esse, mantenendo il significato originale ed esplorando diverse formulazioni e angolazioni.\n",
    "3. Se non ci sono domande pertinenti nella richiesta, non fornire alcuna risposta e indica che non è possibile fornire una risposta.\n",
    "4. L'output deve contenere solo le domande riscritte, senza spiegazioni o commenti.\n",
    "\n",
    "**Esempio di riformulazione di una domanda dell'utente, relativa al contesto del software Panthera:**\n",
    "<Domanda>: <Addebito bollo su nota credito. Su nota credito non mette più addebito bollo: precedente nota credito si.>\n",
    "<Risposta>:\n",
    "< * Perché l’addebito del bollo non viene più applicato sulla nota di credito, mentre su una precedente nota di credito era stato inserito?\n",
    "  * Come mai sulla nuova nota di credito manca l’addebito del bollo, che invece era presente su una precedente?\n",
    "  * Qual è il motivo per cui il bollo non viene più addebitato sulla nota di credito, a differenza di quanto accaduto prima?\n",
    "  * Per quale ragione il bollo non è stato addebitato sulla nota di credito corrente, quando su una nota precedente era presente?\n",
    "  * Perché l’applicazione del bollo su una nota di credito attuale non avviene, mentre su una nota di credito passata era presente? >\n",
    "\n",
    "**Esempio di domanda non rilevante al contesto, a cui il sistema non deve rispondere:**\n",
    "<Domanda>: <Quale è la migliore ricetta per fare la carbonara?>\n",
    "<Risposta>: <Non posso fornire una risposta alla tua domanda in quanto non è relativa al contesto del software Panthera. Posso aiutarti con qualcos'altro?>\n",
    "\n",
    "Domanda originale: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives_ita_few_shot = ChatPromptTemplate.from_template(template_italian_few_shot)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries_ita_few_shot = (\n",
    "    prompt_perspectives_ita_few_shot\n",
    "    | model_llama\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Few-shot results\")\n",
    "pprint.pprint(\"Italian prompting with some examples:\")\n",
    "rewritten_question_ita_few_shot = generate_queries_ita_few_shot.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question_ita_few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_valid_questions)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Few-shot results\")\n",
    "pprint.pprint(\"Italian prompting with some examples:\")\n",
    "rewritten_question_ita_few_shot = generate_queries_ita_few_shot.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question_ita_few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question_out_of_scope)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Few-shot results\")\n",
    "pprint.pprint(\"Italian prompting with some examples:\")\n",
    "rewritten_question_ita_few_shot = generate_queries_ita_few_shot.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question_ita_few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_questions)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Few-shot results\")\n",
    "pprint.pprint(\"Italian prompting with some examples:\")\n",
    "rewritten_question_ita_few_shot = generate_queries_ita_few_shot.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question_ita_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare llama3.2 rewriting results with GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions, in Italian, of the given user question to retrieve relevant documents from a vector database. \n",
    "The context of our application is related to Enterprise Resource Planning (ERP) software's technical manuals (specifically Panthera software) or, more generally, topics related to computer science, including system configuration,\n",
    "module functionality, troubleshooting, and implementation guidelines.\n",
    "Your goal is to generate multiple perspectives on the question to help the user overcome limitations of distance-based similarity search while focusing strictly on the context of ERP software documentation\n",
    "or relevant computer science topics.\n",
    "In cases where the user provides multiple questions, only respond to the relevant ones related to ERP documentation or computer science, generating five different versions of the relevant question. \n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Before generating alternatives, ensure the user's question is related to ERP technical documentation or relevant computer science topics. \n",
    "If any of the questions are out of scope or irrelevant to ERP manuals or computer science topics, disregard them entirely. \n",
    "You don't need to ignore all the questions, but only the ones that are out of scope.\n",
    "\n",
    "Use the ERP context only as information, but do not mention it in the rewritten questions.\n",
    "Provide the created alternative questions separated by newlines, and structure the output to contain only the rewritten questions in a bullet list.\n",
    "Output only the bullet list of the rewritten questions, without any specification about the out of scope parts of the question.\n",
    "\n",
    "**Instructions:**\n",
    "1. Generate rewritten questions that maintain the original meaning, exploring different formulations.\n",
    "2. Ignore questions not relevant to ERP manuals or computer science topics.\n",
    "3. If there are not questions relevant to the context just specify that you cannot answer to out of scope demands.\n",
    "4. If there is any relevant question, provide alternative questions in a bullet-pointed list with new lines separating them, rewriting only the questions relevant to the application.\n",
    "5. The output should contain only the rewritten questions, without explanations or comments. Do not add any comment about the not relevant questions.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries_gpt = (\n",
    "    prompt_perspectives_ita \n",
    "    | ChatOpenAI(temperature=0, model=\"gpt-4o\") \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(q_client)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": q_client})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": q_client})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting + GPT\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": q_client}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": question})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting + GPT:\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question_out_of_scope)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": question_out_of_scope})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting + GPT:\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": question_out_of_scope}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_questions)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": multiple_questions})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting + GPT:\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": multiple_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(multiple_valid_questions)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": multiple_valid_questions})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting + GPT:\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": multiple_valid_questions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(q_client_without_object)\n",
    "print(\"\")\n",
    "pprint.pprint(\"Zero-shot results\")\n",
    "pprint.pprint(\"English prompting:\")\n",
    "rewritten_question = generate_queries.invoke({\"question\": q_client_without_object})\n",
    "pprint.pprint(rewritten_question)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting:\")\n",
    "rewritten_question_ita = generate_queries_ita.invoke({\"question\": q_client_without_object})\n",
    "pprint.pprint(rewritten_question_ita)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(\"Italian prompting + GPT:\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": q_client_without_object}))\n",
    "pprint.pprint(\"Italian prompting + GPT:\")\n",
    "pprint.pprint(generate_queries.invoke({\"question\": q_client}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking of the rewritten questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reranking of the rewritten questions we can decide to:\n",
    "1. Weight the retrieved documents based on the reranking score\n",
    "2. Retrieve documents only for the best reformulations of the question - set appropriately the threshold of good rewritten questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def compute_cosine_similarity(embedding, embeddings):\n",
    "    \"\"\"Computes cosine similarity between one vector and a set of vectors.\"\"\"\n",
    "    # Compute the dot product between the single vector and each vector in embeddings\n",
    "    dot_products = np.dot(embeddings, embedding)\n",
    "    \n",
    "    # Compute the norms\n",
    "    norm_embedding = np.linalg.norm(embedding)\n",
    "    norms_embeddings = np.linalg.norm(embeddings, axis=1)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = dot_products / (norm_embedding * norms_embeddings)\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def compute_bm25_score(original, rewrites):\n",
    "    \"\"\"Computes BM25 scores for original question against rewritten questions.\"\"\"\n",
    "    # Prepare documents for BM25\n",
    "    documents = [original] + rewrites\n",
    "    tokenized_documents = [doc.split(\" \") for doc in documents]\n",
    "    \n",
    "    # Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    \n",
    "    # Get BM25 scores for the original question against all rewrites\n",
    "    scores = bm25.get_scores(tokenized_documents[0])\n",
    "    \n",
    "    return scores[1:]  # Exclude the score for the original question\n",
    "\n",
    "def rerank_questions(original_question, alpha=1.0, threshold=0.0):\n",
    "    \"\"\"Rerank the rewritten questions based on cosine similarity and BM25 scores.\"\"\"\n",
    "    \n",
    "    # Load a pre-trained model\n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    original_embedding = model.encode(original_question)\n",
    "    rewritten_questions = generate_queries_gpt.invoke({\"question\": original_question})\n",
    "    rewritten_embeddings = model.encode(rewritten_questions)\n",
    "    pprint.pprint(original_question)\n",
    "\n",
    "    # Convert list of embeddings to a numpy array\n",
    "    rewritten_embeddings = np.vstack(rewritten_embeddings)  # Stack into a single 2D array\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = compute_cosine_similarity(original_embedding, rewritten_embeddings)\n",
    "    \n",
    "    # Compute BM25 scores\n",
    "    bm25_scores = compute_bm25_score(original_question, rewritten_questions)\n",
    "\n",
    "    # Combine scores\n",
    "    weighted_scores = alpha * cosine_similarities + (1 - alpha) * bm25_scores\n",
    "    \n",
    "    # Create a ranking of rewritten questions\n",
    "    ranked_indices = np.argsort(weighted_scores)[::-1]  # Sort in descending order\n",
    "    \n",
    "    # Filter questions based on the threshold\n",
    "    filtered_questions = [(rewritten_questions[i], weighted_scores[i]) for i in ranked_indices if weighted_scores[i] >= threshold]\n",
    "    questions = [original_question]\n",
    "    for question, score in filtered_questions:\n",
    "        print(question, score)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Rerank questions and apply the threshold\n",
    "filtered_questions = rerank_questions(q_client_without_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the documents based on query preprocessing and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore_fp16 = FAISS.load_local(\"local_model_index\", M3EmbeddingFP16(), allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore_fp16.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_fp16.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(q_client_without_object)\n",
    "pprint.pprint(retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a check to immediately disregard questions out of scope, otherwise the model tries to retrieve some documents, but it doesn't make any sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative part of the RAG system - generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "template_RAG_generation = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template_RAG_generation)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "response_text = rag_chain.invoke(q_client_without_object)\n",
    "pprint.pprint(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domanda = \"Come posso impostare una stampante predefinita per il mio utente?\"\n",
    "text = rag_chain.invoke(domanda)\n",
    "pprint.pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domanda = \"Come inserisco una nuova attività fatturabile per il mio utente?\"\n",
    "text = rag_chain.invoke(domanda)\n",
    "pprint.pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domanda = \"Cosa c'è nella pagina timesheet risorsa?\"\n",
    "text = rag_chain.invoke(domanda)\n",
    "pprint.pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(domanda)\n",
    "pprint.pprint(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(retrieved_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_questions(domanda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
