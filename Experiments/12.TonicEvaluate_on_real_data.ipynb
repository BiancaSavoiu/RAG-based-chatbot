{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import real evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_outputs_from_csv(csv_file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame with semicolon as the delimiter\n",
    "        df = pd.read_csv(csv_file_path, sep=';', encoding='utf-8')\n",
    "        print(f\"Data successfully loaded from {csv_file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No CSV file found at {csv_file_path}. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=[\"question\", \"answer\"])\n",
    "\n",
    "# Usage example - change the input file as needed\n",
    "eval_df = load_outputs_from_csv(\"real_evaluation_set.csv\")\n",
    "print(len(eval_df))\n",
    "display(eval_df.head())  # Display the first few rows of the loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import documents dataset to have the original data for the Retrieval part of the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset for RAG\n",
    "import pandas as pd\n",
    "filename_all_data_dict = \"./Files/final_dataset.csv\"\n",
    "\n",
    "data_df = pd.read_csv(filename_all_data_dict, names = ['file', 'text'], header = None)\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Retrieval part of the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import pprint\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"text\")\n",
    "docs_data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)\n",
    "splits = text_splitter.split_documents(docs_data)\n",
    "pprint.pprint(splits[0:6])\n",
    "pprint.pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.load_local(\"local_model_index\", M3EmbeddingFP16(), allow_dangerous_deserialization=True)\n",
    "vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Generative part of the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def invoke_model_and_store_response(eval_df, model_invoke_func, response_column):\n",
    "    \"\"\"\n",
    "    Iterate over the DataFrame, invoke the model for each question, and store the response.\n",
    "\n",
    "    Parameters:\n",
    "    eval_df (pd.DataFrame): DataFrame containing the column 'question'.\n",
    "    model_invoke_func: A function that takes a question as input and returns a model response.\n",
    "    response_column (str): The column name to store the model responses. Defaults to \"model_response_llama_instruct\".\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with model responses.\n",
    "    \"\"\"\n",
    "    # Iterate over each row in the DataFrame using `iterrows`\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        question = row[\"question\"]  # Get the question from the current row\n",
    "        \n",
    "        # Invoke the model to get the response\n",
    "        response = model_invoke_func(question)  # Assuming the model function takes the question as a string\n",
    "        \n",
    "        # Store the response in the specified column for the corresponding row\n",
    "        eval_df.at[idx, response_column] = response\n",
    "\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG with RAG-evaluator dataset\n",
    "from rag_evaluator import RAGEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = RAGEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_responses(eval_df, evaluator, model_response_df_column):\n",
    "    \"\"\"\n",
    "    Evaluate model responses against reference answers.\n",
    "\n",
    "    Parameters:\n",
    "    eval_df (pd.DataFrame): DataFrame containing the columns 'question', 'model_response', and 'answer'.\n",
    "    evaluator: An object that has an `evaluate_all` method for evaluating responses.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the evaluation results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in eval_df.iterrows():\n",
    "        question = row['question']\n",
    "        response = row[model_response_df_column]\n",
    "        reference = row['answer']\n",
    "        \n",
    "        # Evaluate and store the results\n",
    "        evaluation = evaluator.evaluate_all(question, response, reference)\n",
    "        results.append(evaluation)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Example usage\n",
    "# results_df = evaluate_responses(eval_df, evaluator)\n",
    "# display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aggregate_metrics(results_df):\n",
    "    \"\"\"\n",
    "    Aggregate evaluation metrics from the results DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the aggregated metrics.\n",
    "    \"\"\"\n",
    "    # Aggregate metrics to get a single evaluation for the model\n",
    "    aggregated_results = {\n",
    "        \"BLEU\": results_df[\"BLEU\"].mean(),\n",
    "        \"ROUGE-1\": results_df[\"ROUGE-1\"].mean(),\n",
    "        \"ROUGE-2\": results_df[\"ROUGE-2\"].mean(), \n",
    "        \"ROUGE-L\": results_df[\"ROUGE-L\"].mean(),\n",
    "        \"BERT P\": results_df[\"BERT P\"].mean(),\n",
    "        \"BERT R\": results_df[\"BERT R\"].mean(),\n",
    "        \"BERT F1\": results_df[\"BERT F1\"].mean(),\n",
    "        \"Perplexity\": results_df[\"Perplexity\"].mean(),\n",
    "        \"Diversity\": results_df[\"Diversity\"].mean(),\n",
    "    }\n",
    "\n",
    "    # Convert aggregated results to a DataFrame for better readability\n",
    "    aggregated_results_df = pd.DataFrame(aggregated_results, index=[0])\n",
    "\n",
    "    return aggregated_results_df\n",
    "\n",
    "# Example usage\n",
    "# aggregated_results_df = aggregate_metrics(results_df)\n",
    "# display(aggregated_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_and_plot(eval_df, evaluator, model_response_df_column, title):\n",
    "    \"\"\"\n",
    "    Evaluate model responses and plot the progression of evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    eval_df (pd.DataFrame): DataFrame containing the columns 'question', 'model_response', and 'answer'.\n",
    "    evaluator: An object that has an `evaluate_all` method for evaluating responses.\n",
    "    \"\"\"\n",
    "    cumulative_metrics = {\n",
    "        \"BLEU\": 0,\n",
    "        \"ROUGE-1\": 0,\n",
    "        \"BERT P\": 0,\n",
    "        \"BERT R\": 0,\n",
    "        \"BERT F1\": 0,\n",
    "        \"Perplexity\": 0,\n",
    "        \"Diversity\": 0,\n",
    "    }\n",
    "\n",
    "    # List to store running means\n",
    "    running_means = []\n",
    "\n",
    "    # Evaluate each entry\n",
    "    for index, row in eval_df.iterrows():\n",
    "        question = row['question']\n",
    "        response = row[model_response_df_column]\n",
    "        reference = row['answer']\n",
    "        \n",
    "        # Evaluate and store the results\n",
    "        evaluation = evaluator.evaluate_all(question, response, reference)\n",
    "        \n",
    "        # Update cumulative sums\n",
    "        for metric in cumulative_metrics.keys():\n",
    "            cumulative_metrics[metric] += evaluation[metric]\n",
    "        \n",
    "        # Compute the current means\n",
    "        current_means = {metric: cumulative_metrics[metric] / (index + 1) for metric in cumulative_metrics}\n",
    "        running_means.append(current_means)\n",
    "\n",
    "    # Create a DataFrame for running means\n",
    "    running_means_df = pd.DataFrame(running_means)\n",
    "\n",
    "    # Create subplots for both plots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # First plot: Progression of evaluation metrics\n",
    "    for metric in cumulative_metrics.keys():\n",
    "        axs[0].plot(running_means_df.index + 1, running_means_df[metric], marker='o', label=metric)\n",
    "\n",
    "    axs[0].set_title(title)\n",
    "    axs[0].set_xlabel('Number of Evaluation Points (N)')\n",
    "    axs[0].set_ylabel('Mean Metric Value')\n",
    "    axs[0].axhline(y=0, color='grey', linestyle='--')  # Optional: Add a horizontal line at y=0 for reference\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Second plot: Zoomed in on values between 0 and 1\n",
    "    for metric in cumulative_metrics.keys():\n",
    "        axs[1].plot(running_means_df.index + 1, running_means_df[metric], marker='o', label=metric)\n",
    "\n",
    "    axs[1].set_title('Zoomed Progression of Evaluation Metrics')\n",
    "    axs[1].set_xlabel('Number of Evaluation Points (N)')\n",
    "    axs[1].set_ylabel('Mean Metric Value')\n",
    "    axs[1].set_ylim(0, 1.1)  # Set y-axis limits to zoom\n",
    "    axs[1].legend(loc='best', fontsize='small', frameon=True, borderpad=0.5, bbox_to_anchor=(1, 1))\n",
    "    axs[1].grid()\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# evaluate_and_plot(eval_df, evaluator, 'model_response')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG with Lllama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_llama = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_column = 'model_response'\n",
    "eval_df = invoke_model_and_store_response(eval_df, rag_chain.invoke, response_column)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG with Llama3.2 Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "model_llama_instruct = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "\n",
    "rag_chain_llama_instruct = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama_instruct\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_column = \"model_response_llama_instruct\"\n",
    "eval_df = invoke_model_and_store_response(eval_df, rag_chain_llama_instruct.invoke, response_column)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Chain\n",
    "rag_chain_gpt = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_gpt\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_column = 'model_response_gpt'\n",
    "eval_df = invoke_model_and_store_response(eval_df, rag_chain_gpt.invoke, response_column)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG with GPT-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old_gpt = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Chain\n",
    "rag_chain_old_gpt = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_old_gpt\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_column = 'model_response_old_gpt'\n",
    "eval_df = invoke_model_and_store_response(eval_df, rag_chain_old_gpt.invoke, response_column)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models comparison on basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the BLEU score is correct, given that it should be from 0 to 1, but has higher values\n",
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = 'model_response'\n",
    "title = 'Progression of Evaluation Metrics for Llama3.2'\n",
    "result_df_llama = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(result_df_llama)\n",
    "metrics_llama = aggregate_metrics(result_df_llama)\n",
    "# Overwrite the average BLEU with a more precise computation, which value is maintained among 0 and 1\n",
    "metrics_llama['BLEU'] = bleu.compute(predictions=eval_df[model_response_df_column_evaluated], references=eval_df['answer'])['bleu']\n",
    "display(metrics_llama)\n",
    "evaluate_and_plot(eval_df, evaluator, model_response_df_column_evaluated, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = 'model_response_llama_instruct'\n",
    "title = 'Progression of Evaluation Metrics for Llama3.2 Instruct'\n",
    "result_df_llama_instruct = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(result_df_llama_instruct)\n",
    "metrics_llama_instruct = aggregate_metrics(result_df_llama_instruct)\n",
    "metrics_llama_instruct['BLEU'] = bleu.compute(predictions=eval_df[model_response_df_column_evaluated], references=eval_df['answer'])['bleu']\n",
    "display(metrics_llama_instruct)\n",
    "evaluate_and_plot(eval_df, evaluator, model_response_df_column_evaluated, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = 'model_response_gpt'\n",
    "title = 'Progression of Evaluation Metrics for GPT-4o'\n",
    "result_df_gpt = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(result_df_gpt)\n",
    "metrics_gpt = aggregate_metrics(result_df_gpt)\n",
    "metrics_gpt['BLEU'] = bleu.compute(predictions=eval_df[model_response_df_column_evaluated], references=eval_df['answer'])['bleu']\n",
    "display(metrics_gpt)\n",
    "evaluate_and_plot(eval_df, evaluator, model_response_df_column_evaluated, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = 'model_response_old_gpt'\n",
    "title = 'Progression of Evaluation Metrics for GPT-3.5-turbo'\n",
    "result_df_old_gpt = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(result_df_old_gpt)\n",
    "metrics_old_gpt = aggregate_metrics(result_df_old_gpt)\n",
    "metrics_old_gpt['BLEU'] = bleu.compute(predictions=eval_df[model_response_df_column_evaluated], references=eval_df['answer'])['bleu']\n",
    "display(metrics_old_gpt)\n",
    "evaluate_and_plot(eval_df, evaluator, model_response_df_column_evaluated, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean evaluation metrics of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and RAG types\n",
    "model_data = [\n",
    "    {'Model': 'llama3.2', 'RAG Type': 'Basic RAG', 'Question_rewriting' : False, 'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **metrics_llama},\n",
    "    {'Model': 'llama3.2_Instruct', 'RAG Type': 'Basic RAG', 'Question_rewriting' : False, 'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **metrics_llama_instruct},\n",
    "    {'Model': 'gpt-3.5-turbo', 'RAG Type': 'Basic RAG', 'Question_rewriting': False, 'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **metrics_old_gpt},\n",
    "    {'Model': 'gpt-4o', 'RAG Type': 'Basic RAG', 'Question_rewriting': False, 'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **metrics_gpt},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(model_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to apply the extraction function\n",
    "columns_to_extract = ['BLEU', 'ROUGE-1', 'BERT P', 'BERT R', 'BERT F1', 'Perplexity', 'Diversity']\n",
    "\n",
    "# Drop the unwanted column\n",
    "df_metrics = df_metrics.drop(columns='Racial Bias')\n",
    "\n",
    "# Ensure the 'BLEU' column is treated as strings\n",
    "for column in columns_to_extract:\n",
    "    df_metrics[column] = df_metrics[column].astype(str).str.split().str[1]\n",
    "\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze more the metrics and which makes sense to be tracked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "results = bleurt.compute(predictions=eval_df[\"model_response\"], references=eval_df[\"answer\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from statistics import mean\n",
    "\n",
    "candidates = eval_df[\"model_response\"]\n",
    "references = eval_df[\"answer\"]\n",
    "\n",
    "def evaluate_bert_score(candidates, references):\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    results = bertscore.compute(predictions=candidates, references=references, lang=\"it\")\n",
    "    P = results['precision']\n",
    "    R = results['recall']\n",
    "    F1 = results['f1']\n",
    "    P_mean, R_mean, F1_mean = mean(P), mean(R), mean(F1)\n",
    "    return P, R, F1, P_mean, R_mean, F1_mean\n",
    "\n",
    "evaluate_bert_score(candidates, references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(response_column):\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    results = perplexity.compute(model_id='gpt2',\n",
    "                             add_start_token=False,\n",
    "                             predictions=eval_df[\"model_response\"])\n",
    "    perplexities = results['perplexities']\n",
    "    mean_perplexity = results['mean_perplexity']\n",
    "    return perplexities, mean_perplexity\n",
    "\n",
    "evaluate_perplexity(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the BLEU score is correct, given that it should be from 0 to 1, but has higher values\n",
    "import evaluate\n",
    "\n",
    "def evaluate_bleu(candidates, references):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    return bleu.compute(predictions=candidates, references=references)\n",
    "# the precisions are the precision scores for each n-gram\n",
    "# bleu is the geometric mean of the precision scores\n",
    "\n",
    "evaluate_bleu(candidates, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_rouge(candidates, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [scorer.score(ref, cand) for ref, cand in zip(references, candidates)]\n",
    "    rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "    return rouge_scores, rouge1, rouge2, rougeL\n",
    "\n",
    "evaluate_rouge(candidates, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def evaluate_diversity(texts):\n",
    "    all_tokens = [tok for text in texts for tok in text.split()]\n",
    "    unique_bigrams = set(ngrams(all_tokens, 2))\n",
    "    diversity_score = len(unique_bigrams) / len(all_tokens) if all_tokens else 0\n",
    "    return diversity_score\n",
    "\n",
    "diversity = []\n",
    "for row in candidates:\n",
    "    diversity.append(evaluate_diversity(row))\n",
    "\n",
    "display(diversity)\n",
    "display(mean(diversity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Evaluator implementation, change it for our case\n",
    "import mauve\n",
    "import torch\n",
    "from statistics import mean\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mauve import compute_mauve\n",
    "import nltk\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self):\n",
    "        self.gpt2_model, self.gpt2_tokenizer = self.load_gpt2_model()\n",
    "\n",
    "    def load_gpt2_model(self):\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def evaluate_bert_score(self, candidates, references):\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        results = bertscore.compute(predictions=candidates, references=references, lang=\"it\")\n",
    "        P = float(results['precision'][0])\n",
    "        R = float(results['recall'][0])\n",
    "        F1 = float(results['f1'][0])\n",
    "        return P, R, F1\n",
    "\n",
    "    def evaluate_perplexity(self, text):\n",
    "        encodings = self.gpt2_tokenizer(text, return_tensors='pt')\n",
    "        max_length = self.gpt2_model.config.n_positions\n",
    "        stride = 512\n",
    "        lls = []\n",
    "        for i in range(0, encodings.input_ids.size(1), stride):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "            trg_len = end_loc - i\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = self.gpt2_model(input_ids, labels=target_ids)\n",
    "                log_likelihood = outputs[0] * trg_len\n",
    "            lls.append(log_likelihood)\n",
    "        ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "        return ppl.item()\n",
    "        \n",
    "    def evaluate_bleu(self, candidates, references):\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "        results = bleu.compute(predictions=candidates, references=references)\n",
    "        return results['bleu']\n",
    "    \n",
    "    def evaluate_rouge(self, candidates, references):\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [scorer.score(ref, cand) for ref, cand in zip(references, candidates)]\n",
    "        rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "        rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "        rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "        return rouge1, rouge2, rougeL\n",
    "\n",
    "    def evaluate_diversity(self, texts):\n",
    "        all_tokens = [tok for text in texts for tok in text.split()]\n",
    "        unique_bigrams = set(ngrams(all_tokens, 2))\n",
    "        diversity_score = len(unique_bigrams) / len(all_tokens) if all_tokens else 0\n",
    "        return diversity_score\n",
    "        \n",
    "    def evaluate_all(self, question, response, reference):\n",
    "        candidates = [response]\n",
    "        references = [reference]\n",
    "        bleu = self.evaluate_bleu(candidates, references)\n",
    "        rouge1, rouge2, rougeL = self.evaluate_rouge(candidates, references)\n",
    "        bert_p, bert_r, bert_f1 = self.evaluate_bert_score(candidates, references)\n",
    "        perplexity = self.evaluate_perplexity(response)\n",
    "        diversity = self.evaluate_diversity(candidates)\n",
    "        #mauve_score = self.evaluate_mauve(reference, response)\n",
    "        #meteor = self.evaluate_meteor(candidates, references)\n",
    "        #chrf = self.evaluate_chrf(candidates, references)\n",
    "        #flesch_ease, flesch_grade = self.evaluate_readability(response)\n",
    "        return {\n",
    "            \"BLEU\": bleu,\n",
    "            \"ROUGE-1\": rouge1,\n",
    "            \"ROUGE-2\": rouge2, \n",
    "            \"ROUGE-L\": rougeL,\n",
    "            \"BERT P\": bert_p,\n",
    "            \"BERT R\": bert_r,\n",
    "            \"BERT F1\": bert_f1,\n",
    "            \"Perplexity\": perplexity,\n",
    "            \"Diversity\": diversity,\n",
    "            #\"MAUVE\": mauve_score,\n",
    "            #\"METEOR\": meteor,\n",
    "            #\"CHRF\": chrf,\n",
    "            #\"Flesch Reading Ease\": flesch_ease,\n",
    "            #\"Flesch-Kincaid Grade\": flesch_grade,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_evaluator = RAGEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_library_results = evaluate_responses(eval_df, my_evaluator, model_response_df_column_evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_meteor(self, candidates, references):\n",
    "        nltk.download('punkt', quiet=True)  \n",
    "        \n",
    "        meteor_scores = [\n",
    "            meteor_score([word_tokenize(ref)], word_tokenize(cand))\n",
    "            for ref, cand in zip(references, candidates)\n",
    "        ]\n",
    "        return sum(meteor_scores) / len(meteor_scores)\n",
    "    \n",
    "def evaluate_chrf(self, candidates, references):\n",
    "        chrf_scores = [sentence_chrf(ref, cand) for ref, cand in zip(references, candidates)]\n",
    "        return sum(chrf_scores) / len(chrf_scores)\n",
    "    \n",
    "def evaluate_readability(self, text):\n",
    "        flesch_ease = flesch_reading_ease(text)\n",
    "        flesch_grade = flesch_kincaid_grade(text)\n",
    "        return flesch_ease, flesch_grade\n",
    "        \n",
    "def evaluate_mauve(self,reference_texts, generated_texts):\n",
    "        out = mauve.compute_mauve(\n",
    "                                  p_text=reference_texts,  # List of reference texts\n",
    "                                  q_text=generated_texts,  # List of generated texts\n",
    "                                  device_id=0,             # GPU device ID; set to -1 for CPU\n",
    "                                  max_text_length=1024,     # Maximum length of text to truncate\n",
    "                                  verbose=False            # Whether to print additional information\n",
    "                                )\n",
    "        return  out.mauve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "sentences = [\"limone\", \"arancia\", \"macchina\", \"soldi\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-m3')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p = 2, dim = 1)\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    for j in range(0, len(sentence_embeddings)):\n",
    "        print(\n",
    "            sentences[j],\n",
    "            sentences[i],\n",
    "            (sentence_embeddings[j] @ sentence_embeddings[i]).item()\n",
    "        )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"limone\", \"arancia\", \"macchina\", \"soldi\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p = 2, dim = 1)\n",
    "sentence_embeddings\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    for j in range(0, len(sentence_embeddings)):\n",
    "        print(\n",
    "            sentences[j],\n",
    "            sentences[i],\n",
    "            (sentence_embeddings[j] @ sentence_embeddings[i]).item()\n",
    "        )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"limone\", \"arancia\", \"macchina\", \"soldi\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p = 2, dim = 1)\n",
    "sentence_embeddings\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    for j in range(0, len(sentence_embeddings)):\n",
    "        print(\n",
    "            sentences[j],\n",
    "            sentences[i],\n",
    "            (sentence_embeddings[j] @ sentence_embeddings[i]).item()\n",
    "        )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [eval_df[\"answer\"][0], eval_df[\"model_response\"][0], eval_df[\"model_response_llama_instruct\"][0], eval_df[\"model_response_old_gpt\"][0], eval_df[\"model_response_gpt\"][0]]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-m3')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p = 2, dim = 1)\n",
    "sentence_embeddings\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    print(\n",
    "        (sentence_embeddings[0] @ sentence_embeddings[i]).item()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [eval_df[\"answer\"][0], eval_df[\"model_response\"][0], eval_df[\"model_response_llama_instruct\"][0], eval_df[\"model_response_old_gpt\"][0], eval_df[\"model_response_gpt\"][0]]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p = 2, dim = 1)\n",
    "sentence_embeddings\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    print(\n",
    "        (sentence_embeddings[0] @ sentence_embeddings[i]).item()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [eval_df[\"answer\"][0], eval_df[\"model_response\"][0], eval_df[\"model_response_llama_instruct\"][0], eval_df[\"model_response_old_gpt\"][0], eval_df[\"model_response_gpt\"][0]]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p = 2, dim = 1)\n",
    "sentence_embeddings\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    print(\n",
    "        (sentence_embeddings[0] @ sentence_embeddings[i]).item()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend SemScore to entire evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_sem_score(eval_df, model_name, response_keys):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between the reference answer and different model responses in eval_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - eval_df: DataFrame containing \"answer\" and model response columns\n",
    "    - model_name: name of the model to load from HuggingFace\n",
    "    - tokenizer_name: name of the tokenizer to load from HuggingFace\n",
    "    - response_keys: list of column names in eval_df for different model responses\n",
    "\n",
    "    Returns:\n",
    "    - cosine_similarities_df: DataFrame of individual cosine similarities for each example and each model response\n",
    "    - average_cosine_similarities_df: DataFrame of average cosine similarities for each model response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load specified model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Initialize a list to store cosine similarity results for each example\n",
    "    cosine_similarities_list = []\n",
    "\n",
    "    # Loop over all examples in eval_df\n",
    "    for idx in tqdm(range(len(eval_df)), desc=\"Processing examples\"):\n",
    "        sentences = [eval_df[\"answer\"][idx]] + [eval_df[key][idx] for key in response_keys]\n",
    "        \n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        # Perform mean pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Calculate cosine similarity for this example and store in a dictionary\n",
    "        cosine_scores = {\"example_index\": idx}\n",
    "        for i, key in enumerate(response_keys, start=1):\n",
    "            cosine_similarity = (sentence_embeddings[0] @ sentence_embeddings[i]).item()\n",
    "            cosine_scores[key] = cosine_similarity\n",
    "        \n",
    "        # Append the scores dictionary to the list\n",
    "        cosine_similarities_list.append(cosine_scores)\n",
    "\n",
    "    # Convert the list of cosine similarities into a DataFrame\n",
    "    cosine_similarities_df = pd.DataFrame(cosine_similarities_list)\n",
    "    cosine_similarities_df = cosine_similarities_df.drop(columns= \"example_index\")\n",
    "\n",
    "    # Calculate average similarity scores for each model response and store in a new DataFrame\n",
    "    average_cosine_similarities_df = pd.DataFrame(cosine_similarities_df[response_keys].mean()).T\n",
    "\n",
    "    # Return both the individual and average DataFrames\n",
    "    return cosine_similarities_df, average_cosine_similarities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "response_keys = [\"model_response\", \"model_response_llama_instruct\", \"model_response_old_gpt\", \"model_response_gpt\"]\n",
    "\n",
    "cosine_similarities_miniLM, average_cosine_similarities_miniLM = compute_sem_score(\n",
    "    eval_df, \n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    response_keys=response_keys\n",
    ")\n",
    "\n",
    "cosine_similarities_bge, average_cosine_similarities_bge = compute_sem_score(\n",
    "    eval_df, \n",
    "    model_name='BAAI/bge-m3',\n",
    "    response_keys=response_keys\n",
    ")\n",
    "\n",
    "cosine_similarities_Mpnet, average_cosine_similarities_Mpnet = compute_sem_score(\n",
    "    eval_df, \n",
    "    model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "    response_keys=response_keys\n",
    ")\n",
    "\n",
    "#Display the results\n",
    "print(\"\\n\")\n",
    "print(\"Model sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"Individual Cosine Similarities DataFrame:\")\n",
    "display(cosine_similarities_miniLM)\n",
    "print(\"\\nAverage Cosine Similarities DataFrame:\")\n",
    "display(average_cosine_similarities_miniLM)\n",
    "\n",
    "#Display the results\n",
    "print(\"\\n\")\n",
    "print(\"Model BAAI/bge-m3\")\n",
    "print(\"Individual Cosine Similarities DataFrame:\")\n",
    "display(cosine_similarities_bge)\n",
    "print(\"\\nAverage Cosine Similarities DataFrame:\")\n",
    "display(average_cosine_similarities_bge)\n",
    "\n",
    "#Display the results\n",
    "print(\"\\n\")\n",
    "print(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "print(\"Individual Cosine Similarities DataFrame:\")\n",
    "display(cosine_similarities_Mpnet)\n",
    "print(\"\\nAverage Cosine Similarities DataFrame:\")\n",
    "display(average_cosine_similarities_Mpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign model names to differentiate the averages in each DataFrame\n",
    "average_cosine_similarities_miniLM['model'] = 'MiniLM'\n",
    "average_cosine_similarities_bge['model'] = 'BGE'\n",
    "average_cosine_similarities_Mpnet['model'] = 'Mpnet'\n",
    "\n",
    "# Concatenate the two average DataFrames along the rows\n",
    "combined_averages_df = pd.concat([average_cosine_similarities_miniLM, average_cosine_similarities_bge, average_cosine_similarities_Mpnet], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined Average Cosine Similarities for Different Models, comparing embeddings for Italian SemScore:\")\n",
    "display(combined_averages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTScore Analysis compared to SemScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RAGEvaluator()\n",
    "model_response_df_column_evaluated = \"model_response_old_gpt\"\n",
    "my_library_results_old_gpt = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(my_library_results_old_gpt)\n",
    "my_metrics_old_gpt = aggregate_metrics(my_library_results_old_gpt)\n",
    "display(my_metrics_old_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = \"model_response_gpt\"\n",
    "my_library_results_gpt = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(my_library_results_gpt)\n",
    "my_metrics_gpt = aggregate_metrics(my_library_results_gpt)\n",
    "display(my_metrics_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = \"model_response\"\n",
    "my_library_results_llama = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(my_library_results_llama)\n",
    "my_metrics_llama = aggregate_metrics(my_library_results_llama)\n",
    "display(my_metrics_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_df_column_evaluated = \"model_response_llama_instruct\"\n",
    "my_library_results_llama_instruct = evaluate_responses(eval_df, evaluator, model_response_df_column_evaluated)\n",
    "display(my_library_results_llama_instruct)\n",
    "my_metrics_llama_instruct = aggregate_metrics(my_library_results_llama_instruct)\n",
    "display(my_metrics_llama_instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and RAG types\n",
    "model_data = [\n",
    "    {'Model': 'llama3.2', 'RAG Type': 'Basic RAG', 'Question_rewriting' : False, \n",
    "     'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **my_metrics_llama},\n",
    "    {'Model': 'llama3.2_Instruct', 'RAG Type': 'Basic RAG', 'Question_rewriting' : False, \n",
    "     'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **my_metrics_llama_instruct},\n",
    "    {'Model': 'gpt-3.5-turbo', 'RAG Type': 'Basic RAG', 'Question_rewriting': False, \n",
    "     'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **my_metrics_old_gpt},\n",
    "    {'Model': 'gpt-4o', 'RAG Type': 'Basic RAG', 'Question_rewriting': False, \n",
    "     'Retriver_k' : 10, 'Prompt_engineering': True, 'Prompt_type': 'Zero-shot', **my_metrics_gpt},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_my_metrics = pd.DataFrame(model_data)\n",
    "\n",
    "# Specify the columns to apply the extraction function\n",
    "columns_to_extract = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERT P', 'BERT R', 'BERT F1', 'Perplexity', 'Diversity']\n",
    "\n",
    "# Ensure the 'BLEU' column is treated as strings\n",
    "for column in columns_to_extract:\n",
    "    df_my_metrics[column] = df_my_metrics[column].astype(str).str.split().str[1]\n",
    "\n",
    "display(df_my_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_scores_for_model = combined_averages_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_scores_for_model.columns = sem_scores_for_model.loc['model']\n",
    "sem_scores_for_model = sem_scores_for_model[:-1]\n",
    "sem_scores_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_scores_for_model.reset_index(drop = True, inplace=True)\n",
    "sem_scores_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_metrics = pd.concat([df_my_metrics, sem_scores_for_model], axis = 1)\n",
    "concatenated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = concatenated_metrics.drop(columns=[\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERT P\", \"BERT R\"])\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TonicValidate model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.tonic_validate import (\n",
    "    TonicValidateEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating multiple questions\n",
    "questions = eval_df['question']\n",
    "\n",
    "retrieved_context_lists = []\n",
    "for i in range(0, len(eval_df)):\n",
    "    context = retriever.invoke(eval_df['question'][i])\n",
    "    single_context_list = []\n",
    "    for elem in context:\n",
    "        single_context_list.append(elem.page_content)\n",
    "\n",
    "    retrieved_context_lists.append(single_context_list)\n",
    "\n",
    "references = eval_df['answer']\n",
    "responses = eval_df['model_response']\n",
    "\n",
    "tonic_validate_evaluator = TonicValidateEvaluator()\n",
    "\n",
    "scores_llama = await tonic_validate_evaluator.aevaluate_run(\n",
    "    questions, references, retrieved_context_lists, responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating multiple questions\n",
    "questions = eval_df['question']\n",
    "\n",
    "retrieved_context_lists = []\n",
    "for i in range(0, len(eval_df)):\n",
    "    context = retriever.invoke(eval_df['question'][i])\n",
    "    single_context_list = []\n",
    "    for elem in context:\n",
    "        single_context_list.append(elem.page_content)\n",
    "\n",
    "    retrieved_context_lists.append(single_context_list)\n",
    "\n",
    "references = eval_df['answer']\n",
    "responses = eval_df['model_response_gpt']\n",
    "\n",
    "tonic_validate_evaluator = TonicValidateEvaluator()\n",
    "\n",
    "scores_gpt = await tonic_validate_evaluator.aevaluate_run(\n",
    "    questions, references, retrieved_context_lists, responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating multiple questions\n",
    "questions = eval_df['question']\n",
    "\n",
    "retrieved_context_lists = []\n",
    "for i in range(0, len(eval_df)):\n",
    "    context = retriever.invoke(eval_df['question'][i])\n",
    "    single_context_list = []\n",
    "    for elem in context:\n",
    "        single_context_list.append(elem.page_content)\n",
    "\n",
    "    retrieved_context_lists.append(single_context_list)\n",
    "\n",
    "references = eval_df['answer']\n",
    "responses = eval_df['model_response_llama_instruct']\n",
    "\n",
    "tonic_validate_evaluator = TonicValidateEvaluator()\n",
    "\n",
    "scores_llama_instruct = await tonic_validate_evaluator.aevaluate_run(\n",
    "    questions, references, retrieved_context_lists, responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating multiple questions\n",
    "questions = eval_df['question']\n",
    "\n",
    "retrieved_context_lists = []\n",
    "for i in range(0, len(eval_df)):\n",
    "    context = retriever.invoke(eval_df['question'][i])\n",
    "    single_context_list = []\n",
    "    for elem in context:\n",
    "        single_context_list.append(elem.page_content)\n",
    "\n",
    "    retrieved_context_lists.append(single_context_list)\n",
    "\n",
    "references = eval_df['answer']\n",
    "responses = eval_df['model_response_old_gpt']\n",
    "\n",
    "tonic_validate_evaluator = TonicValidateEvaluator()\n",
    "\n",
    "scores_old_gpt = await tonic_validate_evaluator.aevaluate_run(\n",
    "    questions, references, retrieved_context_lists, responses\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
