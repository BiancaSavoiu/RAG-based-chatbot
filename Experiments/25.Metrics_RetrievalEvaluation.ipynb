{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "csv_input_path = os.path.dirname(path) + \"/Doc_Panthera_Augmented/augmented_dataset_final_outputs.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_input_path, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the contents\n",
    "display(df)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "docs_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Data_preprocessing\n",
    "importlib.reload(Data_preprocessing)\n",
    "\n",
    "# Initialize the Preprocessing object\n",
    "preprocessing = Data_preprocessing.Preprocessing()\n",
    "\n",
    "for doc in docs_data:\n",
    "    doc.page_content = preprocessing.clean_text_template(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = []\n",
    "chunk_number = 1  # Initialize chunk number\n",
    "\n",
    "for doc in docs_data:\n",
    "    # Split the document into chunks\n",
    "    split_docs = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Add chunk number as metadata to each split document\n",
    "    for split_doc in split_docs:\n",
    "        split_doc.metadata['chunk_number'] = chunk_number\n",
    "        docs_processed.append(split_doc)\n",
    "        chunk_number += 1  # Increment chunk number for the next chunk\n",
    "\n",
    "# Print the first 6 processed documents and their count\n",
    "pprint.pprint(docs_processed[0:6])\n",
    "print(len(docs_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "    \n",
    "embd = M3EmbeddingFP16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore = FAISS.from_documents(documents=docs_processed, embedding=embd)\n",
    "#vectorstore.save_local(\"cleaned_whitespaces_recursive_augmented_faiss_index_1500_with_chunk_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the documents without any data preprocessing steps\n",
    "#vectorstore = FAISS.load_local(\"cleaned_recursive_augmented_faiss_index_with_chunk_number\", embd, allow_dangerous_deserialization=True)\n",
    "vectorstore = FAISS.load_local(\"cleaned_whitespaces_recursive_augmented_faiss_index_1500_with_chunk_number\", embd, allow_dangerous_deserialization=True)\n",
    "vectorstore, vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved CSV file\n",
    "eval_df = pd.read_csv('filtered_matching_questions.csv')\n",
    "\n",
    "# Display the first few rows of the loaded DataFrame\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the ground truth and retrieved chunks dictionary to evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run, evaluate, compare\n",
    "\n",
    "qrels = {}  # Ground truth: {query_id: {chunk_id: relevance}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in eval_df.iterrows():\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    \n",
    "    # Ground Truth: Extract relevant chunk numbers\n",
    "    ground_truth_chunks = list(map(str, row['chunk_num'].strip('[]').split()))\n",
    "    qrels[query_id] = {chunk: 1 for chunk in ground_truth_chunks}  # Mark all chunks as relevant\n",
    "\n",
    "qrels_eval = Qrels(qrels)\n",
    "pprint.pprint(qrels_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run, evaluate, compare\n",
    "\n",
    "# Create qrels (ground truth relevance judgments)\n",
    "qrels = {}\n",
    "for idx, row in eval_df.iterrows():\n",
    "    query_id = f\"q{idx}\"\n",
    "    qrels[query_id] = {row['context']: 1}  # Ground truth relevance is 1\n",
    "\n",
    "# Convert qrels and run to ranx objects\n",
    "#qrels_eval = Qrels(qrels)  # Ground truth relevance set\n",
    "pprint.pprint(qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1 - sparse retriever BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, evaluate, compare\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "retriever = BM25Retriever.from_documents(docs_processed)\n",
    "retriever.k =  4 \n",
    "\n",
    "run = {}    # Retrieved chunks: {query_id: {chunk_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in eval_df.iterrows():\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    \n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    question = row['question']\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)  # Replace with actual retriever call\n",
    "    \n",
    "    # Prepare Run: Extract retrieved chunk numbers and scores\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        chunk_id = str(doc.metadata.get('chunk_number'))  # Extract chunk_number from metadata\n",
    "        score = doc.metadata.get('score', 1.0)  # Use retrieval score or default to 1.0\n",
    "        retrieved_chunks_scores[chunk_id] = score\n",
    "    \n",
    "    run[query_id] = retrieved_chunks_scores  # Store retrieved chunks and scores for this query\n",
    "\n",
    "run1 = Run(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2 - dense retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, evaluate, compare\n",
    "\n",
    "# Initialize the dense retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "run = {}    # Retrieved chunks: {query_id: {chunk_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in eval_df.iterrows():\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    \n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    question = row['question']\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)  # Replace with actual retriever call\n",
    "    \n",
    "    # Prepare Run: Extract retrieved chunk numbers and scores\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        chunk_id = str(doc.metadata.get('chunk_number'))  # Extract chunk_number from metadata\n",
    "        score = doc.metadata.get('score', 1.0)  # Use retrieval score or default to 1.0\n",
    "        retrieved_chunks_scores[chunk_id] = score\n",
    "    \n",
    "    run[query_id] = retrieved_chunks_scores  # Store retrieved chunks and scores for this query\n",
    "\n",
    "run2 = Run(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3 - hybrid retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, evaluate, compare\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Initialize the dense retriever\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_processed)\n",
    "bm25_retriever.k =  3 \n",
    "\n",
    "# Initialize the ensemble retriever\n",
    "retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.4, 0.6])\n",
    "\n",
    "run = {}    # Retrieved chunks: {query_id: {chunk_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in eval_df.iterrows():\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    \n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    question = row['question']\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)  # Replace with actual retriever call\n",
    "    \n",
    "    # Prepare Run: Extract retrieved chunk numbers and scores\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        chunk_id = str(doc.metadata.get('chunk_number'))  # Extract chunk_number from metadata\n",
    "        score = doc.metadata.get('score', 1.0)  # Use retrieval score or default to 1.0\n",
    "        retrieved_chunks_scores[chunk_id] = score\n",
    "    \n",
    "    run[query_id] = retrieved_chunks_scores  # Store retrieved chunks and scores for this query\n",
    "\n",
    "run3 = Run(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant not having similar chunking - comparing semantic similarity for extracted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to store results for different retrieval models\n",
    "retrieval_results = pd.DataFrame(columns=[\n",
    "    \"Model\", \"Hit Rate\", \"Precision\", \"Recall\", \"F1-score\", \"MAP\", \"MRR\", \"NDCG\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, evaluate, compare\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tqdm \n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "retriever = BM25Retriever.from_documents(docs_processed)\n",
    "retriever.k = 4\n",
    "\n",
    "# Initialize a semantic similarity model\n",
    "similarity_model = SentenceTransformer('BAAI/bge-m3')  # Use a suitable pre-trained model\n",
    "\n",
    "# Semantic similarity threshold\n",
    "similarity_threshold = 0.7  # Adjust based on experimentation\n",
    "\n",
    "run_sparse = {}  # Retrieved contexts: {query_id: {retrieved_context_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in tqdm.tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Processing queries\"):\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    question = row['question']\n",
    "    eval_context = row['context']  # The ground truth context from the evaluation dataframe\n",
    "\n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Prepare Run: Compute semantic similarity and determine matches\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        retrieved_context = doc.page_content  # Extract document text\n",
    "        score = util.pytorch_cos_sim(\n",
    "            similarity_model.encode(eval_context, convert_to_tensor=True),\n",
    "            similarity_model.encode(retrieved_context, convert_to_tensor=True)\n",
    "        ).item()  # Compute semantic similarity score\n",
    "\n",
    "        if score >= similarity_threshold:\n",
    "            # If similarity surpasses threshold, assign a score of 1 (match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 1\n",
    "        else:\n",
    "            # Otherwise, assign a score of 0 (no match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 0\n",
    "\n",
    "    run_sparse[query_id] = retrieved_chunks_scores  # Store retrieved contexts and scores for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Filepath to save the CSV\n",
    "file_path = 'run_sparse_data.csv'\n",
    "\n",
    "# Define the explicit list of contexts\n",
    "header_contexts = ['context1', 'context2', 'context3', 'context4']\n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    header = ['Question'] + header_contexts\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write each row\n",
    "    for question, contexts in run_sparse.items():\n",
    "        # Start the row with the question\n",
    "        row = [question]\n",
    "        for content, score in contexts.items():\n",
    "            row.append(score)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "        \n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV into a Pandas DataFrame\n",
    "file_path = 'run_sparse_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert columns to integers\n",
    "df[['context1', 'context2', 'context3', 'context4']] = df[['context1', 'context2', 'context3', 'context4']].astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Extract relevance values\n",
    "binary_relevance = df.iloc[:, 1:].values  # Extract only 0/1 relevance values\n",
    "\n",
    "### HIT RATE (HR) ###\n",
    "hit_rate = np.mean(np.any(binary_relevance, axis=1))\n",
    "\n",
    "### PRECISION, RECALL, AND F1 ###\n",
    "precision_per_q = [precision_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "recall_per_q = [recall_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "f1_per_q = [f1_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "\n",
    "precision_avg = np.mean(precision_per_q)\n",
    "recall_avg = np.mean(recall_per_q)\n",
    "f1_avg = np.mean(f1_per_q)\n",
    "\n",
    "### MEAN AVERAGE PRECISION (MAP) ###\n",
    "def average_precision(row):\n",
    "    relevant = np.sum(row)\n",
    "    if relevant == 0:\n",
    "        return 0\n",
    "    precisions = [np.sum(row[:i+1]) / (i+1) for i in range(len(row)) if row[i] == 1]\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "map_score = np.mean([average_precision(row) for row in binary_relevance])\n",
    "\n",
    "### MEAN RECIPROCAL RANK (MRR) ###\n",
    "def reciprocal_rank(row):\n",
    "    for i, val in enumerate(row):\n",
    "        if val == 1:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "mrr_score = np.mean([reciprocal_rank(row) for row in binary_relevance])\n",
    "\n",
    "### NORMALIZED DISCOUNTED CUMULATIVE GAIN (NDCG) ###\n",
    "def dcg(row):\n",
    "    return np.sum(row / np.log2(np.arange(2, len(row) + 2)))  # DCG formula\n",
    "\n",
    "def ndcg(row):\n",
    "    ideal = np.sort(row)[::-1]  # Ideal ranking\n",
    "    ideal_dcg = dcg(ideal) if np.sum(ideal) > 0 else 1  # Avoid division by zero\n",
    "    return dcg(row) / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "ndcg_score = np.mean([ndcg(row) for row in binary_relevance])\n",
    "\n",
    "# Add results for Hybrid Retriever\n",
    "retrieval_results = pd.concat([retrieval_results, pd.DataFrame([{\n",
    "    \"Model\": \"Sparse Retriever\",\n",
    "    \"Hit Rate\": hit_rate,\n",
    "    \"Precision\": precision_avg,\n",
    "    \"Recall\": recall_avg,\n",
    "    \"F1-score\": f1_avg,\n",
    "    \"MAP\": map_score,\n",
    "    \"MRR\": mrr_score,\n",
    "    \"NDCG\": ndcg_score\n",
    "}])], ignore_index=True)\n",
    "\n",
    "display(retrieval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, evaluate, compare\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tqdm \n",
    "\n",
    "# Initialize the dense retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Initialize a semantic similarity model\n",
    "similarity_model = SentenceTransformer('BAAI/bge-m3')  # Use a suitable pre-trained model\n",
    "\n",
    "# Semantic similarity threshold\n",
    "similarity_threshold = 0.7  # Adjust based on experimentation\n",
    "\n",
    "run_dense = {}  # Retrieved contexts: {query_id: {retrieved_context_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in tqdm.tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Processing queries\"):\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    question = row['question']\n",
    "    eval_context = row['context']  # The ground truth context from the evaluation dataframe\n",
    "\n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Prepare Run: Compute semantic similarity and determine matches\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        retrieved_context = doc.page_content  # Extract document text\n",
    "        score = util.pytorch_cos_sim(\n",
    "            similarity_model.encode(eval_context, convert_to_tensor=True),\n",
    "            similarity_model.encode(retrieved_context, convert_to_tensor=True)\n",
    "        ).item()  # Compute semantic similarity score\n",
    "\n",
    "        if score >= similarity_threshold:\n",
    "            # If similarity surpasses threshold, assign a score of 1 (match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 1\n",
    "        else:\n",
    "            # Otherwise, assign a score of 0 (no match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 0\n",
    "\n",
    "    run_dense[query_id] = retrieved_chunks_scores  # Store retrieved contexts and scores for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Filepath to save the CSV\n",
    "file_path = 'run_dense_data.csv'\n",
    "\n",
    "# Define the explicit list of contexts\n",
    "header_contexts = ['context1', 'context2', 'context3', 'context4']\n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    header = ['Question'] + header_contexts\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write each row\n",
    "    for question, contexts in run_dense.items():\n",
    "        # Start the row with the question\n",
    "        row = [question]\n",
    "        for content, score in contexts.items():\n",
    "            row.append(score)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "        \n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV into a Pandas DataFrame\n",
    "file_path = 'run_dense_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert columns to integers\n",
    "df[['context1', 'context2', 'context3', 'context4']] = df[['context1', 'context2', 'context3', 'context4']].astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Extract relevance values\n",
    "binary_relevance = df.iloc[:, 1:].values  # Extract only 0/1 relevance values\n",
    "\n",
    "### HIT RATE (HR) ###\n",
    "hit_rate = np.mean(np.any(binary_relevance, axis=1))\n",
    "\n",
    "### PRECISION, RECALL, AND F1 ###\n",
    "precision_per_q = [precision_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "recall_per_q = [recall_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "f1_per_q = [f1_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "\n",
    "precision_avg = np.mean(precision_per_q)\n",
    "recall_avg = np.mean(recall_per_q)\n",
    "f1_avg = np.mean(f1_per_q)\n",
    "\n",
    "### MEAN AVERAGE PRECISION (MAP) ###\n",
    "def average_precision(row):\n",
    "    relevant = np.sum(row)\n",
    "    if relevant == 0:\n",
    "        return 0\n",
    "    precisions = [np.sum(row[:i+1]) / (i+1) for i in range(len(row)) if row[i] == 1]\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "map_score = np.mean([average_precision(row) for row in binary_relevance])\n",
    "\n",
    "### MEAN RECIPROCAL RANK (MRR) ###\n",
    "def reciprocal_rank(row):\n",
    "    for i, val in enumerate(row):\n",
    "        if val == 1:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "mrr_score = np.mean([reciprocal_rank(row) for row in binary_relevance])\n",
    "\n",
    "### NORMALIZED DISCOUNTED CUMULATIVE GAIN (NDCG) ###\n",
    "def dcg(row):\n",
    "    return np.sum(row / np.log2(np.arange(2, len(row) + 2)))  # DCG formula\n",
    "\n",
    "def ndcg(row):\n",
    "    ideal = np.sort(row)[::-1]  # Ideal ranking\n",
    "    ideal_dcg = dcg(ideal) if np.sum(ideal) > 0 else 1  # Avoid division by zero\n",
    "    return dcg(row) / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "ndcg_score = np.mean([ndcg(row) for row in binary_relevance])\n",
    "\n",
    "# Add results for Hybrid Retriever\n",
    "retrieval_results = pd.concat([retrieval_results, pd.DataFrame([{\n",
    "    \"Model\": \"Dense Retriever\",\n",
    "    \"Hit Rate\": hit_rate,\n",
    "    \"Precision\": precision_avg,\n",
    "    \"Recall\": recall_avg,\n",
    "    \"F1-score\": f1_avg,\n",
    "    \"MAP\": map_score,\n",
    "    \"MRR\": mrr_score,\n",
    "    \"NDCG\": ndcg_score\n",
    "}])], ignore_index=True)\n",
    "\n",
    "display(retrieval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, evaluate, compare\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tqdm \n",
    "\n",
    "# Initialize the dense retriever\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_processed)\n",
    "bm25_retriever.k = 4\n",
    "\n",
    "# Initialize the ensemble retriever\n",
    "retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.4, 0.6])\n",
    "\n",
    "# Initialize a semantic similarity model\n",
    "similarity_model = SentenceTransformer('BAAI/bge-m3')  # Use a suitable pre-trained model\n",
    "\n",
    "# Semantic similarity threshold\n",
    "similarity_threshold = 0.7  # Adjust based on experimentation\n",
    "\n",
    "run = {}  # Retrieved contexts: {query_id: {retrieved_context_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in tqdm.tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Processing queries\"):\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    question = row['question']\n",
    "    eval_context = row['context']  # The ground truth context from the evaluation dataframe\n",
    "\n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Prepare Run: Compute semantic similarity and determine matches\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        retrieved_context = doc.page_content  # Extract document text\n",
    "        score = util.pytorch_cos_sim(\n",
    "            similarity_model.encode(eval_context, convert_to_tensor=True),\n",
    "            similarity_model.encode(retrieved_context, convert_to_tensor=True)\n",
    "        ).item()  # Compute semantic similarity score\n",
    "\n",
    "        if score >= similarity_threshold:\n",
    "            # If similarity surpasses threshold, assign a score of 1 (match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 1\n",
    "        else:\n",
    "            # Otherwise, assign a score of 0 (no match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 0\n",
    "\n",
    "    run[query_id] = retrieved_chunks_scores  # Store retrieved contexts and scores for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Filepath to save the CSV\n",
    "file_path = 'run_hybrid_data.csv'\n",
    "\n",
    "# Define the explicit list of contexts\n",
    "header_contexts = ['context1', 'context2', 'context3', 'context4', 'context5', 'context6']\n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    header = ['Question'] + header_contexts\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write each row\n",
    "    for question, contexts in run.items():\n",
    "        # Start the row with the question\n",
    "        row = [question]\n",
    "        for content, score in contexts.items():\n",
    "            row.append(score)\n",
    "        \n",
    "        writer.writerow(row[:7])\n",
    "        \n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV into a Pandas DataFrame\n",
    "file_path = 'run_hybrid_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert columns to integers\n",
    "df[['context1', 'context2', 'context3', 'context4', 'context5', 'context6']] = df[['context1', 'context2', 'context3', 'context4', 'context5', 'context6']].astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Extract relevance values\n",
    "binary_relevance = df.iloc[:, 1:].values  # Extract only 0/1 relevance values\n",
    "\n",
    "### HIT RATE (HR) ###\n",
    "hit_rate = np.mean(np.any(binary_relevance, axis=1))\n",
    "\n",
    "### PRECISION, RECALL, AND F1 ###\n",
    "precision_per_q = [precision_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "recall_per_q = [recall_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "f1_per_q = [f1_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "\n",
    "precision_avg = np.mean(precision_per_q)\n",
    "recall_avg = np.mean(recall_per_q)\n",
    "f1_avg = np.mean(f1_per_q)\n",
    "\n",
    "### MEAN AVERAGE PRECISION (MAP) ###\n",
    "def average_precision(row):\n",
    "    relevant = np.sum(row)\n",
    "    if relevant == 0:\n",
    "        return 0\n",
    "    precisions = [np.sum(row[:i+1]) / (i+1) for i in range(len(row)) if row[i] == 1]\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "map_score = np.mean([average_precision(row) for row in binary_relevance])\n",
    "\n",
    "### MEAN RECIPROCAL RANK (MRR) ###\n",
    "def reciprocal_rank(row):\n",
    "    for i, val in enumerate(row):\n",
    "        if val == 1:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "mrr_score = np.mean([reciprocal_rank(row) for row in binary_relevance])\n",
    "\n",
    "### NORMALIZED DISCOUNTED CUMULATIVE GAIN (NDCG) ###\n",
    "def dcg(row):\n",
    "    return np.sum(row / np.log2(np.arange(2, len(row) + 2)))  # DCG formula\n",
    "\n",
    "def ndcg(row):\n",
    "    ideal = np.sort(row)[::-1]  # Ideal ranking\n",
    "    ideal_dcg = dcg(ideal) if np.sum(ideal) > 0 else 1  # Avoid division by zero\n",
    "    return dcg(row) / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "ndcg_score = np.mean([ndcg(row) for row in binary_relevance])\n",
    "\n",
    "# Add results for Hybrid Retriever\n",
    "retrieval_results = pd.concat([retrieval_results, pd.DataFrame([{\n",
    "    \"Model\": \"Hybrid Retriever\",\n",
    "    \"Hit Rate\": hit_rate,\n",
    "    \"Precision\": precision_avg,\n",
    "    \"Recall\": recall_avg,\n",
    "    \"F1-score\": f1_avg,\n",
    "    \"MAP\": map_score,\n",
    "    \"MRR\": mrr_score,\n",
    "    \"NDCG\": ndcg_score\n",
    "}])], ignore_index=True)\n",
    "\n",
    "display(retrieval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use created retrieval evaluation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "#from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from retrieval_evaluation import RetrievalEvaluator\n",
    "\n",
    "# Initialize the dense retriever\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_processed)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "# Initialize the ensemble retriever\n",
    "retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.2, 0.8])\n",
    "\n",
    "# Instantiate the evaluator\n",
    "evaluator = RetrievalEvaluator(retriever=retriever, similarity_model=SentenceTransformer('BAAI/bge-m3'))\n",
    "\n",
    "evaluator.evaluate(eval_df)  # Pass the evaluation dataframe (eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save_to_csv('run_hybrid2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"run_hybrid2_data.csv\"\n",
    "model_name = \"Hybrid retriever 2\"\n",
    "results = evaluator.calculate_metrics(file_path=file_path, model_name=model_name)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top hybrid configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dense retriever\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_processed)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "# Initialize the ensemble retriever\n",
    "retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.2, 0.8])\n",
    "\n",
    "# Initialize a semantic similarity model\n",
    "similarity_model = SentenceTransformer('BAAI/bge-m3')  # Use a suitable pre-trained model\n",
    "\n",
    "# Semantic similarity threshold\n",
    "similarity_threshold = 0.7  # Adjust based on experimentation\n",
    "\n",
    "run_hybrid_top = {}  # Retrieved contexts: {query_id: {retrieved_context_id: score}}\n",
    "\n",
    "# Iterate through each question in the evaluation dataframe\n",
    "for idx, row in tqdm.tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Processing queries\"):\n",
    "    query_id = f\"q{idx}\"  # Query IDs: q0, q1, etc.\n",
    "    question = row['question']\n",
    "    eval_context = row['context']  # The ground truth context from the evaluation dataframe\n",
    "\n",
    "    # Retrieve relevant documents using EnsembleRetriever\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Prepare Run: Compute semantic similarity and determine matches\n",
    "    retrieved_chunks_scores = {}\n",
    "    for doc in retrieved_docs:\n",
    "        retrieved_context = doc.page_content  # Extract document text\n",
    "        score = util.pytorch_cos_sim(\n",
    "            similarity_model.encode(eval_context, convert_to_tensor=True),\n",
    "            similarity_model.encode(retrieved_context, convert_to_tensor=True)\n",
    "        ).item()  # Compute semantic similarity score\n",
    "\n",
    "        if score >= similarity_threshold:\n",
    "            # If similarity surpasses threshold, assign a score of 1 (match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 1\n",
    "        else:\n",
    "            # Otherwise, assign a score of 0 (no match)\n",
    "            retrieved_chunks_scores[retrieved_context] = 0\n",
    "\n",
    "    run_hybrid_top[query_id] = retrieved_chunks_scores  # Store retrieved contexts and scores for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Filepath to save the CSV\n",
    "file_path = 'run_hybrid2_data.csv'\n",
    "\n",
    "# Define the explicit list of contexts\n",
    "header_contexts = ['context1', 'context2', 'context3', 'context4']\n",
    "\n",
    "# Write to CSV\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    header = ['Question'] + header_contexts\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write each row\n",
    "    for question, contexts in run_hybrid_top.items():\n",
    "        # Start the row with the question\n",
    "        row = [question]\n",
    "        for content, score in contexts.items():\n",
    "            row.append(score)\n",
    "        \n",
    "        writer.writerow(row[:5])\n",
    "        \n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV into a Pandas DataFrame\n",
    "file_path = 'run_hybrid2_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert columns to integers\n",
    "df[['context1', 'context2', 'context3', 'context4']] = df[['context1', 'context2', 'context3', 'context4']].astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Extract relevance values\n",
    "binary_relevance = df.iloc[:, 1:].values  # Extract only 0/1 relevance values\n",
    "\n",
    "### HIT RATE (HR) ###\n",
    "hit_rate = np.mean(np.any(binary_relevance, axis=1))\n",
    "\n",
    "### PRECISION, RECALL, AND F1 ###\n",
    "precision_per_q = [precision_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "recall_per_q = [recall_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "f1_per_q = [f1_score(row, np.ones_like(row), zero_division=0) for row in binary_relevance]\n",
    "\n",
    "precision_avg = np.mean(precision_per_q)\n",
    "recall_avg = np.mean(recall_per_q)\n",
    "f1_avg = np.mean(f1_per_q)\n",
    "\n",
    "### MEAN AVERAGE PRECISION (MAP) ###\n",
    "def average_precision(row):\n",
    "    relevant = np.sum(row)\n",
    "    if relevant == 0:\n",
    "        return 0\n",
    "    precisions = [np.sum(row[:i+1]) / (i+1) for i in range(len(row)) if row[i] == 1]\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "map_score = np.mean([average_precision(row) for row in binary_relevance])\n",
    "\n",
    "### MEAN RECIPROCAL RANK (MRR) ###\n",
    "def reciprocal_rank(row):\n",
    "    for i, val in enumerate(row):\n",
    "        if val == 1:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "mrr_score = np.mean([reciprocal_rank(row) for row in binary_relevance])\n",
    "\n",
    "### NORMALIZED DISCOUNTED CUMULATIVE GAIN (NDCG) ###\n",
    "def dcg(row):\n",
    "    return np.sum(row / np.log2(np.arange(2, len(row) + 2)))  # DCG formula\n",
    "\n",
    "def ndcg(row):\n",
    "    ideal = np.sort(row)[::-1]  # Ideal ranking\n",
    "    ideal_dcg = dcg(ideal) if np.sum(ideal) > 0 else 1  # Avoid division by zero\n",
    "    return dcg(row) / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "ndcg_score = np.mean([ndcg(row) for row in binary_relevance])\n",
    "\n",
    "# Add results for Hybrid Retriever\n",
    "retrieval_results = pd.concat([retrieval_results, pd.DataFrame([{\n",
    "    \"Model\": \"Hybrid2 Retriever\",\n",
    "    \"Hit Rate\": hit_rate,\n",
    "    \"Precision\": precision_avg,\n",
    "    \"Recall\": recall_avg,\n",
    "    \"F1-score\": f1_avg,\n",
    "    \"MAP\": map_score,\n",
    "    \"MRR\": mrr_score,\n",
    "    \"NDCG\": ndcg_score\n",
    "}])], ignore_index=True)\n",
    "\n",
    "display(retrieval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
