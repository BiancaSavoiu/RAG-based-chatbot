{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import pprint\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "    \n",
    "embd = M3EmbeddingFP16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM - the used model\n",
    "generative_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "csv_input_path = os.path.dirname(path) + \"/Doc_Panthera_Augmented/augmented_dataset_final_outputs.csv\"\n",
    "csv_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_input_path, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the contents\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "docs_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Data_preprocessing\n",
    "importlib.reload(Data_preprocessing)\n",
    "\n",
    "# Initialize the Preprocessing object\n",
    "preprocessing = Data_preprocessing.Preprocessing()\n",
    "\n",
    "# Iterate through each document in docs_data and clean the text\n",
    "for doc in docs_data:\n",
    "    cleaned_content = preprocessing.clean_text_template(doc.page_content)\n",
    "    doc.page_content = cleaned_content\n",
    "\n",
    "pprint.pprint(docs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to chunk a document semantically based on cosine distances\n",
    "def chunk_document(doc, embd, buffer_size=1, breakpoint_percentile_threshold=95, max_seq_length=8000):\n",
    "    # Split text into sentences\n",
    "    pprint.pprint(doc.metadata)\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', doc.page_content)\n",
    "    print(f\"{len(single_sentences_list)} sentences were found\")\n",
    "\n",
    "    # Create sentence dictionaries\n",
    "    sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "    \n",
    "    # Combine sentences with buffer (e.g., 1 sentence buffer to smooth chunks)\n",
    "    sentences = combine_sentences(sentences, buffer_size=buffer_size)\n",
    "    \n",
    "    # Truncate combined sentences if they exceed the maximum sequence length\n",
    "    for sentence in sentences:\n",
    "        if len(sentence['combined_sentence']) > max_seq_length:\n",
    "            sentence['combined_sentence'] = sentence['combined_sentence'][:max_seq_length]\n",
    "    \n",
    "    # Embedding sentences\n",
    "    embeddings = embd([x['combined_sentence'] for x in sentences])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "\n",
    "    # Calculate cosine distances\n",
    "    distances = calculate_cosine_distances(sentences)\n",
    "\n",
    "    # Set distance threshold for breakpoints\n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "\n",
    "    # Get indices of breakpoints\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "\n",
    "    # Create chunks based on breakpoints\n",
    "    start_index = 0\n",
    "    chunks = []\n",
    "    for index in indices_above_thresh:\n",
    "        end_index = index\n",
    "        group = sentences[start_index:end_index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        start_index = index + 1\n",
    "\n",
    "    # Handle last chunk if any sentences remain\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Assuming docs_data is a list of Document objects\n",
    "all_chunks = []\n",
    "\n",
    "for doc in docs_data:\n",
    "    # Chunk each document semantically\n",
    "    chunks = chunk_document(doc, embd, buffer_size=1, breakpoint_percentile_threshold=95)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# Create a DataFrame with the chunks\n",
    "df = pd.DataFrame(all_chunks, columns=['chunk'])\n",
    "display(df[:3])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = \"semantic_chunks_augmented_dataset.csv\"\n",
    "df.to_csv(csv_file_path, sep=\"|\", index=False)\n",
    "\n",
    "# Load the data from the CSV file using the CSVLoader\n",
    "loader = CSVLoader(file_path=csv_file_path)\n",
    "data = loader.load()\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorstore = FAISS.load_local(\"augmented_faiss_index\", embd, allow_dangerous_deserialization=True)\n",
    "new_vectorstore,  new_vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Index\n",
    "# Data driven changes - change 'k' the number of retrieved documents given the query\n",
    "retriever = new_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "question = \"Quando mi conviene gestire un articolo a PSO rispetto a pianificazione?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
