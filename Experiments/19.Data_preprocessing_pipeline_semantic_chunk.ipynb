{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset for RAG\n",
    "import pandas as pd\n",
    "\n",
    "csv_name = \"Gemini_augmented_dataset.csv\"\n",
    "data_df = pd.read_csv(csv_name, names = ['file_name', 'updated_text'], delimiter=\"|\")\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import pprint\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"updated_text\")\n",
    "docs_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = docs_data[0].page_content\n",
    "pprint.pprint(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embd = M3EmbeddingFP16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(splits):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# LLM - the used model\n",
    "generative_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_description = \"No preprocessing\"\n",
    "chunking_description = \"Semantic chunking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Data_preprocessing\n",
    "importlib.reload(Data_preprocessing)\n",
    "\n",
    "# Initialize the Preprocessing object\n",
    "preprocessing = Data_preprocessing.Preprocessing()\n",
    "\n",
    "# Clean the essay text using the clean_text_template method\n",
    "cleaned_essay = preprocessing.clean_text_template(essay)\n",
    "\n",
    "# Print the cleaned text\n",
    "pprint.pprint(cleaned_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Splitting the text on '.', '?', and '!'\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', cleaned_essay)\n",
    "print (f\"{len(single_sentences_list)} senteneces were found\")\n",
    "\n",
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "sentences = combine_sentences(sentences, buffer_size=1)\n",
    "embeddings = embd([x['combined_sentence'] for x in sentences])\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "\n",
    "pprint.pprint(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "distances = calculate_cosine_distances(sentences)\n",
    "\n",
    "plt.plot(distances)\n",
    "\n",
    "plt.xlim(0, len(distances))\n",
    "\n",
    "# We need to get the distance threshold that we'll consider an outlier\n",
    "# We'll use numpy .percentile() for this\n",
    "breakpoint_percentile_threshold = 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');\n",
    "\n",
    "# Then we'll see how many distances are actually above this one\n",
    "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "plt.text(x=(len(distances)*.01), y = 0, s=f\"{num_distances_above_theshold + 1} Chunks\");\n",
    "\n",
    "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "plt.title(\"Chunks Based On Embedding Breakpoints\")\n",
    "plt.xlabel(\"Index of sentences\")\n",
    "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks[-3:]):  \n",
    "    print(f\"Chunk #{i}\")\n",
    "    pprint.pprint(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Create a DataFrame with the given data\n",
    "df = pd.DataFrame(chunks)\n",
    "display(df[:3])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = \"temporary_file.csv\"\n",
    "df.to_csv(csv_file_path, sep = \"|\", index=False)\n",
    "\n",
    "loader = CSVLoader(file_path=csv_file_path)\n",
    "data = loader.load()\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=data, embedding=embd)\n",
    "k = 4\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Come posso decidere se nel calcolo della percentuale di saturazione del contratto vadano considerate anche la quantità in previsione?\"\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "pprint.pprint(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | generative_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain_response = rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = []\n",
    "\n",
    "# Manually convert Document objects to dictionaries\n",
    "retrieved_docs_dict = [{\n",
    "    \"metadata\": doc.metadata,\n",
    "    \"page_content\": doc.page_content\n",
    "} for doc in retrieved_docs]\n",
    "\n",
    "# The new entry to be added\n",
    "new_entry = {\n",
    "    \"question\": question,\n",
    "    \"retrieved_doc\": json.dumps(retrieved_docs_dict),  # Convert the list of dicts to JSON string\n",
    "    \"retrieved_doc_k\": k,\n",
    "    \"rag_chain_response\": rag_chain_response,\n",
    "    \"chunking\": chunking_description,\n",
    "    \"description\": preprocessing_description\n",
    "}\n",
    "\n",
    "# Define output file path\n",
    "output_file = \"retrieved_docs_rag_chain_results.csv\"\n",
    "\n",
    "# Load existing data into a DataFrame if the CSV exists, else create a new DataFrame\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_csv(output_file, delimiter=\"|\")\n",
    "else:\n",
    "    # Define the columns\n",
    "    columns = [\"question\", \"retrieved_doc\", \"retrieved_doc_k\", \"rag_chain_response\", \"chunking\", \"description\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Convert the new entry into a DataFrame\n",
    "new_entry_df = pd.DataFrame([new_entry])\n",
    "\n",
    "# Concatenate the new entry with the existing DataFrame\n",
    "df = pd.concat([df, new_entry_df], ignore_index=True)\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "\n",
    "df.to_csv(output_file, mode=\"w\", header=True, index=False, sep=\"|\")\n",
    "print(f\"Data successfully saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the results with different preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# File path (same as the output file from the previous script)\n",
    "input_file = \"retrieved_docs_rag_chain_results.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(input_file, delimiter=\"|\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Access the first row (index 0)\n",
    "row = df.iloc[0]\n",
    "\n",
    "# Deserialize the JSON string back into a Python object (list of dictionaries)\n",
    "retrieved_doc = json.loads(row[\"retrieved_doc\"])\n",
    "\n",
    "for doc in retrieved_doc:\n",
    "    print(f\"  Metadata: {doc['metadata']}\")\n",
    "    pprint.pprint(f\"  Page Content: {doc['page_content']}\")\n",
    "    print(\"-\" * 80)  # Print a separator line for better readability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
