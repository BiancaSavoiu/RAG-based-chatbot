{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from unstructured.staging.base import elements_to_json\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "import pprint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print(\"Current directory:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints parent directory\n",
    "parent_directory = os.path.abspath(os.path.join(path, os.pardir))\n",
    "print(\"Parent directory:\", parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = parent_directory + \"/Doc_Panthera/Gestionale/VEN_Contratti_Vendita_Ordini_Aperti.pdf\"\n",
    "print(\"Filepath:\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=filepath,\n",
    "    \n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    \n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    \n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=\"Images/pdfImages/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "import os\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.getcwd() + \"/figures/figure-31-42.jpg\"\n",
    "print(\"Image path:\", image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert image to base64\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=image.format)\n",
    "        img_str = base64.b64encode(buffered.getvalue())\n",
    "        return img_str.decode('utf-8')\n",
    "\n",
    "image_str = image_to_base64(image_path)\n",
    "len(image_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4-vision-preview\")\n",
    "\n",
    "prompt1 = \"Fornisci una descrizione dell'immagine fornita. Sii chiaro nella spiegazione dei vari campi della sezione principale della videata e i possibili utilizzi.\"\n",
    "\n",
    "msg = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \n",
    "                 \"text\" : prompt1},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_str}\"\n",
    "                    },\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction with placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = parent_directory + \"/Doc_Panthera/Gestionale/VEN_Contratti_Vendita_Ordini_Aperti.pdf\"\n",
    "print(\"Filepath:\", pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "\n",
    "file_name = os.path.basename(pdf_path)\n",
    "print(file_name)\n",
    "#txt_output_path = \"estrazione_testo.txt\"\n",
    "txt_output_path = \"Augmented_\" + file_name\n",
    "images_folder = \"Images\"  # Folder to save images\n",
    "\n",
    "# Create the Images folder if it doesn't exist\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "# Open the PDF\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    extracted_text = \"\"\n",
    "    image_counter = 1  # Counter for image naming\n",
    "\n",
    "    # Loop through each page in the PDF\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        # Extract text\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            extracted_text += page_text + \"\\n\"\n",
    "\n",
    "        # Extract images\n",
    "        for img in page.images:\n",
    "            # Define the file name for the image, saving it in the Images folder\n",
    "            img_name = os.path.join(images_folder, f\"image_{image_counter}.png\")\n",
    "            \n",
    "            # Calculate the region of the image\n",
    "            x0, y0, x1, y1 = img[\"x0\"], img[\"top\"], img[\"x1\"], img[\"bottom\"]\n",
    "            \n",
    "            # Extract and save the image\n",
    "            image = page.within_bbox((x0, y0, x1, y1)).to_image()\n",
    "            image.save(img_name, format=\"PNG\")\n",
    "            \n",
    "            # Add a placeholder in the extracted text\n",
    "            extracted_text += f\"[IMAGE: {img_name}]\\n\"\n",
    "            \n",
    "            # Increment the counter\n",
    "            image_counter += 1\n",
    "\n",
    "# Save the extracted text with placeholders to a text file\n",
    "with open(txt_output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "    txt_file.write(extracted_text)\n",
    "\n",
    "print(\"Extraction complete. Text has been saved, and images have been extracted to the 'Images' folder with placeholders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "chat = ChatOpenAI(model=\"gpt-4-vision-preview\")\n",
    "\n",
    "# Folder containing images\n",
    "image_folder = \"Images\"\n",
    "\n",
    "# Function to convert an image to base64\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=image.format)\n",
    "        img_str = base64.b64encode(buffered.getvalue())\n",
    "        return img_str.decode('utf-8')\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Loop through all files in the image folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    # Check if the file is an image (optional: filter by extensions if necessary)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        # Convert image to base64\n",
    "        image_str = image_to_base64(image_path)\n",
    "        \n",
    "        # Send the image to the model\n",
    "        msg = chat.invoke(\n",
    "            [\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt1\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{image_str}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Store the response using the filename as the key\n",
    "        results[filename] = msg.content\n",
    "\n",
    "# Output the results\n",
    "for image_id, description in results.items():\n",
    "    print(f\"{image_id}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV output file\n",
    "import csv\n",
    "csv_output_path = \"image_descriptions.csv\"\n",
    "\n",
    "# Output the results to a CSV file\n",
    "with open(csv_output_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Image ID\", \"Description\"])  # Write header\n",
    "    for image_id, description in results.items():\n",
    "        writer.writerow([image_id, description])\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Descriptions have been saved to {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern to match image placeholders like [IMAGE: Images/image_x.png]\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "text_file_path = \"estrazione_testo.txt\"\n",
    "# Load the text from the file\n",
    "with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "pattern = r\"\\[IMAGE: Images/(image_\\d+\\.png)\\]\"\n",
    "\n",
    "# Function to replace the image placeholders with their descriptions or empty string if description starts with \"Mi dispiace\"\n",
    "def replace_image_placeholders(match):\n",
    "    image_id = match.group(1)  # Extract the image filename\n",
    "    description = results.get(image_id, None)  # Get the description from the dictionary\n",
    "    \n",
    "    # If the description exists and starts with \"Mi dispiace\", return an empty string instead\n",
    "    if description and description.startswith(\"Mi dispiace\"):\n",
    "        return \"\"  # Replace with empty string if description starts with \"Mi dispiace\"\n",
    "    \n",
    "    # Otherwise, return the description or leave the placeholder if description is missing\n",
    "    return description if description else f\"[IMAGE: Images/{image_id}]\"\n",
    "\n",
    "# Replace placeholders in the text\n",
    "updated_text = re.sub(pattern, replace_image_placeholders, text)\n",
    "\n",
    "print(\"Updated Text:\\n\")\n",
    "print(updated_text)\n",
    "\n",
    "# Save the updated text to a new file\n",
    "with open(\"updated_estrazione_testo.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(updated_text)\n",
    "\n",
    "print(\"Text updated with image descriptions and saved to 'updated_estrazione_testo.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated text to a new CSV file with file_name and updated_text as columns\n",
    "csv_name = \"Augmented_dataset.csv\"\n",
    "with open(csv_name, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter=\"|\")\n",
    "    csv_writer.writerow([\"file_name\", \"updated_text\"])  # Write header\n",
    "    csv_writer.writerow([file_name, updated_text])  # Write data row\n",
    "\n",
    "print(\"Text updated with image descriptions and saved to 'updated_text.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset for RAG\n",
    "import pandas as pd\n",
    "\n",
    "csv_name = \"Augmented_dataset.csv\"\n",
    "data_df = pd.read_csv(csv_name, names = ['file_name', 'updated_text'], delimiter=\"|\")\n",
    "data_df = data_df.drop(index = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import pprint\n",
    "\n",
    "loader = DataFrameLoader(data_df, page_content_column=\"updated_text\")\n",
    "docs_data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents and keep track of chunk numbers within each document\n",
    "splits = []\n",
    "for doc in docs_data:\n",
    "    # Split each document into chunks\n",
    "    doc_chunks = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Add chunk number as metadata\n",
    "    for chunk_num, chunk in enumerate(doc_chunks):\n",
    "        chunk.metadata[\"chunk_number\"] = chunk_num + 1  # Adding 1 to start counting from 1\n",
    "        splits.append(chunk)\n",
    "\n",
    "# Print the first few splits with chunk numbers\n",
    "pprint.pprint(splits[0:6])\n",
    "pprint.pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(splits[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embd = M3EmbeddingFP16()\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embd)\n",
    "vectorstore.save_local(\"trial_VEN_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Come posso decidere se nel calcolo della percentuale di saturazione del contratto vadano considerate anche la quantità in previsione?\"\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM - the used model\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# max_token\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "pprint.pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM - the used model\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "# max_token\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "pprint.pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_llama = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "pprint.pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "model_llama_instruct = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "\n",
    "rag_chai = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_llama_instruct\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint.pprint(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "token_pro = os.getenv('HUGGINGFACE_TOKEN')\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=token_pro\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1024},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Il tuo compito è scrivere una domanda e una risposta data un contesto.\n",
    "La tua domanda deve essere rispondibile con un'informazione specifica dal contesto. Se nel contesto ci sono errori grammaticali o morfologici correggili nell'output fornito.\n",
    "La tua domanda deve essere formulata nello stesso stile delle domande che gli utenti potrebbero porre ad un helpdesk, che si occupa di assistenza clienti per un software aziendale.\n",
    "Questo significa che la tua domanda NON deve menzionare frasi come \"secondo il passaggio\" o \"nel contesto\". \n",
    "La tua domanda può menzionare frasi come \"Ho un errore\" o \"Come posso sistemare il problema\".\n",
    "\n",
    "Domanda e risposta devono essere generate in italiano.\n",
    "\n",
    "Fornisci la tua risposta come segue:\n",
    "\n",
    "Output:::\n",
    "Domanda: (la tua domanda)\n",
    "Risposta: (la tua risposta alla domanda)\n",
    "\n",
    "Ora ecco il contesto.\n",
    "\n",
    "Contesto: {context}\\n\n",
    "Output:::\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_GENERATIONS = 5\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(splits, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Domanda: \")[-1].split(\"Risposta: \")[0]\n",
    "        answer = output_QA_couple.split(\"Risposta: \")[-1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"source_doc\": sampled_context.metadata[\"file_name\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "display(generated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.iloc[0]['question'], generated_questions.iloc[0]['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
