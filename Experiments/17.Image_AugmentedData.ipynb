{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a Gemini API model.\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process all files with unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "parent_directory = os.path.abspath(os.path.join(path, os.pardir))\n",
    "input_dir = parent_directory + \"/Doc_Panthera\"\n",
    "output_dir_base = parent_directory + \"/Doc_Panthera_Augmented\"\n",
    "print(\"Filepath:\", input_dir)\n",
    "print(\"Output path:\", output_dir_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration and Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Define the types of elements to exclude\n",
    "excluded_values = [\"Header\", \"Footer\", \"PageNumber\"]\n",
    "\n",
    "# Get the current working directory\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Determine the parent directory\n",
    "parent_directory = os.path.abspath(os.path.join(current_path, os.pardir))\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = os.path.join(parent_directory, \"Doc_Panthera\")\n",
    "output_dir_base = os.path.join(parent_directory, \"Doc_Panthera_Augmented\")\n",
    "\n",
    "# Print input and output paths for verification\n",
    "print(\"Input Directory:\", input_dir)\n",
    "print(\"Output Directory:\", output_dir_base)\n",
    "\n",
    "# Create the base output directory if it doesn't exist\n",
    "os.makedirs(output_dir_base, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV output\n",
    "csv_output_path = os.path.join(output_dir_base, \"extracted_texts.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# CSV Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Open the CSV file for writing extracted texts\n",
    "with open(csv_output_path, mode=\"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=\"|\")\n",
    "    # Write the header row\n",
    "    csv_writer.writerow([\"File Name\", \"Extracted Text\"])\n",
    "\n",
    "    # -------------------------------\n",
    "    # Directory Traversal and PDF Processing\n",
    "    # -------------------------------\n",
    "\n",
    "    # Walk through the input directory and its subdirectories\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            # Process only PDF files (case-insensitive)\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(root, filename)\n",
    "                file_name_without_extension = os.path.splitext(filename)[0]\n",
    "\n",
    "                print(f\"\\nProcessing PDF: {filename}\")\n",
    "\n",
    "                # Define a specific output directory for images from this PDF\n",
    "                pdf_output_dir = os.path.join(output_dir_base, f\"{file_name_without_extension}_Images\")\n",
    "                os.makedirs(pdf_output_dir, exist_ok=True)\n",
    "\n",
    "                # -------------------------------\n",
    "                # PDF Parsing and Element Extraction\n",
    "                # -------------------------------\n",
    "\n",
    "                # Extract elements from the PDF\n",
    "                elements = partition_pdf(\n",
    "                    pdf_path,\n",
    "                    extract_images_in_pdf=True,\n",
    "                    strategy=\"hi_res\",\n",
    "                    languages=['ita'],  # Specify languages as needed\n",
    "                    extract_image_block_output_dir=pdf_output_dir\n",
    "                )\n",
    "\n",
    "                # Initialize lists and dictionaries to store processed data\n",
    "                processed_elements = []\n",
    "                image_metadata = {}\n",
    "\n",
    "                # Iterate over each extracted element\n",
    "                for el in elements:\n",
    "                    el_dict = el.to_dict()  # Convert element to a dictionary\n",
    "                    el_type = el_dict.get(\"type\", None)  # Get the type of the element\n",
    "\n",
    "                    # -------------------------------\n",
    "                    # Filtering Unwanted Elements\n",
    "                    # -------------------------------\n",
    "\n",
    "                    if el_type in excluded_values:\n",
    "                        print(el)\n",
    "                        # Skip unwanted elements\n",
    "                        continue\n",
    "\n",
    "                    # -------------------------------\n",
    "                    # Handling Image Elements\n",
    "                    # -------------------------------\n",
    "                    if el_type == \"Image\":\n",
    "                        # Retrieve the image filename from metadata\n",
    "                        image_filename = el_dict.get(\"metadata\", {}).get(\"filename\")\n",
    "                        image_path = os.path.join(pdf_output_dir, image_filename) if image_filename else None\n",
    "\n",
    "                        # Create a placeholder for the image in the text output\n",
    "                        placeholder = f\"[IMAGE: {el_dict['element_id']}]\"\n",
    "                        processed_elements.append(placeholder)\n",
    "\n",
    "                        # Store image metadata for future reference\n",
    "                        image_metadata[el_dict['element_id']] = {\n",
    "                            \"metadata\": el_dict,\n",
    "                            \"image_path\": image_path,\n",
    "                        }\n",
    "                    else:\n",
    "                        # -------------------------------\n",
    "                        # Handling Text Elements\n",
    "                        # -------------------------------\n",
    "                        text = el_dict.get(\"text\", \"\").strip()\n",
    "                        if text:  # Ensure that empty strings are not added\n",
    "                            processed_elements.append(text)\n",
    "\n",
    "                # -------------------------------\n",
    "                # Combining and Saving Extracted Data\n",
    "                # -------------------------------\n",
    "\n",
    "                # Combine all processed elements into a single text block\n",
    "                output_text = \"\\n\".join(processed_elements)\n",
    "\n",
    "                # Define the path for the image metadata JSON file\n",
    "                image_metadata_path = os.path.join(output_dir_base, f\"{file_name_without_extension}_image_metadata.json\")\n",
    "                with open(image_metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(image_metadata, f, indent=2)\n",
    "\n",
    "                # -------------------------------\n",
    "                # Writing to CSV and Text Files\n",
    "                # -------------------------------\n",
    "\n",
    "                # Write the filename and extracted text to the CSV file\n",
    "                csv_writer.writerow([filename, output_text])\n",
    "\n",
    "                # Define the path for the processed text output\n",
    "                text_output_path = os.path.join(output_dir_base, f\"{file_name_without_extension}_processed_output.txt\")\n",
    "                with open(text_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(output_text)\n",
    "\n",
    "                print(f\"Processed and saved: {filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Completion Message\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"\\nProcessing complete. Extracted texts saved to {csv_output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image description generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def process_images_and_get_descriptions(directory_path, model):\n",
    "    \"\"\"\n",
    "    Processes each .jpg image in the specified folder, opens it using PIL, \n",
    "    and calls the LLM model to get a description.\n",
    "\n",
    "    Args:\n",
    "    - directory_path: The path to the folder containing images.\n",
    "    - model: The LLM model that will generate descriptions for images.\n",
    "\n",
    "    Returns:\n",
    "    - descriptions: A dictionary where the keys are image filenames \n",
    "      and the values are the generated descriptions.\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "\n",
    "    # Loop through the directory and find all .jpg files\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            image_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Open the image using PIL\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    # Generate description using the LLM model\n",
    "                    print(f\"Processing image: {filename}\")\n",
    "                    response = model.generate_content(\n",
    "                        [image_path, \"Descrivi la parte principale della finestra, nell'immagine fornita.\"]\n",
    "                    )\n",
    "                    \n",
    "                    # Extract description from the model's response\n",
    "                    description = response.text\n",
    "                    descriptions[filename] = description\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                descriptions[filename] = \"Error processing image\"\n",
    "\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_path_and_id_from_element(element):\n",
    "    \"\"\"\n",
    "    Extracts the nested 'image_path' for .jpg files and the 'element_id' from a given element.\n",
    "\n",
    "    Args:\n",
    "    - element: A dictionary representing an element containing metadata.\n",
    "\n",
    "    Returns:\n",
    "    - (image_path, element_id): A tuple containing the .jpg image path and element id if present, else None for each.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract 'element_id'\n",
    "        element_id = element.get(\"metadata\", {}).get(\"element_id\")\n",
    "\n",
    "        # Extract nested 'image_path' for the .jpg files\n",
    "        nested_metadata = element.get(\"metadata\", {}).get(\"metadata\", {})\n",
    "        image_path = nested_metadata.get(\"image_path\")\n",
    "\n",
    "        # Ensure the extracted path is a .jpg\n",
    "        if image_path and image_path.endswith(\".jpg\"):\n",
    "            return image_path, element_id\n",
    "        else:\n",
    "            print(f\"Warning: No .jpg image_path found for element_id {element_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing element: {element}. Error: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_image_paths_and_ids(metadata_file):\n",
    "    \"\"\"\n",
    "    Extracts the .jpg 'image_path' and 'element_id' for each element in the image metadata JSON file.\n",
    "\n",
    "    Args:\n",
    "    - metadata_file: The path to the image metadata JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - image_data: A list of tuples containing the .jpg image path and element id.\n",
    "    \"\"\"\n",
    "    image_data = []\n",
    "\n",
    "    try:\n",
    "        # Load the image metadata from the JSON file\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            image_metadata = json.load(f)\n",
    "\n",
    "        # Iterate through the elements in the metadata and extract the .jpg image_path and element_id\n",
    "        for element in image_metadata.values():\n",
    "            data = extract_image_path_and_id_from_element(element)\n",
    "            if data:\n",
    "                image_data.append(data)\n",
    "            else:\n",
    "                print(f\"Warning: No valid .jpg 'image_path' or 'element_id' found in element.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading metadata file: {e}\")\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def integrate_descriptions(metadata_file, descriptions, processed_output_path, final_output_path):\n",
    "    \"\"\"\n",
    "    Integrates image descriptions into the text by replacing [IMAGE: element_id] placeholders.\n",
    "\n",
    "    Args:\n",
    "    - metadata_file: Path to the metadata JSON file containing image_path and element_id.\n",
    "    - descriptions: Dictionary where keys are image filenames, and values are descriptions.\n",
    "    - processed_output_path: Path to the text file containing placeholders.\n",
    "    - final_output_path: Path to save the final text with descriptions integrated.\n",
    "\n",
    "    Returns:\n",
    "    - The final text with descriptions integrated.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract image paths and element IDs using the helper functions\n",
    "    print(\"Extracting image paths and element IDs...\")\n",
    "    image_data = extract_image_paths_and_ids(metadata_file)\n",
    "\n",
    "    # Step 2: Map element IDs to descriptions\n",
    "    element_id_to_description = {}\n",
    "    for image_path, element_id in image_data:\n",
    "        filename = os.path.basename(image_path)  # Extract the filename\n",
    "        description = descriptions.get(filename)  # Match filename to description\n",
    "        if description:\n",
    "            element_id_to_description[element_id] = description\n",
    "        else:\n",
    "            print(f\"Warning: No description found for {filename}\")\n",
    "\n",
    "    # Step 3: Replace placeholders in processed output text\n",
    "    print(\"\\nReading processed output text...\")\n",
    "    with open(processed_output_path, \"r\", encoding=\"utf-8\") as processed_file:\n",
    "        processed_text = processed_file.read()\n",
    "\n",
    "    print(\"Replacing placeholders...\")\n",
    "    unresolved_placeholders = []\n",
    "    for element_id, description in element_id_to_description.items():\n",
    "        placeholder = f\"[IMAGE: {element_id}]\"\n",
    "        if placeholder in processed_text:\n",
    "            processed_text = processed_text.replace(placeholder, description)\n",
    "        else:\n",
    "            unresolved_placeholders.append(placeholder)\n",
    "\n",
    "    # Log unresolved placeholders\n",
    "    if unresolved_placeholders:\n",
    "        print(\"\\nUnresolved Placeholders:\")\n",
    "        for placeholder in unresolved_placeholders:\n",
    "            print(f\"  {placeholder}\")\n",
    "\n",
    "    # Step 4: Write the final output to a file\n",
    "    print(f\"\\nWriting final output to {final_output_path}...\")\n",
    "    with open(final_output_path, \"w\", encoding=\"utf-8\") as final_file:\n",
    "        final_file.write(processed_text)\n",
    "\n",
    "    print(\"Descriptions successfully integrated.\")\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "output_dir_base = parent_directory + \"/Doc_Panthera_Augmented\"\n",
    "\n",
    "# Iterate over processed metadata files\n",
    "for root, dirs, files in os.walk(output_dir_base):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name.endswith(\"_Images\"):  # Look for directories storing images\n",
    "            pdf_name = dir_name.replace(\"_Images\", \"\")\n",
    "            image_dir = os.path.join(root, dir_name)\n",
    "            metadata_file = os.path.join(output_dir_base, f\"{pdf_name}_image_metadata.json\")\n",
    "            processed_output_path = os.path.join(output_dir_base, f\"{pdf_name}_processed_output.txt\")\n",
    "            final_output_path = os.path.join(output_dir_base, f\"{pdf_name}_final_output.txt\")\n",
    "\n",
    "            # Check if the final output already exists\n",
    "            if os.path.exists(final_output_path):\n",
    "                print(f\"Skipping {pdf_name}: Final output already exists at {final_output_path}.\")\n",
    "                continue\n",
    "\n",
    "            # Skip if required files are missing\n",
    "            if not (os.path.exists(metadata_file) and os.path.exists(processed_output_path)):\n",
    "                print(f\"Skipping {pdf_name}: Missing metadata or processed text file.\")\n",
    "                continue\n",
    "\n",
    "            # Step 1: Generate descriptions for images in the directory\n",
    "            print(f\"Processing images in {image_dir}...\")\n",
    "            descriptions = process_images_and_get_descriptions(image_dir, model)\n",
    "\n",
    "            # Step 2: Integrate descriptions into processed text\n",
    "            print(f\"Integrating descriptions into text for {pdf_name}...\")\n",
    "            try:\n",
    "                integrate_descriptions(metadata_file, descriptions, processed_output_path, final_output_path)\n",
    "                print(f\"Descriptions integrated successfully for {pdf_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error integrating descriptions for {pdf_name}: {e}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a CSV for final augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store file information\n",
    "data = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for root, dirs, files in os.walk(output_dir_base):\n",
    "    for file in files:\n",
    "        if file.endswith(\"_final_output.txt\"):\n",
    "            # Extract the original file name\n",
    "            original_file = file.replace(\"_final_output.txt\", \"\")\n",
    "\n",
    "            # Read the contents of the final output file\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text_content = f.read()\n",
    "\n",
    "            # Append to the data list\n",
    "            data.append({\"FileName\": original_file, \"Text\": text_content})\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "csv_output_path = os.path.join(output_dir_base, \"augmented_dataset_final_outputs.csv\")\n",
    "df.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Dataframe saved to {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment the improvement obtained with the image description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "Comportati come un assistente che risponde alle domande del cliente.\n",
    "Rispondi alla domanda basandoti solo sui seguenti documenti: {context}\n",
    "\n",
    "Rispondi in modo conciso e chiaro, spiegando passo passo al cliente le azioni necessarie da effettuare.\n",
    "Se possibile, dai indicazioni dettagliate al cliente, su come risolvere il problema o effettuare l'azione desiderata.\n",
    "Evita troppe ripetizioni nella risposta fornita.\n",
    "Quando spieghi che cosa è o cosa significa un certo elemento richiesto, non parlarne come se fosse un problema.\n",
    "\n",
    "In caso di più domande rispondi solo a quelle inerenti alla documentazione e rimani a disposizione per altre domande sull'argomento, specificando,\n",
    "invece, che le altre domande non sono state trovate pertinenti in questo contesto.\n",
    "\n",
    "Domanda relativa al software Panthera: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model common to both solutions\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model_fp16 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "class M3EmbeddingFP16:\n",
    "    def embed_documents(self, texts):\n",
    "        return model_fp16.encode(texts)['dense_vecs']\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "    \n",
    "embd = M3EmbeddingFP16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro-latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without augmented data\n",
    "# Contains the documents without any data preprocessing steps\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "vectorstore = FAISS.load_local(\"local_model_index\", embd, allow_dangerous_deserialization=True)\n",
    "print(vectorstore, vectorstore.index.ntotal)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Post-processing\n",
    "def baseline_format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "basic_rag_chain = (\n",
    "    {\"context\": retriever | baseline_format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_gemini\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "question = \"Come posso decidere se nel calcolo della percentuale di saturazione del contratto vadano considerate anche le quantità in previsione e come si aggiungono?\"\n",
    "print(basic_rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmented data\n",
    "# Contains the documents without any data preprocessing steps\n",
    "vectorstore_augmented = FAISS.load_local(\"augmented_faiss_index\", embd, allow_dangerous_deserialization=True)\n",
    "print(vectorstore_augmented, vectorstore_augmented.index.ntotal)\n",
    "\n",
    "retriever_augmented = vectorstore_augmented.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Chain\n",
    "basic_rag_chain_augmented = (\n",
    "    {\"context\": retriever_augmented | baseline_format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_gemini\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basic_rag_chain_augmented.invoke(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_bianca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
